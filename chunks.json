{
  "book_id": "physical-ai-robotics",
  "book_version": "v1.0.0",
  "total_chunks": 195,
  "chunks": [
    {
      "content": "---\nid: chapter-1-introduction\ntitle: \"Chapter 1: Introduction to Physical AI\"\nsidebar_label: \"1. Introduction\"\nsidebar_position: 1\ndescription: \"Comprehensive introduction to physical AI, embodied cognition, and the fundamental challenges of creating intelligent systems that interact with the physical world\"\ndifficulty: beginner\nestimated_time: 4 hours\nprerequisites:\n  - Basic artificial intelligence concepts\n  - Fundamental machine learning knowledge\n  - Introduction to robotics (helpful but not required)\nlearning_objectives:\n  - Define physical AI and distinguish it from disembodied AI systems\n  - Understand the principles of embodied cognition and their implications for AI\n  - Trace the historical development of physical AI from early robots to modern humanoids\n  - Explain Moravec's paradox and its significance for robot design\n  - Identify and analyze the key challenges of embodiment in AI systems\ntags:\n  - physical-ai\n  - embodied-cognition\n  - robotics-history\n  - moravec-paradox\n  - humanoid-robotics\ncode_examples: []\n---\n\n# Chapter 1: Introduction to Physical AI\n\n**Learning Objectives:**\n- Define physical AI and distinguish it from disembodied AI systems\n- Understand the principles of embodied cognition and their implications for artificial intelligence\n- Trace the historical development of physical AI from early robots to modern humanoids\n- Explain Moravec's paradox and its significance for contemporary robot design\n- Identify and analyze the key challenges of embodiment in AI systems\n\n**Prerequisites:** Basic AI/ML knowledge, familiarity with fundamental computer science concepts\n**Estimated Reading Time:** 4 hours\n**Difficulty:** Beginner",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 222,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "The field of artificial intelligence has traditionally focused on disembodied cognitive tasks—playing chess, proving theorems, recognizing patterns in data, or generating human-like text. These achievements, while remarkable, represent only one dimension of intelligence. A chess-playing algorithm that can defeat world champions struggles to pick up a single chess piece without dropping it. A language model that can discuss quantum mechanics in eloquent prose cannot navigate a cluttered room or grasp a door handle. This disparity reveals a fundamental truth: intelligence, as it evolved in biological systems, is intrinsically tied to physical interaction with the world (Brooks, 1991; Pfeifer & Bongard, 2006).\n\nPhysical AI—artificial intelligence systems embodied in physical agents that sense and act upon their environment—represents a paradigm shift from purely computational intelligence to intelligence grounded in sensorimotor experience. These systems must not only process information but also contend with the complexities of the physical world: uncertain sensor data, dynamic environments, real-time constraints, and the consequences of physical actions. As robotics and AI converge, we are witnessing the emergence of increasingly sophisticated physical AI systems, from warehouse robots that navigate complex logistics centers to humanoid robots capable of parkour and manipulation tasks that rival human performance (Kuindersma et al., 2016).\n\nThis chapter introduces the foundational concepts of physical AI with a specific focus on humanoid robotics—systems designed to approximate human morphology and capabilities. We begin by exploring embodied cognition and how it reframes our understanding of intelligence. We then distinguish physical AI from disembodied AI, examining what changes when intelligence must operate through physical embodiment. A historical survey traces the evolution from early robotic systems like SHAKEY to modern humanoids such as Boston Dynamics' Atlas and Tesla's Optimus. We examine Moravec's paradox—the observation that high-level reasoning is computationally easier than low-level sensorimotor skills—and its profound implications for robot design. Finally, we identify the key technical challenges that arise from embodiment: perception under uncertainty, real-time control, physical safety, and the integration of learning with physical interaction.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 325,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "### 1.1.1 The Embodied Cognition Hypothesis\n\nTraditional cognitive science, influenced heavily by the computational theory of mind, viewed intelligence as symbol manipulation—abstract, disembodied computation that could in principle occur independently of any physical substrate (Brooks, 1991). Under this view, a mind is fundamentally a program, and bodies are merely input-output devices for interacting with the environment. This perspective led to early AI research focusing almost exclusively on symbolic reasoning, logical inference, and knowledge representation.\n\nThe embodied cognition hypothesis challenges this view fundamentally. It posits that cognitive processes are deeply rooted in the body's interactions with the world—that perception, action, and cognition are inseparably intertwined (Pfeifer & Bongard, 2006). According to this perspective, intelligence cannot be fully understood or replicated without considering the physical body, its sensorimotor capabilities, and its dynamic interaction with the environment. The body is not merely a vessel for the mind but an integral component of cognitive processing itself.\n\nSeveral key principles characterize embodied cognition:\n\n**Morphological Computation**: The physical structure and dynamics of a body can perform computational functions, offloading processing from the brain or central controller. For example, the passive dynamics of a robot's legs during walking can provide stability without explicit computational control, reducing the complexity of gait control algorithms (Pfeifer & Bongard, 2006). The mechanical properties of tendons, joints, and limbs contribute to control in ways that would require extensive computation if implemented purely algorithmically.\n\n**Sensorimotor Coupling**: Cognition emerges from tight, real-time coupling between sensory perception and motor action. An agent does not first perceive the world completely, then plan, then act; instead, perception and action occur in continuous, coupled loops. A robot navigating a corridor, for instance, continuously adjusts its trajectory based on visual feedback rather than pre-computing a complete path (Brooks, 1991).\n\n**Situatedness**: Intelligent behavior is fundamentally situated in specific environments and contexts. Rather than abstract reasoning over world models, embodied intelligence involves direct, context-specific responses to environmental stimuli. A legged robot traversing rough terrain must respond to local terrain features in real-time rather than planning every footstep from a global map.\n\n**Environmental Scaffolding**: The environment itself can serve as an extension of cognitive processing. Animals and robots alike use environmental structure to reduce computational demands—using landmarks for navigation, using surfaces for support, exploiting gravity for manipulation. The environment is not merely a backdrop for cognition but an active participant in it.\n\nThese principles have profound implications for how we design and understand physical AI systems. Rather than attempting to replicate human-level abstract reasoning first and then \"add\" a body, embodied AI approaches suggest that intelligence must be built from the ground up through sensorimotor interaction.\n\n### 1.1.2 Defining Physical AI\n\nPhysical AI, also referred to as embodied AI or robotic AI, encompasses artificial intelligence systems that possess physical bodies enabling them to sense and act in the physical world. More formally, we can define physical AI as:",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.1 Embodied Cognition and Physical AI",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 476,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "### 1.1.2 Defining Physical AI\n\nPhysical AI, also referred to as embodied AI or robotic AI, encompasses artificial intelligence systems that possess physical bodies enabling them to sense and act in the physical world. More formally, we can define physical AI as:\n\n> **Physical AI**: An autonomous or semi-autonomous agent comprising (1) a physical body with sensors for perceiving environmental states, (2) actuators for executing physical actions, (3) computational systems for processing sensory information and generating control commands, and (4) algorithms enabling the agent to achieve goals through physical interaction with its environment.\n\nThis definition encompasses several key elements:\n\n**Physical Embodiment**: The system has a tangible body subject to physical laws—gravity, friction, inertia, and material constraints. This embodiment introduces challenges absent in purely computational systems: mechanical wear, energy limitations, mass and inertia effects, and physical safety constraints.\n\n**Perception**: The system acquires information about its environment and internal state through sensors—cameras, force sensors, proprioceptive sensors, inertial measurement units (IMUs), and others. Unlike simulated agents with perfect access to world state, physical AI systems must deal with noisy, incomplete, and sometimes contradictory sensor data.\n\n**Action**: The system affects its environment through actuators—motors, hydraulic systems, pneumatic actuators, or other mechanisms that convert control signals into physical motion. Actions have consequences that cannot always be predicted perfectly: objects slip, surfaces are uneven, and disturbances occur.\n\n**Autonomy**: The system makes decisions and executes actions without continuous human intervention, though the degree of autonomy varies widely. Full autonomy represents a long-term goal; many current physical AI systems operate with varying levels of human supervision or teleoperation.\n\n**Goal-Directedness**: The system pursues objectives, whether explicitly programmed, learned through experience, or specified through natural language commands. Goals may range from simple (navigate to a location) to complex (prepare a meal in an unfamiliar kitchen).\n\nPhysical AI systems span a wide spectrum of embodiments and capabilities. Industrial robot arms performing repetitive assembly tasks represent one end of this spectrum—highly specialized systems operating in structured environments. Autonomous vehicles navigating public roads represent intermediate complexity, dealing with dynamic environments but constrained to two-dimensional motion on relatively predictable surfaces. Humanoid robots represent the high-complexity end: general-purpose systems designed to operate in human environments, manipulate human tools, and potentially collaborate with humans across diverse tasks.\n\n### 1.1.3 The Importance of Physical Interaction\n\nWhy build physical AI systems when simulation environments can train agents far more quickly and safely? The answer lies in several irreducible aspects of physical interaction:\n\n**Reality Gap**: Simulations, no matter how sophisticated, cannot perfectly model physical reality. Subtle effects—friction variations, material deformation, sensor noise characteristics, actuator dynamics—differ between simulation and reality. Agents trained purely in simulation often fail when deployed on real robots, a challenge known as the \"sim-to-real\" gap (Tobin et al., 2017). While techniques like domain randomization can narrow this gap, complete closure remains elusive for complex tasks.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.1 Embodied Cognition and Physical AI",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 469,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "**Physical Common Sense**: Humans develop intuitions about physical causality through embodied experience—understanding that unsupported objects fall, that liquids spill, that fragile objects break when dropped. These intuitions, trivial for humans, are extraordinarily difficult to encode explicitly or learn from static data. Physical interaction provides the grounding necessary for developing robust common-sense physical reasoning.\n\n**Social and Economic Value**: Many valuable tasks require physical interaction with the world—manufacturing, logistics, construction, healthcare, domestic assistance, and emergency response. The economic incentive to automate physical labor drives substantial investment in physical AI. Beyond economic value, physical AI systems can perform tasks too dangerous for humans (disaster response, hazardous material handling) or extend human capabilities (surgery assistance, elder care).\n\n**Scientific Understanding**: Building physical AI systems advances our understanding of intelligence itself. Neuroscience and cognitive science benefit from computational models that must contend with real-world physical constraints. Conversely, understanding biological intelligence—how animals solve locomotion, manipulation, and navigation problems—informs robotics (Pfeifer & Bongard, 2006).\n\nThe progression from disembodied AI to physical AI thus represents not merely an engineering challenge but a fundamental expansion of what we mean by artificial intelligence.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.1 Embodied Cognition and Physical AI",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 182,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "### 1.2.1 Contrasting Paradigms\n\nDisembodied AI systems—those operating purely in computational or virtual environments—and physical AI systems differ along several critical dimensions. Understanding these differences illuminates the unique challenges of physical embodiment.\n\n**State Representation and Uncertainty**: Disembodied AI systems often operate with complete or near-complete state information. A chess engine has perfect knowledge of the board state; a theorem prover has complete access to axioms and current proof steps. Physical AI systems, conversely, must infer environmental state from noisy, partial sensor observations. A robot navigating a building doesn't know precisely where it is or what objects surround it—it must estimate these from camera images, lidar scans, or other sensors, each with characteristic noise and failure modes (Thrun et al., 2005).\n\n**Action Consequences and Reversibility**: In many disembodied domains, actions are easily reversible or consequence-free. A chess player can analyze millions of hypothetical moves without moving a physical piece. An AI exploring a decision tree can backtrack costlessly. Physical actions, however, have irreversible consequences. A robot that drops a glass cannot undo that action. Energy expended in motion cannot be recovered fully. This irreversibility demands more cautious, robust decision-making (Deisenroth et al., 2013).\n\n**Real-Time Constraints**: Disembodied systems often operate without hard real-time constraints. A language model can take minutes to generate a response; a game-playing AI can deliberate for extended periods. Physical AI systems frequently face hard real-time deadlines. A humanoid robot maintaining balance must compute motor commands at hundreds of Hertz; delays of even milliseconds can result in falls. Perception, planning, and control must all execute within stringent time budgets (Siciliano & Khatib, 2016).\n\n**Safety and Risk**: Errors in disembodied AI systems typically have limited consequences—a wrong answer, a poor game move, or an inappropriate text generation. Physical AI system failures can cause physical harm—colliding with humans, damaging property, or harming the robot itself. Safety becomes a first-order design constraint, requiring redundant systems, conservative behavior, and extensive validation (Haddadin et al., 2017).\n\n**Dimensionality and Continuous State-Action Spaces**: Many disembodied AI problems have discrete state and action spaces (chess has ~10^43 possible board states, large but discrete). Physical AI systems inhabit continuous, high-dimensional state and action spaces. A humanoid robot might have 30+ degrees of freedom, each represented by continuous angles and velocities, yielding state spaces with 60+ dimensions. Action spaces are similarly continuous and high-dimensional. This continuity complicates planning, learning, and control (LaValle, 2006).\n\n### 1.2.2 Task Complexity Inversions: Moravec's Paradox Preview\n\nOne of the most striking contrasts between disembodied and physical AI lies in what tasks prove difficult. Disembodied AI has achieved superhuman performance in chess, Go, mathematical reasoning, and language generation—tasks that represent the pinnacle of human cognitive achievement. Yet even the most advanced robots struggle with tasks that human toddlers perform effortlessly: walking across uneven ground, picking up unfamiliar objects, or recognizing a face in varied lighting conditions.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.2 Physical AI versus Disembodied AI",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 471,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "This inversion—that what's computationally hard for disembodied AI is often easy for physical AI, and vice versa—is known as Moravec's paradox, which we will explore in detail in Section 1.4. For now, we note that this paradox highlights a fundamental difference in problem character: Abstract reasoning often reduces to search and pattern-matching in well-defined spaces, amenable to computational brute force. Physical skills require navigating continuous, high-dimensional spaces under uncertainty and real-time constraints—a very different challenge.\n\n### 1.2.3 Hybrid Approaches: Combining Physical and Disembodied AI\n\nContemporary AI systems increasingly blur the boundary between physical and disembodied AI through hybrid architectures. Consider a household service robot tasked with preparing a meal:\n\n- **High-level planning** (disembodied): The robot uses language understanding to interpret the recipe and abstract reasoning to determine the sequence of steps—tasks better suited to disembodied AI approaches.\n- **Perception and manipulation** (physical): The robot uses computer vision to locate ingredients, force sensing to grasp objects appropriately, and motor control to manipulate utensils—intrinsically physical tasks.\n- **Learning from simulation and reality** (hybrid): The robot may train manipulation policies initially in simulation (disembodied), then fine-tune them through physical interaction (embodied), bridging the sim-to-real gap (Tobin et al., 2017).\n\nThese hybrid systems leverage the strengths of both paradigms: disembodied AI for abstract reasoning, planning, and rapid simulated learning; physical AI for sensorimotor skills, real-world validation, and common-sense physical understanding. Understanding both paradigms and their integration is essential for building capable, general-purpose robots.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.2 Physical AI versus Disembodied AI",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 239,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "### 1.3.1 Early Mobile Robots: SHAKEY and Its Contemporaries (1960s-1980s)\n\nThe history of physical AI begins in earnest in the 1960s with the development of mobile robots capable of autonomous reasoning and action. The most iconic of these early systems was SHAKEY, developed at the Stanford Research Institute (now SRI International) from 1966 to 1972 (Siciliano & Khatib, 2016).\n\n**SHAKEY the Robot**: SHAKEY was a mobile robot equipped with a television camera, range finder, and bump sensors, connected via radio to a large mainframe computer (a DEC PDP-10 and PDP-15). Despite its mobility, SHAKEY's \"brain\" remained off-board due to the computational demands of its AI systems. SHAKEY operated in carefully constructed environments—rooms with geometric blocks, ramps, and clearly marked boundaries.\n\nSHAKEY's significance lay not in its physical capabilities, which were modest, but in its integration of multiple AI subsystems:\n\n- **Vision**: Image processing to identify objects and spatial relationships\n- **Planning**: The STRIPS planner, which could decompose high-level goals into sequences of primitive actions\n- **Navigation**: Path planning algorithms for obstacle avoidance\n- **Reasoning**: Logical inference over a symbolic world model\n\nSHAKEY demonstrated that an artificial agent could perceive its environment, reason about actions, plan sequences of operations, and execute them to achieve goals—the fundamental loop of autonomous robotics. However, SHAKEY also revealed profound limitations:\n\n- **Computational Requirements**: Even simple tasks required minutes of computation on powerful mainframes.\n- **Fragile Perception**: Vision systems worked only in controlled environments with good lighting and simple objects.\n- **Brittle Planning**: The symbolic planner assumed perfect world knowledge and could not handle uncertainty or unexpected disturbances.\n- **Limited Robustness**: Small deviations from expected conditions could cause complete failure.\n\n**Contemporary Systems**: Other notable early mobile robots included the Stanford Cart (1960s-1970s), which pioneered stereo vision for obstacle avoidance, and mobile robots developed at MIT's AI Lab, which explored reactive control architectures as alternatives to the sense-plan-act paradigm exemplified by SHAKEY.\n\n### 1.3.2 The Reactive Revolution: Brooks and Behavior-Based Robotics (1980s-1990s)\n\nBy the mid-1980s, the limitations of SHAKEY-style \"sense-plan-act\" robotics were evident. Systems were slow, fragile, and worked only in highly controlled environments. Rodney Brooks, then at MIT, proposed a radical alternative: reactive, behavior-based robotics (Brooks, 1991).\n\n**Subsumption Architecture**: Brooks introduced the subsumption architecture, which decomposed robot control into layers of simple behaviors that operated in parallel, directly connecting sensors to actuators without centralized planning or world models. Higher layers could \"subsume\" (override) lower layers when appropriate. For example, a mobile robot might have:\n\n- **Layer 0**: Wander randomly, avoiding immediate obstacles\n- **Layer 1**: Head toward distant goals\n- **Layer 2**: Recognize and approach specific objects\n\nEach layer operated independently with its own sensorimotor loop. There was no central world model, no explicit planning—just collections of simple behaviors that, in aggregate, produced intelligent-seeming behavior.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.3 Historical Development of Physical AI",
        "subsection": null,
        "page_number": null,
        "chunk_index": 7,
        "word_count": 459,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "Each layer operated independently with its own sensorimotor loop. There was no central world model, no explicit planning—just collections of simple behaviors that, in aggregate, produced intelligent-seeming behavior.\n\n**\"Intelligence Without Representation\"**: Brooks famously argued that intelligence doesn't require explicit internal representations of the world. Instead, \"the world is its own best model\"—robots should react directly to sensory stimuli rather than building abstract models (Brooks, 1991). This perspective, heavily influenced by embodied cognition, suggested that sensorimotor coupling and reactive behaviors were more fundamental to intelligence than abstract reasoning.\n\nBrooks' robots, including Genghis (a hexapod robot) and later planetary rovers, demonstrated impressive robustness compared to earlier systems. They navigated cluttered, unstructured environments and gracefully handled sensor noise and unexpected obstacles. The subsumption architecture influenced a generation of roboticists and contributed to the embodied AI perspective discussed in Section 1.1.\n\n**Limitations of Pure Reactivity**: However, purely reactive approaches also had limitations. They struggled with tasks requiring long-term planning, abstract reasoning, or learning complex skills. A purely reactive robot could navigate a cluttered room but couldn't assemble furniture or engage in multi-step manipulation. The field eventually recognized that both reactive and deliberative capabilities were necessary, leading to hybrid architectures combining fast reactive behaviors with slower deliberative planning.\n\n### 1.3.3 Humanoid Robotics Emerges (1990s-2000s)\n\nWhile early robotics focused primarily on mobile platforms or industrial manipulators, the 1990s saw growing interest in humanoid robots—systems designed to approximate human morphology and capabilities.\n\n**Why Humanoid Robots?** Several motivations drove humanoid robotics research:\n\n1. **Human-Centered Environments**: Human buildings, tools, and infrastructure are designed for human body proportions and capabilities. A humanoid robot could potentially use existing tools and navigate existing spaces without environmental modification.\n\n2. **Social Interaction**: Human-like form facilitates natural human-robot interaction. Humans intuitively understand humanoid gestures, expressions, and movements.\n\n3. **Scientific Understanding**: Building humanoid robots serves as a test of our understanding of human biomechanics, cognition, and sensorimotor control.\n\n4. **Versatility**: The human body is remarkably versatile, capable of locomotion across diverse terrains, manipulation of countless objects, and a vast repertoire of motor skills. A robot approaching human capabilities could be similarly versatile.\n\n**Early Humanoids**: Notable early humanoid robots included:\n\n- **Honda P-Series and ASIMO** (1993-2000): Honda's humanoid program, beginning with the P1 prototype in 1993, culminated in ASIMO (Advanced Step in Innovative Mobility), unveiled in 2000. ASIMO demonstrated stable biped walking, stair climbing, and basic manipulation, representing a major engineering achievement (Siciliano & Khatib, 2016).\n\n- **Sony QRIO** (2003): Sony developed QRIO, a small humanoid capable of running, dancing, and social interaction. QRIO emphasized fluid, dynamic motion and human-robot interaction rather than task performance.\n\n- **Waseda University WL-Series** (1973-present): Waseda University in Japan pioneered humanoid research with the WABOT series starting in 1973, developing successive generations exploring biped locomotion, manipulation, and human interaction.\n\nThese early humanoids demonstrated the feasibility of stable biped locomotion and basic manipulation but were far from general-purpose capabilities. Walking was slow and fragile; manipulation was limited to carefully staged demonstrations. Energy efficiency was poor, and robustness to disturbances was minimal.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.3 Historical Development of Physical AI",
        "subsection": null,
        "page_number": null,
        "chunk_index": 8,
        "word_count": 498,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "These early humanoids demonstrated the feasibility of stable biped locomotion and basic manipulation but were far from general-purpose capabilities. Walking was slow and fragile; manipulation was limited to carefully staged demonstrations. Energy efficiency was poor, and robustness to disturbances was minimal.\n\n**Perception and Control Challenges**: Early humanoids revealed profound challenges in perception and control. Biped balance requires real-time feedback and rapid control adjustments. Manipulation requires coordinating many degrees of freedom while processing uncertain visual and tactile feedback. These challenges drove advances in control theory, sensor fusion, and real-time computing (Siciliano & Khatib, 2016).\n\n### 1.3.4 Modern Humanoids and the Age of Learning (2010s-Present)\n\nThe 2010s and 2020s have witnessed dramatic advances in humanoid robotics driven by several converging trends: advances in machine learning (especially deep learning and reinforcement learning), improved sensors and actuators, increased computational power, and substantial industrial investment.\n\n**Boston Dynamics Atlas**: Perhaps the most iconic modern humanoid, Boston Dynamics' Atlas robot (introduced in 2013 for the DARPA Robotics Challenge) represents the state of the art in dynamic locomotion and whole-body control. Atlas has demonstrated capabilities that seemed futuristic only years ago:\n\n- **Dynamic Locomotion**: Walking, running, and jumping across rough terrain, including backflips and parkour sequences\n- **Robust Balance**: Recovering from pushes, slips, and other disturbances\n- **Whole-Body Manipulation**: Lifting and moving heavy objects, using tools\n- **Perception-Based Navigation**: Navigating complex, unstructured environments using vision and lidar\n\nAtlas's capabilities rest on sophisticated optimization-based control algorithms that compute motor commands at high rates (100+ Hz) by solving constrained optimization problems in real-time (Kuindersma et al., 2016). These controllers plan footsteps, body trajectories, and joint motions simultaneously while respecting physical constraints (friction limits, joint limits, balance constraints). We will explore Atlas in detail in Chapter 9.\n\n**Tesla Optimus**: Tesla's entry into humanoid robotics, Optimus (unveiled in 2022), represents a different approach: end-to-end learning from human demonstrations. Rather than hand-engineering control algorithms, Tesla aims to learn manipulation and locomotion skills from large datasets of human teleoperation, similar to how Tesla trains autonomous driving systems from fleet data. Optimus emphasizes manufacturability and cost reduction, targeting eventual mass production for industrial and domestic applications. We will examine Optimus in detail in Chapter 10.\n\n**Research Platforms**: Academic and industry research labs use sophisticated humanoid platforms including:\n\n- **PAL Robotics TALOS**: A torque-controlled humanoid designed for research in locomotion, manipulation, and human-robot collaboration (Stasse et al., 2017)\n- **Italian Institute of Technology iCub**: A child-sized humanoid emphasizing cognitive development, learning, and human interaction (Metta et al., 2010)\n- **NASA Valkyrie**: A humanoid designed for operations in hazardous environments\n\nThese platforms enable researchers worldwide to explore algorithms for perception, control, learning, and human-robot interaction without building custom hardware. We will discuss these platforms in Chapter 11.\n\n**Machine Learning Integration**: Modern humanoids increasingly leverage machine learning, particularly deep reinforcement learning, to acquire complex skills:",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.3 Historical Development of Physical AI",
        "subsection": null,
        "page_number": null,
        "chunk_index": 9,
        "word_count": 467,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "These platforms enable researchers worldwide to explore algorithms for perception, control, learning, and human-robot interaction without building custom hardware. We will discuss these platforms in Chapter 11.\n\n**Machine Learning Integration**: Modern humanoids increasingly leverage machine learning, particularly deep reinforcement learning, to acquire complex skills:\n\n- **End-to-End Visuomotor Policies**: Training policies that map directly from camera images to motor commands, bypassing hand-engineered perception and planning modules (Levine et al., 2016)\n- **Imitation Learning**: Learning from human demonstrations via teleoperation or motion capture\n- **Sim-to-Real Transfer**: Training policies in simulation with domain randomization, then deploying them on real robots (Tobin et al., 2017)\n\nThese learning approaches complement traditional control methods, with hybrid systems using learning for perception and high-level skills while retaining model-based control for critical low-level functions like balance.\n\n### 1.3.5 The Current State and Future Trajectory\n\nAs of 2025, physical AI—and humanoid robotics specifically—stands at an inflection point. We have robots capable of impressive demonstrations: dynamic parkour, robust locomotion, dexterous manipulation in structured settings. Yet we lack robots capable of open-ended, general-purpose operation in human environments. Several factors will shape the next decade:\n\n**Scaling Laws**: Will data-driven learning scale to general-purpose physical AI as it has for language models? Can we collect and leverage massive datasets of physical interaction?\n\n**Hardware Advances**: Improved actuators (compact, powerful, energy-efficient), better sensors (robust vision, tactile sensing), and edge computing may remove current hardware bottlenecks.\n\n**Integration Challenges**: Combining perception, planning, control, and learning into robust, safe, general-purpose systems remains a profound systems engineering challenge.\n\n**Economic Drivers**: Labor shortages, dangerous work environments, and aging populations create strong economic incentives for capable physical AI systems, driving investment and research.\n\nUnderstanding this historical progression—from the symbolic reasoning of SHAKEY through the reactive revolution to modern learning-based systems—provides essential context for the technical material in subsequent chapters.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.3 Historical Development of Physical AI",
        "subsection": null,
        "page_number": null,
        "chunk_index": 10,
        "word_count": 298,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "### 1.4.1 Formulating the Paradox\n\nIn the 1980s, Hans Moravec, a roboticist at Carnegie Mellon University, observed a striking pattern in AI and robotics research:\n\n> \"It is comparatively easy to make computers exhibit adult-level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility.\" (Moravec, 1988, cited in Siciliano & Khatib, 2016)\n\nThis observation, now known as Moravec's paradox, captures a fundamental inversion in computational difficulty: Tasks that humans find cognitively demanding—abstract reasoning, complex calculations, logical deduction, game-playing—prove relatively tractable for AI systems. Conversely, tasks that humans (and animals) perform effortlessly and unconsciously—walking, grasping objects, recognizing faces, navigating cluttered environments—prove extraordinarily difficult for AI and robotics.\n\n**Counterintuitive Difficulty**: Consider these contrasts:\n\n- Deep Blue defeated world chess champion Garry Kasparov in 1997, yet no robot in 1997 could reliably set up a chessboard.\n- AlphaGo defeated world Go champion Lee Sedol in 2016, yet no robot could match a five-year-old child in stacking blocks.\n- GPT-4 can write coherent essays on quantum mechanics, yet struggles to reason about physical causality (e.g., what happens if you stack a book on top of an egg).\n\n### 1.4.2 Evolutionary Explanations\n\nWhy are sensorimotor skills so much harder than abstract reasoning? Moravec's explanation appeals to evolutionary timescales and computational complexity.\n\n**Evolutionary Perspective**: Abstract reasoning—the kind tested by IQ tests and exemplified by mathematics, logic, and language—is evolutionarily recent. Written language emerged only ~5,000 years ago; formal mathematics even more recently. Natural selection has had minimal time to optimize human brains for these tasks.\n\nIn contrast, sensorimotor skills—vision, locomotion, manipulation, spatial navigation—have been under intense evolutionary pressure for hundreds of millions of years. Every mobile animal must solve these problems to survive. Consequently, biological brains have evolved extraordinarily sophisticated, specialized circuits for perception and motor control (Pfeifer & Bongard, 2006).\n\n**Computational Complexity**: Moravec argued that sensorimotor skills appear easy to us precisely because they rely on massively parallel, unconscious processing by specialized neural circuits refined over evolutionary time. Abstract reasoning, being more recent, relies on slow, serial, conscious processing. The computational work required for sensorimotor skills is enormous—far exceeding that required for chess or theorem-proving—but it's performed unconsciously by dedicated neural hardware, making it feel effortless.\n\nConsider visual object recognition: The human visual cortex contains billions of neurons forming hierarchical processing stages that extract edges, textures, shapes, and object categories from raw retinal input at millisecond timescales. Replicating this with conventional algorithms stumped AI researchers for decades. Only with deep convolutional neural networks, which mimic the hierarchical structure of the visual cortex, did performance approach human levels—and these networks require millions of training examples and substantial computation (Levine et al., 2016).\n\n### 1.4.3 Implications for Robot Design and Development Priorities\n\nMoravec's paradox has profound implications for how we approach physical AI and humanoid robotics:",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.4 Moravec's Paradox and Implications for Robot Design",
        "subsection": null,
        "page_number": null,
        "chunk_index": 11,
        "word_count": 476,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "### 1.4.3 Implications for Robot Design and Development Priorities\n\nMoravec's paradox has profound implications for how we approach physical AI and humanoid robotics:\n\n**Development Priorities**: The paradox suggests that enabling robots to perform \"simple\" physical tasks—walking reliably, manipulating diverse objects, perceiving cluttered environments—should be a primary focus, potentially more so than high-level reasoning. A robot capable of robust, general-purpose physical interaction can be given reasoning capabilities; a robot that cannot navigate or manipulate is useless regardless of its reasoning ability.\n\n**Respect for \"Low-Level\" Skills**: The paradox elevates the status of perception, motor control, and sensorimotor integration from mere implementation details to central scientific and engineering challenges. These are not simple input-output functions to be dispatched quickly; they are core problems deserving sustained research attention.\n\n**Learning and Data Requirements**: If sensorimotor skills are computationally intensive, we should expect that learning these skills requires substantial data and computation—much as visual recognition required massive labeled image datasets (ImageNet) and deep networks. This insight motivates efforts to collect large-scale robot interaction datasets and train models via simulation (Levine et al., 2016; Tobin et al., 2017).\n\n**Specialization and Morphological Computation**: The paradox reinforces the importance of specialized hardware and morphological computation. Just as biological systems use dedicated neural circuits and exploit biomechanics, robots can benefit from specialized actuators, sensors, and mechanical designs that simplify control problems. For instance, compliant actuators (mimicking muscle elasticity) can provide passive stability, reducing computational control demands (Pfeifer & Bongard, 2006).\n\n**Hybrid Architectures**: The paradox supports hybrid architectures combining learned sensorimotor skills with symbolic reasoning. Reactive, learned controllers handle perception and low-level motor control—the \"hard\" problems. Higher-level planning and reasoning, being \"easier\" computationally, can use more conventional AI methods.\n\n**Humility and Long-Term Perspective**: Finally, Moravec's paradox counsels humility. Tasks that appear simple often hide profound complexity. General-purpose humanoid robotics remains a long-term challenge precisely because it requires solving many Moravec-hard problems simultaneously: robust vision, dexterous manipulation, dynamic locomotion, and their integration. Achieving human-level performance in these domains may require decades of research, specialized hardware, and massive datasets.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.4 Moravec's Paradox and Implications for Robot Design",
        "subsection": null,
        "page_number": null,
        "chunk_index": 12,
        "word_count": 336,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "Having established what physical AI is, how it differs from disembodied AI, its historical development, and the paradoxical difficulty of sensorimotor skills, we now survey the key technical challenges that embodiment introduces. These challenges will be explored in depth in subsequent chapters; here we provide an overview.\n\n### 1.5.1 Perception Under Uncertainty\n\nPhysical AI systems must infer environmental state from sensor measurements that are inevitably noisy, incomplete, and sometimes contradictory.\n\n**Sensor Noise and Failure**: All physical sensors introduce noise. Cameras capture blur, motion artifacts, lighting variations, and occlusions. Lidar measurements are corrupted by reflections and material properties. Tactile sensors drift and saturate. Perception algorithms must be robust to these imperfections (Cadena et al., 2016).\n\n**Partial Observability**: Unlike game-playing AIs with complete state knowledge, robots observe only a tiny fraction of the world at any moment. A robot in a building cannot see through walls, doesn't know the location of objects in closed drawers, and has limited fields of view. Perception requires integrating observations over time to build coherent estimates of world state—a process fraught with uncertainty (Thrun et al., 2005).\n\n**Dynamic Environments**: Environments change while robots act. People move, doors open and close, lighting varies, objects are displaced. Perception systems must track these changes, distinguish stable from transient features, and update beliefs accordingly.\n\n**Multi-Modal Integration**: Robots typically have many sensors—cameras, depth sensors, force sensors, proprioceptive sensors (joint encoders, IMUs). Integrating these heterogeneous data sources into coherent state estimates requires sophisticated sensor fusion techniques, such as Kalman filtering or particle filters (Thrun et al., 2005). We will explore perception and sensor fusion in Chapter 4.\n\n### 1.5.2 Real-Time Control and Stability\n\nPhysical systems have continuous dynamics governed by physics. Controlling these systems requires generating appropriate motor commands at high rates under hard real-time constraints.\n\n**Feedback Control**: Unlike open-loop systems that execute pre-planned commands, physical robots require continuous feedback control—measuring state, comparing to desired state, computing corrective actions. For humanoid robots, balance control operates at hundreds of Hertz; slower control rates lead to instability and falls (Siciliano & Khatib, 2016).\n\n**High-Dimensional Control**: A humanoid robot may have 30+ degrees of freedom (joints). Controlling all these joints simultaneously to achieve whole-body motions—walking, reaching, manipulating—requires solving high-dimensional optimization problems in real-time. This is computationally intensive and requires efficient algorithms (Kuindersma et al., 2016).\n\n**Stability and Safety**: Physical systems can become unstable—a robot can fall, collide, or damage itself or its environment. Control algorithms must guarantee stability under disturbances and ensure safe operation even when perception or planning fails. Safety constraints (avoiding collisions, respecting joint limits, maintaining balance) must be enforced continuously (Haddadin et al., 2017).\n\n**Model Uncertainty**: Control often relies on models of robot dynamics—how forces and torques translate to motion. But models are never perfect; parameters like mass, friction, and actuator response are uncertain and vary. Robust control must account for model uncertainty. We will explore control architectures in Chapter 6.\n\n### 1.5.3 Planning in Continuous, High-Dimensional Spaces",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.5 Key Challenges of Embodiment",
        "subsection": null,
        "page_number": null,
        "chunk_index": 13,
        "word_count": 485,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "### 1.5.3 Planning in Continuous, High-Dimensional Spaces\n\nPhysical robots inhabit continuous configuration spaces. Planning motions or action sequences in these spaces is computationally challenging.\n\n**Configuration Space Complexity**: A humanoid robot's configuration space has 30+ dimensions (one per joint). A 6-DOF manipulator has a 6D configuration space. Planning paths in such high-dimensional spaces using naive grid-based methods is intractable due to the curse of dimensionality (LaValle, 2006).\n\n**Kinodynamic Constraints**: Plans must respect not only geometric constraints (collision avoidance) but also kinodynamic constraints—velocity limits, acceleration limits, balance constraints. A humanoid cannot teleport between configurations; it must maintain balance throughout a motion. These constraints couple spatial and temporal planning (LaValle & Kuffner, 2001).\n\n**Replanning Under Uncertainty**: Environments are dynamic and partially observable. Initial plans often become invalid as new information arrives or disturbances occur. Robots need replanning capabilities—updating plans efficiently in real-time as conditions change.\n\n**Sampling-Based Planning**: Modern motion planning algorithms use sampling-based methods (Rapidly-exploring Random Trees, Probabilistic Roadmaps) that efficiently explore high-dimensional spaces by randomly sampling configurations and building graphs or trees. These methods trade completeness for computational tractability (Karaman & Frazzoli, 2011). We will explore motion planning in Chapter 5.\n\n### 1.5.4 Learning from Physical Interaction\n\nMachine learning has revolutionized many AI domains, but learning in physical systems introduces unique challenges.\n\n**Sample Efficiency**: Physical robots cannot train for millions of episodes as simulated agents can. Collecting data on real robots is slow, expensive, and risky. Learning algorithms for robotics must be far more sample-efficient than those for game-playing or language modeling (Deisenroth et al., 2013).\n\n**Safety During Learning**: Exploration is essential for learning, but robots exploring freely can damage themselves, their environment, or harm humans. Safe exploration—learning while respecting safety constraints—is a critical challenge (Haddadin et al., 2017).\n\n**Credit Assignment**: When a robot executes a long sequence of actions and eventually succeeds or fails at a task, determining which actions were responsible (the credit assignment problem) is difficult. Sparse rewards—feedback only at task completion—make credit assignment particularly challenging in robotics.\n\n**Sim-to-Real Transfer**: To overcome sample efficiency limitations, robots often train in simulation. But simulation differs from reality (the reality gap). Transferring policies learned in simulation to real robots requires techniques like domain randomization, which trains policies robust to simulation-reality discrepancies (Tobin et al., 2017).\n\n**Integration with Control**: Learned policies must integrate with low-level controllers. Often, learning occurs at high levels (what to do) while control handles low levels (how to do it). Designing interfaces between learned and engineered components is a systems challenge. We will explore machine learning for robotics in Chapter 7.\n\n### 1.5.5 Human-Robot Interaction and Safety\n\nHumanoid robots designed to operate in human environments must interact safely and intuitively with people.\n\n**Physical Safety**: Robots are heavy, powerful machines. Collisions can cause injury. Robots must detect imminent collisions, reduce impact forces, and stop safely when contact occurs. This requires advanced sensors (e.g., whole-body tactile sensing) and collision-aware control (Haddadin et al., 2017).",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.5 Key Challenges of Embodiment",
        "subsection": null,
        "page_number": null,
        "chunk_index": 14,
        "word_count": 483,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "**Physical Safety**: Robots are heavy, powerful machines. Collisions can cause injury. Robots must detect imminent collisions, reduce impact forces, and stop safely when contact occurs. This requires advanced sensors (e.g., whole-body tactile sensing) and collision-aware control (Haddadin et al., 2017).\n\n**Intent Recognition and Prediction**: To collaborate effectively, robots must infer human intent from observations—predicting what a person will do next to plan complementary actions. This requires modeling human behavior, which is complex and variable.\n\n**Natural Interfaces**: Humans should be able to instruct and correct robots using natural communication—language, gestures, demonstrations. Developing robust natural language understanding and gesture recognition for human-robot interaction is an active research area.\n\n**Social Acceptability**: Beyond functional interaction, robots in human spaces must behave in socially acceptable ways—respecting personal space, moving predictably, exhibiting appropriate nonverbal cues. Social robotics studies these factors (Hoffman & Breazeal, 2007). We will explore human-robot interaction in Chapter 8.\n\n### 1.5.6 Energy and Autonomy\n\nUnlike simulated agents or tethered systems, autonomous robots have limited on-board energy.\n\n**Power Constraints**: Humanoid robots consume substantial power—hundreds of watts or more during dynamic motion. Battery capacity limits operation time, often to 1-2 hours for current systems. Energy-efficient actuation, computation, and operation strategies are essential for practical autonomy.\n\n**Recharging and Self-Maintenance**: For long-term autonomy, robots must locate and use charging stations autonomously. More ambitiously, they might eventually perform self-maintenance—detecting worn components and seeking repairs.\n\n**Computational Resources**: On-board computing is limited by power, weight, and heat dissipation. Complex perception and control algorithms must run on embedded computers with limited CPU, GPU, and memory. Offloading computation to remote servers introduces latency, which is problematic for real-time control. Balancing computational capability with energy constraints is a persistent challenge (Siciliano & Khatib, 2016).\n\n### 1.5.7 Integration and Systems Engineering\n\nPerhaps the most profound challenge is integration—combining perception, planning, control, learning, and human interaction into a cohesive, reliable system.\n\n**Modularity vs. End-to-End Learning**: Traditional robotics uses modular architectures—separate perception, planning, and control modules. This modularity simplifies development and debugging but can introduce information bottlenecks and compounding errors. End-to-end learning, conversely, trains unified policies but sacrifices interpretability and is data-intensive (Levine et al., 2016).\n\n**Fault Tolerance and Graceful Degradation**: Physical robots must handle component failures gracefully. If a sensor fails, can the robot continue with reduced capabilities? If a joint seizes, can it adapt? Building fault-tolerant systems requires redundancy, monitoring, and adaptive strategies.\n\n**Real-World Deployment**: Transitioning from laboratory demonstrations to real-world deployment introduces unforeseen challenges—lighting variation, unexpected obstacles, human behavior, communication failures. Robust systems engineering, extensive testing, and iterative refinement are essential.\n\nThese challenges are deeply interrelated. Progress in perception enables better planning; advances in learning improve control; safe human interaction requires robust perception and predictive control. The following chapters explore each of these areas in depth, providing the theoretical foundations, algorithmic techniques, and practical considerations necessary to address these challenges.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.5 Key Challenges of Embodiment",
        "subsection": null,
        "page_number": null,
        "chunk_index": 15,
        "word_count": 467,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "To ground the concepts introduced in this chapter, we briefly compare SHAKEY (1966-1972) with Tesla Optimus (2022-present), illustrating how the field has evolved over five decades.\n\n**SHAKEY (1966-1972)**:\n- **Computation**: Off-board mainframe computers (DEC PDP-10, PDP-15); tasks required minutes of computation\n- **Perception**: Monochrome camera, range finder; simple geometric environments only\n- **Mobility**: Wheeled platform, slow and deliberate movement\n- **Control Paradigm**: Sense-plan-act; symbolic reasoning over explicit world models\n- **Capabilities**: Navigate simple rooms, push blocks, plan action sequences\n- **Limitations**: Slow, brittle, required carefully engineered environments, no learning\n\n**Tesla Optimus (2022-present)**:\n- **Computation**: On-board embedded computers with GPU acceleration; real-time operation\n- **Perception**: Multiple cameras, depth sensors, possibly lidar; operates in unstructured environments\n- **Mobility**: Biped locomotion with dynamic balance; human-like form factor\n- **Control Paradigm**: Hybrid—end-to-end learned visuomotor policies for high-level tasks, model-based control for low-level balance and actuation\n- **Capabilities**: Biped walking, manipulation of diverse objects, learning from human demonstrations\n- **Limitations**: Still in development; robustness, generalization, and long-term autonomy remain challenges\n\n**Key Differences**: The transformation from SHAKEY to Optimus reflects several fundamental shifts:\n\n1. **From Symbolic to Subsymbolic**: SHAKEY relied on symbolic AI—explicit representations, logical reasoning, search-based planning. Optimus relies heavily on neural networks and learned representations—subsymbolic, data-driven approaches.\n\n2. **From Engineered to Learned**: SHAKEY's behaviors were hand-engineered by expert programmers. Optimus learns many behaviors from data—human demonstrations, simulated experience, trial and error.\n\n3. **From Static to Dynamic**: SHAKEY operated in static, carefully controlled environments. Optimus targets dynamic, unstructured human environments.\n\n4. **From Computation-Limited to Data-Limited**: SHAKEY was limited by computational power; even simple tasks required extensive computation. Optimus has ample computation but is limited by data—collecting diverse, high-quality robot interaction data remains a bottleneck.\n\n5. **From Single-Task to Multi-Task**: SHAKEY performed narrow, predefined tasks. Optimus aims for general-purpose capabilities, learning a repertoire of skills applicable to diverse tasks.\n\nThis comparison illustrates the extraordinary progress in physical AI over five decades while also highlighting persistent challenges. Despite advances, truly general-purpose, robust, long-term autonomous humanoid robots remain aspirational.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.6 Case Study: From SHAKEY to Optimus—A Comparative Perspective",
        "subsection": null,
        "page_number": null,
        "chunk_index": 16,
        "word_count": 331,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "1. **Embodied Cognition Analysis** (Difficulty: Beginner, Type: Conceptual):\n\nConsider a household service robot tasked with setting a table for dinner. Describe three specific ways in which embodied cognition principles (morphological computation, sensorimotor coupling, situatedness, or environmental scaffolding) could be applied to this task. For each principle, explain how it would reduce computational demands compared to a purely disembodied planning approach.\n\n2. **Physical AI vs. Disembodied AI** (Difficulty: Intermediate, Type: Conceptual):\n\nSelect a task domain where both physical AI and disembodied AI have achieved success (e.g., game-playing for disembodied AI and locomotion for physical AI). Compare and contrast the two domains along the following dimensions:\n   - State representation and observability\n   - Action consequences and reversibility\n   - Evaluation and success metrics\n   - Safety considerations\n   - Sample efficiency for learning\n\nDiscuss why methods successful in one domain may not transfer directly to the other.\n\n3. **Moravec's Paradox in Modern AI** (Difficulty: Intermediate, Type: Conceptual):\n\nLarge language models (LLMs) such as GPT-4 can perform impressive abstract reasoning tasks—writing code, explaining scientific concepts, solving logic puzzles. Yet they struggle with physical common sense—reasoning about what happens if you stack a book on an egg, or predicting the trajectory of a thrown ball.\n\na) Explain this phenomenon in terms of Moravec's paradox.\n\nb) Propose and justify an approach to give LLMs better physical common sense. Should it involve embodied interaction, physics simulation, structured knowledge, or some combination?\n\n4. **Historical Development Timeline** (Difficulty: Beginner, Type: Research):\n\nCreate a timeline of major milestones in physical AI and humanoid robotics from 1960 to 2025. For each milestone, briefly note:\n   - The system or achievement\n   - Key technical innovations\n   - Limitations at the time\n\nInclude at least 8-10 milestones spanning the full period. Cite sources for each milestone using APA format.\n\n5. **Challenge Analysis and Prioritization** (Difficulty: Advanced, Type: Conceptual):\n\nSection 1.5 identified seven key challenges of embodiment: perception under uncertainty, real-time control, planning in high-dimensional spaces, learning from physical interaction, human-robot interaction, energy and autonomy, and integration/systems engineering.\n\nConsider a specific application domain (e.g., warehouse logistics, elder care, disaster response, or agricultural automation). For your chosen domain:\n\na) Rank these seven challenges from most critical to least critical for success in that domain. Justify your ranking.\n\nb) For the top three challenges in your ranking, propose a concrete technical approach to address each challenge, citing relevant work from the chapter's references.\n\nc) Discuss potential tradeoffs between your proposed approaches (e.g., between safety and performance, or between autonomy and complexity).",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 17,
        "word_count": 410,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "This chapter introduced the foundational concepts of physical AI and humanoid robotics, establishing the context and key challenges for the remainder of the book. We explored:\n\n- **Embodied Cognition**: Intelligence is not abstract symbol manipulation but deeply rooted in sensorimotor interaction with the physical world. Principles such as morphological computation, sensorimotor coupling, and situatedness inform how we design physical AI systems (Brooks, 1991; Pfeifer & Bongard, 2006).\n\n- **Physical AI Defined**: Physical AI systems have bodies, sensors, actuators, and must achieve goals through physical interaction. This embodiment introduces challenges absent in disembodied AI—uncertain perception, real-time constraints, irreversible actions, safety considerations, and continuous high-dimensional state-action spaces.\n\n- **Physical vs. Disembodied AI**: The two paradigms differ fundamentally in state observability, action consequences, temporal constraints, safety requirements, and problem dimensionality. Understanding these differences is essential for designing physical AI systems and for appreciating why techniques from disembodied AI cannot be applied naively to robotics.\n\n- **Historical Development**: The field evolved from early symbolic systems like SHAKEY through the reactive revolution led by Brooks to modern learning-based humanoids like Atlas and Optimus. This progression reflects shifts from symbolic to subsymbolic processing, from engineered to learned behaviors, and from narrow to increasingly general-purpose capabilities (Siciliano & Khatib, 2016).\n\n- **Moravec's Paradox**: High-level reasoning is computationally easier than low-level sensorimotor skills—an inversion of intuitive difficulty. This paradox reflects evolutionary timescales and the enormous computational complexity hidden in \"simple\" perceptual and motor tasks. It has profound implications for development priorities and architectural choices in robotics.\n\n- **Key Challenges**: Embodiment introduces multifaceted challenges—perception under uncertainty, real-time control, high-dimensional planning, sample-efficient learning, safe human interaction, energy constraints, and systems integration. Subsequent chapters address each challenge in depth, providing theoretical foundations and practical techniques.\n\nAs we proceed through the book, remember that physical AI is not merely AI \"with a body added.\" Embodiment fundamentally transforms the nature of intelligence, introducing constraints, opportunities, and complexities that shape every aspect of system design. The goal of this book is to equip you with the conceptual foundations, algorithmic techniques, and practical insights needed to build physical AI systems—particularly humanoid robots—capable of robust, safe, and versatile operation in human environments.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 18,
        "word_count": 356,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "- **Brooks, R. A. (1991)**. Intelligence without representation. *Artificial Intelligence*, 47(1-3), 139-159. — Seminal paper arguing for reactive, behavior-based robotics over symbolic planning.\n\n- **Pfeifer, R., & Bongard, J. C. (2006)**. *How the body shapes the way we think: A new view of intelligence*. MIT Press. — Comprehensive exploration of embodied cognition and its implications for AI and robotics.\n\n- **Siciliano, B., & Khatib, O. (Eds.). (2016)**. *Springer handbook of robotics* (2nd ed.). Springer. — Authoritative, comprehensive reference covering all aspects of robotics; excellent for deep dives into specific topics.\n\n- **Deisenroth, M. P., Neumann, G., & Peters, J. (2013)**. A survey on policy search for robotics. *Foundations and Trends in Robotics*, 2(1-2), 1-142. — Thorough survey of learning approaches for robot control, essential for understanding Chapter 7 material.\n\n- **Kuindersma, S., et al. (2016)**. Optimization-based locomotion planning, estimation, and control design for the Atlas humanoid robot. *Autonomous Robots*, 40(3), 429-455. — Detailed technical description of Atlas's control architecture, preview of Chapter 9 case study.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "Further Reading",
        "subsection": null,
        "page_number": null,
        "chunk_index": 19,
        "word_count": 165,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "Brooks, R. A. (1991). Intelligence without representation. *Artificial Intelligence*, 47(1-3), 139-159. https://doi.org/10.1016/0004-3702(91)90053-M\n\nCadena, C., Carlone, L., Carrillo, H., Latif, Y., Scaramuzza, D., Neira, J., Reid, I., & Leonard, J. J. (2016). Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age. *IEEE Transactions on Robotics*, 32(6), 1309-1332. https://doi.org/10.1109/TRO.2016.2624754\n\nDai, H., Valenzuela, A., & Tedrake, R. (2014). Whole-body motion planning with centroidal dynamics and full kinematics. *IEEE-RAS International Conference on Humanoid Robots*, 295-302. https://doi.org/10.1109/HUMANOIDS.2014.7041375\n\nDeisenroth, M. P., Neumann, G., & Peters, J. (2013). A survey on policy search for robotics. *Foundations and Trends in Robotics*, 2(1-2), 1-142. https://doi.org/10.1561/2300000021\n\nHaddadin, S., De Luca, A., & Albu-Schäffer, A. (2017). Robot collisions: A survey on detection, isolation, and identification. *IEEE Transactions on Robotics*, 33(6), 1292-1312. https://doi.org/10.1109/TRO.2017.2723903\n\nHoffman, G., & Breazeal, C. (2007). Cost-based anticipatory action selection for human-robot fluency. *IEEE Transactions on Robotics*, 23(5), 952-961. https://doi.org/10.1109/TRO.2007.907483\n\nKaraman, S., & Frazzoli, E. (2011). Sampling-based algorithms for optimal motion planning. *The International Journal of Robotics Research*, 30(7), 846-894. https://doi.org/10.1177/0278364911406761\n\nKuindersma, S., Deits, R., Fallon, M., Valenzuela, A., Dai, H., Permenter, F., Koolen, T., Marion, P., & Tedrake, R. (2016). Optimization-based locomotion planning, estimation, and control design for the Atlas humanoid robot. *Autonomous Robots*, 40(3), 429-455. https://doi.org/10.1007/s10514-015-9479-3\n\nLaValle, S. M. (2006). *Planning algorithms*. Cambridge University Press. https://doi.org/10.1017/CBO9780511546877\n\nLaValle, S. M., & Kuffner, J. J. (2001). Randomized kinodynamic planning. *The International Journal of Robotics Research*, 20(5), 378-400. https://doi.org/10.1177/02783640122067453\n\nLevine, S., Finn, C., Darrell, T., & Abbeel, P. (2016). End-to-end training of deep visuomotor policies. *Journal of Machine Learning Research*, 17(1), 1334-1373.\n\nMetta, G., Natale, L., Nori, F., Sandini, G., Vernon, D., Fadiga, L., Von Hofsten, C., Rosander, K., Lopes, M., Santos-Victor, J., Bernardino, A., & Montesano, L. (2010). The iCub humanoid robot: An open-systems platform for research in cognitive development. *Neural Networks*, 23(8-9), 1125-1134. https://doi.org/10.1016/j.neunet.2010.08.010\n\nPfeifer, R., & Bongard, J. C. (2006). *How the body shapes the way we think: A new view of intelligence*. MIT Press.\n\nSiciliano, B., & Khatib, O. (Eds.). (2016). *Springer handbook of robotics* (2nd ed.). Springer. https://doi.org/10.1007/978-3-319-32552-1\n\nStasse, O., Flayols, T., Budhiraja, R., Giraud-Esclasse, K., Carpentier, J., Del Prete, A., Saurel, G., Mansard, N., Lamiraux, F., Laumond, J.-P., Marchionni, L., Tome, H., & Ferro, F. (2017). TALOS: A new humanoid research platform targeted for industrial applications. *IEEE-RAS International Conference on Humanoid Robots*, 689-695. https://doi.org/10.1109/HUMANOIDS.2017.8246947\n\nThrun, S., Burgard, W., & Fox, D. (2005). *Probabilistic robotics*. MIT Press.\n\nTobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., & Abbeel, P. (2017). Domain randomization for transferring deep neural networks from simulation to the real world. *IEEE/RSJ International Conference on Intelligent Robots and Systems*, 23-30. https://doi.org/10.1109/IROS.2017.8202133\n\n---\n\n:::tip\nThe chatbot provides answers grounded in the book content with source references. Try asking questions like:\n- \"What is Moravec's paradox?\"\n- \"How does embodied cognition differ from traditional AI?\"\n- \"What are the main challenges in physical AI?\"\n:::",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 1,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "References",
        "subsection": null,
        "page_number": null,
        "chunk_index": 20,
        "word_count": 475,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "---\nsidebar_position: 1\ntitle: \"Chapter 1: Introduction to Physical AI\"\ndescription: Foundational concepts, history, and current state of Physical AI\ntags: [physical-ai, robotics-history, humanoid-robotics]\n---\n\n# Chapter 1: Introduction to Physical AI",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 2,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 32,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "By the end of this chapter, you will:\n1. Define Physical AI and distinguish it from traditional AI\n2. Understand the historical evolution from industrial robots to humanoids\n3. Identify key players, current capabilities, and limitations",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 2,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 36,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "**Physical AI** = AI systems with physical bodies that interact with the real world through sensors and actuators.\n\n```mermaid\ngraph TB\n    subgraph Environment[\"Physical Environment\"]\n        Objects[Objects & Obstacles]\n        Terrain[Terrain & Surfaces]\n        Humans[Humans & Agents]\n    end\n\n    subgraph Robot[\"Physical AI System\"]\n        Sensors[Sensors<br/>Cameras, LiDAR, IMU, Tactile]\n        Perception[Perception<br/>Object detection, SLAM, State estimation]\n        Reasoning[Reasoning<br/>Planning, Decision-making, VLA]\n        Control[Control<br/>Motor control, Trajectory planning]\n        Actuators[Actuators<br/>Motors, Grippers, Joints]\n    end\n\n    Environment -->|Light, Forces, State| Sensors\n    Sensors -->|Sensor Data| Perception\n    Perception -->|World Model| Reasoning\n    Reasoning -->|Commands| Control\n    Control -->|Torques/Forces| Actuators\n    Actuators -->|Physical Action| Environment\n\n    style Robot fill:#e1f5ff\n    style Environment fill:#fff4e1\n```\n\n**Figure 1.1**: Physical AI system architecture showing the perception-reasoning-action loop embedded in a physical environment.\n\n```mermaid\ngraph TB\n    subgraph Environment[\"Physical Environment\"]\n        Objects[Objects & Obstacles]\n        Terrain[Terrain & Surfaces]\n        Humans[Humans & Agents]\n    end\n\n    subgraph Robot[\"Physical AI System\"]\n        Sensors[Sensors<br/>Cameras, LiDAR, IMU, Tactile]\n        Perception[Perception<br/>Object detection, SLAM, State estimation]\n        Reasoning[Reasoning<br/>Planning, Decision-making, VLA]\n        Control[Control<br/>Motor control, Trajectory planning]\n        Actuators[Actuators<br/>Motors, Grippers, Joints]\n    end\n\n    Environment -->|Light, Forces, State| Sensors\n    Sensors -->|Sensor Data| Perception\n    Perception -->|World Model| Reasoning\n    Reasoning -->|Commands| Control\n    Control -->|Torques/Forces| Actuators\n    Actuators -->|Physical Action| Environment\n\n    style Robot fill:#e1f5ff\n    style Environment fill:#fff4e1\n```\n\n**Figure 1.1**: Physical AI system architecture showing the perception-reasoning-action loop embedded in a physical environment.\n\n### Core Characteristics\n\nPhysical AI systems must:\n- **Perceive**: Process sensor data in real-time (cameras, LiDAR, IMUs, tactile)\n- **Reason**: Make decisions under uncertainty and partial observability\n- **Act**: Control physical actuators (motors, hydraulics, pneumatics)\n- **Learn**: Adapt through physical interaction (sim, demonstrations, RL)\n\n### Physical AI vs Traditional AI\n\n| Aspect | Traditional AI | Physical AI |\n|--------|---------------|-------------|\n| Environment | Digital | Physical world |\n| Time | Flexible | Real-time (ms) |\n| Safety | Low stakes | High stakes |\n| Embodiment | None | Critical |\n\n**Example**: ChatGPT processes text with no physical consequences. A humanoid must balance at 100Hz while avoiding obstacles—errors cause falls.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 2,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.1 What is Physical AI?",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 299,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "### 1960s: Programmable Automation\n- **Unimate** (1961): First industrial robot at GM\n- Fixed, pre-programmed, no sensors\n\n### 1970s-80s: Sensing and Planning  \n- **Shakey** (1966-72): First mobile robot with vision\n- **Stanford Cart** (1979): Vision-guided navigation\n- Too slow for dynamic environments\n\n### 1986: Subsumption Revolution\n- **Rodney Brooks**: Reactive behaviors > planning\n- No world model—direct sensor-to-actuator\n- *\"The world is its own best model\"*\n\n### 1990s-2000s: Probabilistic Robotics\n- SLAM, Kalman filters, particle filters\n- DARPA Grand Challenge (2005)\n\n### 2010s: Deep Learning\n- CNNs for vision (ImageNet 2012)\n- End-to-end learning, sim-to-real\n- Boston Dynamics Atlas (2013)\n\n### 2020s: VLA Models\n- RT-2, OpenVLA, SmolVLA\n- Web-scale + robot data\n- Humanoid renaissance (Figure, Tesla, 1X)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 2,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.2 Historical Context",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 119,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "### Capabilities (2024)\n\n**Locomotion**:\n- ✓ Flat terrain, stairs\n- ⚠ Rough terrain\n- ✗ Running with obstacles\n\n**Manipulation**:\n- ✓ Pick-and-place rigid objects\n- ⚠ Deformable objects\n- ✗ High-precision assembly\n\n**Autonomy**:\n- ✓ Scripted sequences\n- ⚠ Natural language tasks\n- ✗ Open-ended problems\n\n### Major Limitations\n\n1. **Energy**: 1 hour battery vs human all-day\n2. **Sim-to-Real**: Policies fail on real robots\n3. **Generalization**: Out-of-distribution failures\n4. **Safety**: Cannot verify learned policies\n5. **Cost**: `$100k-$500k+` (target `<$50k`)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 2,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "1.3 Current State",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 79,
        "has_code_block": false,
        "has_math": true,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "Physical AI = embodied intelligence confronting real-time, uncertainty, and physical constraints.\n\nEvolution: Industrial robots → planners → reactive → probabilistic → deep learning → VLAs\n\n**Next**: Chapter 2 explores why embodiment matters.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 2,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 32,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "**Exercise 1.1**: Compare and contrast a web-based chatbot with a humanoid robot. List three capabilities that are unique to physical AI systems and explain why embodiment is necessary for each.\n\n**Exercise 1.2**: Research one of the historical systems mentioned (Shakey, Stanford Cart, or Atlas). Write a brief report on its technical specifications, key innovations, and limitations at the time.\n\n**Exercise 1.3**: Identify a current limitation of Physical AI (from Section 1.3) and propose one research direction or engineering approach that could address it. Justify your proposal with technical reasoning.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 2,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 89,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "**Exercise 1.1**: Compare and contrast a web-based chatbot with a humanoid robot. List three capabilities that are unique to physical AI systems and explain why embodiment is necessary for each.\n\n**Exercise 1.2**: Research one of the historical systems mentioned (Shakey, Stanford Cart, or Atlas). Write a brief report on its technical specifications, key innovations, and limitations at the time.\n\n**Exercise 1.3**: Identify a current limitation of Physical AI (from Section 1.3) and propose one research direction or engineering approach that could address it. Justify your proposal with technical reasoning.\n\n---",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 2,
        "chapter_title": "Chapter 1: Introduction to Physical AI",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 7,
        "word_count": 90,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-introduction.md"
      }
    },
    {
      "content": "---\nsidebar_position: 2\ntitle: \"Chapter 2: Embodied Intelligence\"\ndescription: How physical bodies enable and shape intelligence\ntags: [embodied-cognition, morphological-computation, developmental-robotics]\n---\nnimport { ChatbotWidget } from '@site/src/components/ChatbotWidget';\n\n# Chapter 2: Embodied Intelligence",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 3,
        "chapter_title": "Chapter 2: Embodied Intelligence",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 32,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-embodied-intelligence.md"
      }
    },
    {
      "content": "1. Explain the embodiment hypothesis and its implications\n2. Understand morphological computation with robotics examples\n3. Describe how developmental robotics mirrors infant learning",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 3,
        "chapter_title": "Chapter 2: Embodied Intelligence",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 23,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-embodied-intelligence.md"
      }
    },
    {
      "content": "**Core Idea**: Intelligence emerges from body-brain-environment interactions, not abstract reasoning alone.\n\n### Theoretical Foundation\n\nTraditional AI (GOFAI - \"Good Old-Fashioned AI\"):\n- Intelligence = symbol manipulation + logic\n- Body is just I/O device for the brain\n- **Problem**: Fails at \"simple\" tasks (walking, grasping)\n\n```mermaid\ngraph LR\n    subgraph Traditional[\"Traditional AI (GOFAI)\"]\n        Symbols[Symbol Manipulation]\n        Logic[Logical Reasoning]\n        Symbols --> Logic\n        Logic --> Output1[Abstract Output]\n    end\n\n    subgraph Embodied[\"Embodied AI\"]\n        Body[Physical Body<br/>Morphology]\n        Brain[Control System<br/>Brain/Computer]\n        Env[Environment<br/>Physical World]\n        \n        Env -->|Sensory Input| Brain\n        Brain -->|Motor Commands| Body\n        Body -->|Physical Action| Env\n        Env -->|Feedback| Body\n        Body -->|Proprioception| Brain\n    end\n\n    Traditional -.->|Cannot handle| RealWorld[Real-world<br/>Interaction]\n    Embodied -->|Enables| RealWorld\n\n    style Traditional fill:#ffcccc\n    style Embodied fill:#ccffcc\n    style RealWorld fill:#fff4cc\n```\n\n**Figure 2.1**: Traditional AI vs Embodied AI. Embodied systems form a closed loop with the environment through sensing and acting, while traditional symbolic AI operates in isolation from the physical world.\n\n**Embodied Cognition** (Brooks, Pfeifer, Varela):\n- Intelligence grounded in sensorimotor experience\n- Body shapes what can be perceived and learned\n- Environment is part of the cognitive system\n\n### Evidence from Neuroscience\n\n**Mirror Neurons** (Rizzolatti, 1990s):\n- Fire both when acting AND observing actions\n- Understanding comes from motor simulation\n- \"I understand grasping because I can grasp\"\n\n**Sensorimotor Contingencies**:\n- Vision is active exploration (saccades, head movements)\n- Not passive image processing\n- Robots with active cameras outperform fixed ones\n\n### Robotics Implications\n\n1. **Cheap design**: Offload computation to body morphology\n2. **Faster learning**: Exploit physical dynamics\n3. **Robustness**: Body provides implicit feedback",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 3,
        "chapter_title": "Chapter 2: Embodied Intelligence",
        "section": "2.1 The Embodiment Hypothesis",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 244,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-embodied-intelligence.md"
      }
    },
    {
      "content": "**Definition**: Using body structure to simplify or eliminate control complexity.\n\n### Example 1: Passive Dynamic Walkers\n\n**McGeer's Walker** (1990):\n- Bipedal robot with NO motors, NO sensors, NO computer\n- Walks down shallow slopes purely from leg geometry\n- Demonstrates: Control can emerge from mechanics\n\n**Key Insight**: The \"right\" body does computation for free.\n\n### Example 2: Compliant Grippers\n\n**Soft Robotics**:\n- Rigid gripper: Must plan finger trajectories, force control\n- Soft gripper: Material compliance adapts to object shape\n- **Result**: 10x fewer control parameters\n\n**Boston Dynamics**: Uses leg compliance to absorb landing impacts without complex control.\n\n### Example 3: Whiskers\n\n**Rat Whiskers**:\n- Passive sensors, no actuation needed\n- Material properties encode distance and texture\n- Computational cost: near-zero\n\n**Robotic Implementation**:\n- Artificial whiskers for navigation in dark/dusty environments\n- Body (whisker stiffness) does feature extraction",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 3,
        "chapter_title": "Chapter 2: Embodied Intelligence",
        "section": "2.2 Morphological Computation",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 137,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-embodied-intelligence.md"
      }
    },
    {
      "content": "**Inspiration**: Human infants learn through sensorimotor exploration.\n\n### Developmental Stages\n\n**1. Random Motor Babbling** (0-3 months):\n- Infant: Flail arms randomly, discover hand-eye coordination\n- Robot: Random joint commands → learn forward kinematics\n\n**2. Object Permanence** (4-8 months):\n- Infant: Realize objects exist when occluded\n- Robot: Track objects behind obstacles (predictive models)\n\n**3. Tool Use** (12-18 months):\n- Infant: Use spoon, extend reach with stick\n- Robot: Learn body schema includes grasped tools\n\n### Curiosity-Driven Learning\n\n**Intrinsic Motivation**:\n- Not task-specific rewards\n- Explore novel states, maximize information gain\n- Example: Robot arm explores workspace, discovers objects\n\n**Benefits**:\n- No manual reward engineering\n- Discovers affordances (what actions are possible)\n- Robust to changing environments",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 3,
        "chapter_title": "Chapter 2: Embodied Intelligence",
        "section": "2.3 Developmental Robotics",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 116,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-embodied-intelligence.md"
      }
    },
    {
      "content": "Embodiment is not optional—bodies shape intelligence.\n\n**Key Concepts**:\n- Embodiment hypothesis: Intelligence from interaction\n- Morphological computation: Bodies do computation\n- Developmental robotics: Learn like infants\n\n**Next**: Chapter 3 on sensing and perception.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 3,
        "chapter_title": "Chapter 2: Embodied Intelligence",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 33,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-embodied-intelligence.md"
      }
    },
    {
      "content": "**Exercise 2.1**: Explain the concept of \"morphological computation\" using a specific example not covered in this chapter. How does the physical structure reduce computational requirements?\n\n**Exercise 2.2**: Design a simple robot appendage (e.g., gripper, leg, whisker) that uses passive compliance to simplify a control task. Sketch the design and explain how material properties contribute to its function.\n\n**Exercise 2.3**: Compare motor babbling in infants with random exploration in reinforcement learning agents. What are the similarities and differences in terms of learning objectives and outcomes?\n\n---",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 3,
        "chapter_title": "Chapter 2: Embodied Intelligence",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 85,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-embodied-intelligence.md"
      }
    },
    {
      "content": "Have questions about the content? Use our AI-powered chatbot to get instant answers based on the book material:\n\n<ChatbotWidget \n  bookId=\"physical-ai-robotics\" \n  chapterNumber={0} \n/>\n\n:::tip\nThe chatbot provides answers grounded in the book content with source references. Try asking questions about the concepts covered in this chapter.\n:::",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 3,
        "chapter_title": "Chapter 2: Embodied Intelligence",
        "section": "Ask Questions About This Chapter",
        "subsection": null,
        "page_number": null,
        "chunk_index": 7,
        "word_count": 46,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-embodied-intelligence.md"
      }
    },
    {
      "content": "---\nsidebar_position: 3\ntitle: \"Chapter 3: Sensing and Perception\"\ndescription: Sensor modalities, fusion, and perception-action loops\ntags: [sensors, perception, sensor-fusion, proprioception]\n---\nnimport { ChatbotWidget } from '@site/src/components/ChatbotWidget';\n\n# Chapter 3: Sensing and Perception",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 4,
        "chapter_title": "Chapter 3: Sensing and Perception",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 34,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-sensing-perception.md"
      }
    },
    {
      "content": "1. Identify key sensor modalities and their trade-offs\n2. Explain sensor fusion architectures\n3. Understand the perception-action loop",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 4,
        "chapter_title": "Chapter 3: Sensing and Perception",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 18,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-sensing-perception.md"
      }
    },
    {
      "content": "### Vision: Cameras\n\n**RGB Cameras**:\n- Rich semantic information (colors, textures, text)\n- Cheap ($10-$100)\n- **Limitations**: No direct depth, lighting-dependent\n\n**Depth Cameras** (RealSense, Kinect):\n- Stereo or structured light for 3D\n- **Trade-off**: Indoor only, limited range (0.5-5m)\n\n**Event Cameras**:\n- Asynchronous pixels (fire on brightness change)\n- High temporal resolution (microseconds)\n- Low latency for fast motion\n\n### Ranging: LiDAR\n\n**Principle**: Time-of-flight laser measurement\n\n**Advantages**:\n- Accurate 3D (mm precision)\n- Works in dark, immune to lighting\n\n**Disadvantages**:\n- Expensive ($1k-$10k)\n- Reflective/transparent surfaces fail\n- Moving parts (mechanical LiDAR)\n\n### Inertial: IMU\n\n**Components**:\n- Accelerometer: Linear acceleration (m/s²)\n- Gyroscope: Angular velocity (rad/s)\n- Magnetometer: Orientation (compass)\n\n**Use Cases**:\n- Balance control (detect tipping)\n- Dead reckoning (integrate acceleration → velocity → position)\n- **Drift Problem**: Integration error accumulates\n\n### Proprioception\n\n**Joint Encoders**: Measure joint angles (0.01° resolution)\n**Force/Torque Sensors**: Detect contact, measure grip strength\n\n**Importance**: Know body state independent of vision.\n\n### Tactile\n\n**Types**:\n- Resistive (pressure changes resistance)\n- Capacitive (proximity/contact)\n- Optical (camera inside soft skin)\n\n**Applications**: Grasp stability, texture recognition, safe human contact.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 4,
        "chapter_title": "Chapter 3: Sensing and Perception",
        "section": "3.1 Sensor Modalities",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 180,
        "has_code_block": false,
        "has_math": true,
        "source_file": "03-sensing-perception.md"
      }
    },
    {
      "content": "**Problem**: Each sensor has errors, blind spots, delays.\n\n**Solution**: Combine redundant sensors for robustness.\n\n```mermaid\ngraph TB\n    subgraph Sensors[\"Sensor Array\"]\n        GPS[GPS<br/>Position<br/>Slow, Accurate]\n        IMU[IMU<br/>Acceleration<br/>Fast, Drifts]\n        Camera[Camera<br/>Visual Features<br/>Rich, Noisy]\n        Lidar[LiDAR<br/>3D Points<br/>Precise, Expensive]\n    end\n\n    subgraph Fusion[\"Sensor Fusion (Kalman Filter)\"]\n        Predict[Prediction Step<br/>Use IMU for fast update]\n        Update[Update Step<br/>Correct with GPS/Camera/LiDAR]\n        Predict --> Update\n        Update --> Predict\n    end\n\n    subgraph Output[\"Fused Estimate\"]\n        State[Robot State<br/>Position, Velocity, Orientation<br/>Fast + Accurate]\n    end\n\n    GPS --> Update\n    IMU --> Predict\n    Camera --> Update\n    Lidar --> Update\n    Update --> State\n\n    style Sensors fill:#ffe1cc\n    style Fusion fill:#cce1ff\n    style Output fill:#ccffcc\n```\n\n**Figure 3.1**: Sensor fusion pipeline combining complementary sensors (GPS, IMU, camera, LiDAR) using a Kalman filter to produce a fast and accurate state estimate.\n\n### Kalman Filter\n\n**Use Case**: Fusing GPS + IMU for position\n\n**Idea**:\n- Prediction: Use IMU to predict position (fast, drifts)\n- Update: Correct with GPS (slow, accurate)\n- **Result**: Fast + accurate estimate\n\n**Formula** (simplified):\n```\nx̂ = Prediction + K × (Measurement - Prediction)\n```\nK = Kalman Gain (how much to trust measurement vs prediction)\n\n**Pseudocode Example: Kalman Filter for GPS + IMU Fusion**\n\n```python\n# State: [position_x, position_y, velocity_x, velocity_y]\n# Measurements: GPS (position), IMU (acceleration)\n\ndef kalman_filter(state, covariance, measurement, dt):\n    # Prediction step (use IMU)\n    F = [[1, 0, dt, 0],   # State transition matrix\n         [0, 1, 0, dt],\n         [0, 0, 1, 0],\n         [0, 0, 0, 1]]\n    \n    predicted_state = F @ state + B @ imu_acceleration\n    predicted_cov = F @ covariance @ F.T + process_noise\n    \n    # Update step (use GPS)\n    H = [[1, 0, 0, 0],    # Measurement matrix (GPS measures position only)\n         [0, 1, 0, 0]]\n    \n    innovation = gps_measurement - H @ predicted_state\n    kalman_gain = predicted_cov @ H.T @ inv(H @ predicted_cov @ H.T + measurement_noise)\n    \n    updated_state = predicted_state + kalman_gain @ innovation\n    updated_cov = (I - kalman_gain @ H) @ predicted_cov\n    \n    return updated_state, updated_cov\n```\n\nThis combines fast IMU predictions with slower but accurate GPS corrections.\n\n### Particle Filter\n\n**Use Case**: Robot localization with unknown position\n\n**Idea**:\n- Represent belief as particles (samples)\n- Each particle = possible robot pose\n- Resample based on sensor likelihood\n- **Handles**: Multi-modal distributions (multiple hypotheses)\n\n### Modern Approach: Learned Fusion\n\n**Neural Networks**:\n- Input: Multi-modal data (image + LiDAR + IMU)\n- Output: Fused representation (e.g., occupancy grid)\n- **Advantage**: Learns sensor correlations from data",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 4,
        "chapter_title": "Chapter 3: Sensing and Perception",
        "section": "3.2 Sensor Fusion Architectures",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 381,
        "has_code_block": true,
        "has_math": false,
        "source_file": "03-sensing-perception.md"
      }
    },
    {
      "content": "```mermaid\ngraph LR\n    subgraph World[\"Physical Environment\"]\n        State[Environmental State]\n    end\n\n    subgraph Robot[\"Robot System\"]\n        Sensors[Sensors]\n        Perception[Perception<br/>State Estimation]\n        Planning[Action Selection<br/>Planning/Control]\n        Actuators[Actuators]\n    end\n\n    State -->|Sense| Sensors\n    Sensors -->|Data| Perception\n    Perception -->|World Model| Planning\n    Planning -->|Commands| Actuators\n    Actuators -->|Action| State\n    \n    Planning -.->|Active Perception<br/>Move to improve sensing| Actuators\n\n    style World fill:#fff4e1\n    style Robot fill:#e1f5ff\n```\n\n**Figure 3.2**: The perception-action loop showing how action affects perception (active perception) and how sensory feedback continuously guides action. This is a continuous cycle, not a sequential pipeline.\n\n**Classical View**: Sense → Perceive → Plan → Act (sequential)\n\n**Embodied View**: Continuous loop, action affects perception\n\n### Active Perception\n\n**Concept**: Move to improve sensing\n\n**Examples**:\n- Turn head to see occluded object\n- Poke object to infer mass/compliance\n- Shake box to hear contents\n\n### Sensorimotor Contingencies\n\n**Definition**: Learned relationships between actions and sensory changes\n\n**Robot Example**:\n- Move forward → objects grow in image\n- Turn left → visual flow rightward\n- **Learning**: Build model of these contingencies\n\n### Real-Time Constraints\n\n**Humanoid Walking**:\n- Sense: IMU at 1kHz, cameras at 30Hz\n- Control: Joint commands at 100Hz\n- **Challenge**: Fuse asynchronous sensors for real-time control",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 4,
        "chapter_title": "Chapter 3: Sensing and Perception",
        "section": "3.3 Perception-Action Loop",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 184,
        "has_code_block": true,
        "has_math": false,
        "source_file": "03-sensing-perception.md"
      }
    },
    {
      "content": "**Sensors**: Vision, LiDAR, IMU, proprioception, tactile\n**Fusion**: Kalman, particle filters, learned fusion\n**Loop**: Perception and action are coupled, not sequential\n\n**Next**: Chapter 4 on locomotion and motor control.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 4,
        "chapter_title": "Chapter 3: Sensing and Perception",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 28,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-sensing-perception.md"
      }
    },
    {
      "content": "**Exercise 3.1**: Choose two sensor modalities from Section 3.1 and design a sensor fusion approach for a specific task (e.g., autonomous navigation, grasping). Justify why combining these sensors improves performance over using either alone.\n\n**Exercise 3.2**: Implement the Kalman filter pseudocode in Python or your preferred language. Test it with simulated GPS and IMU data where the IMU has high-frequency noise and the GPS has low-frequency drift.\n\n**Exercise 3.3**: Give three examples of \"active perception\" in everyday human activities (beyond the examples in this chapter). For each, explain how action improves sensing and what would be lost if the system were purely passive.\n\n---",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 4,
        "chapter_title": "Chapter 3: Sensing and Perception",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 104,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-sensing-perception.md"
      }
    },
    {
      "content": "Have questions about the content? Use our AI-powered chatbot to get instant answers based on the book material:\n\n<ChatbotWidget \n  bookId=\"physical-ai-robotics\" \n  chapterNumber={0} \n/>\n\n:::tip\nThe chatbot provides answers grounded in the book content with source references. Try asking questions about the concepts covered in this chapter.\n:::",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 4,
        "chapter_title": "Chapter 3: Sensing and Perception",
        "section": "Ask Questions About This Chapter",
        "subsection": null,
        "page_number": null,
        "chunk_index": 7,
        "word_count": 46,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-sensing-perception.md"
      }
    },
    {
      "content": "---\nsidebar_position: 4\ntitle: \"Chapter 4: Locomotion and Motor Control\"\ndescription: Locomotion strategies, kinematics, and balance\ntags: [locomotion, motor-control, kinematics, balance]\n---\nnimport { ChatbotWidget } from '@site/src/components/ChatbotWidget';\n\n# Chapter 4: Locomotion and Motor Control",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 5,
        "chapter_title": "Chapter 4: Locomotion and Motor Control",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 35,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-locomotion-motor-control.md"
      }
    },
    {
      "content": "1. Compare locomotion strategies (wheeled, legged, hybrid)\n2. Understand forward/inverse kinematics\n3. Explain balance and stability for legged robots",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 5,
        "chapter_title": "Chapter 4: Locomotion and Motor Control",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 19,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-locomotion-motor-control.md"
      }
    },
    {
      "content": "### Wheeled\n\n**Advantages**:\n- Energy efficient on flat ground\n- Simple control (differential drive, Ackermann steering)\n- High speed, stable\n\n**Disadvantages**:\n- Cannot climb stairs\n- Poor on rough terrain\n- Limited maneuverability in tight spaces\n\n**Use Cases**: Warehouses, roads, indoor navigation\n\n### Legged (Bipedal, Quadrupedal, Hexapod)\n\n**Bipedal** (humanoids):\n- Human environments (stairs, narrow paths)\n- **Challenge**: Unstable (small support polygon)\n\n**Quadrupedal** (Spot, ANYmal):\n- Stable (always 3+ feet on ground)\n- Good rough terrain performance\n- **Trade-off**: Larger footprint than bipeds\n\n**Hexapod**:\n- Maximum stability (can lift 3 legs, still stable)\n- Slow, complex coordination\n\n**Advantages**: Stairs, rocks, rubble, disaster sites\n\n**Disadvantages**: High energy cost, complex control\n\n### Hybrid\n\n**Wheeled-Legged** (Handle by Boston Dynamics):\n- Wheels on legs\n- Efficiency of wheels + mobility of legs\n\n**Transforming**: Switch modes based on terrain",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 5,
        "chapter_title": "Chapter 4: Locomotion and Motor Control",
        "section": "4.1 Locomotion Strategies",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 133,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-locomotion-motor-control.md"
      }
    },
    {
      "content": "### Forward Kinematics\n\n**Problem**: Given joint angles θ, find end effector pose (x, y, z, orientation)\n\n**Example** (2D, 2-link arm):\n```\nx = L1*cos(θ1) + L2*cos(θ1+θ2)\ny = L1*sin(θ1) + L2*sin(θ1+θ2)\n```\n\n**Use**: Visualization, collision checking\n\n### Inverse Kinematics (IK)\n\n**Problem**: Given desired pose (x, y, z), find joint angles θ\n\n**Challenges**:\n- Multiple solutions (elbow up/down)\n- No solution (out of reach)\n- Singularities (loss of DOF)\n\n**Methods**:\n- **Analytical**: Closed-form (fast, limited to simple chains)\n- **Numerical**: Jacobian-based optimization (general, slower)\n\n**Use**: Reaching, manipulation, walking (foot placement)\n\n### Dynamics\n\n**Forward Dynamics**: Forces → accelerations\n**Inverse Dynamics**: Accelerations → required forces/torques\n\n**Application**: Gravity compensation (hold arm against gravity)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 5,
        "chapter_title": "Chapter 4: Locomotion and Motor Control",
        "section": "4.2 Motor Control Fundamentals",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 109,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-locomotion-motor-control.md"
      }
    },
    {
      "content": "### Zero Moment Point (ZMP)\n\n**Definition**: Point on ground where net moment = 0\n\n**Stable Walking**: ZMP inside support polygon (foot/feet area)\n\n**Control Strategy**:\n- Plan footsteps to keep ZMP inside\n- Adjust center of mass (CoM) trajectory\n- **Limitation**: Assumes flat ground, no slipping\n\n### Central Pattern Generators (CPG)\n\n**Biological Inspiration**: Spinal cord rhythmic patterns (no brain needed)\n\n**Robot Implementation**:\n- Coupled oscillators generate leg motions\n- **Advantage**: Robust, natural gait transitions\n\n**Example**: Quadruped trot (diagonal legs in phase)\n\n### Learning-Based Control\n\n**Reinforcement Learning**:\n- Train in Isaac Gym (1000s of parallel robots)\n- Reward: Forward velocity, penalty for falling\n- **Result**: Emergent gaits (walk, trot, bound)\n\n**Advantages**:\n- Handles complex dynamics (humanoid = 20+ DOF)\n- Adapts to terrain changes\n\n**Challenges**:\n- Sim-to-real gap\n- Safety (no guarantees)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 5,
        "chapter_title": "Chapter 4: Locomotion and Motor Control",
        "section": "4.3 Balance and Stability",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 130,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-locomotion-motor-control.md"
      }
    },
    {
      "content": "**Locomotion**: Wheeled (efficient, limited terrain) vs Legged (versatile, complex)\n**Control**: IK for reaching, dynamics for torque, ZMP for balance\n**Modern**: Learning-based methods dominate for complex robots\n\n**Module 0 Complete!** Next modules cover ROS 2, simulation, Isaac, and VLAs.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 5,
        "chapter_title": "Chapter 4: Locomotion and Motor Control",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 38,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-locomotion-motor-control.md"
      }
    },
    {
      "content": "**Exercise 4.1**: For a 2-DOF robot arm with link lengths L1=1m and L2=0.5m, compute the forward kinematics for joint angles θ1=45° and θ2=30°. Then solve the inverse kinematics problem to find joint angles that reach the point (x=1.0, y=0.5).\n\n**Exercise 4.2**: Compare the Zero Moment Point (ZMP) criterion with a learning-based balance controller. What are the advantages and disadvantages of each approach for humanoid walking?\n\n**Exercise 4.3**: Design a hybrid locomotion system for a search-and-rescue robot that must navigate both urban environments (flat) and disaster sites (rubble). Specify the locomotion modes, when to switch between them, and how the switching decision would be made.\n\n**Exercise 4.4**: Research one of the locomotion robots mentioned (Spot, ANYmal, Handle, or Atlas) and write a technical summary including: degrees of freedom, actuation type, key sensors, and one example application where its locomotion strategy provides a unique advantage.\n\n---",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 5,
        "chapter_title": "Chapter 4: Locomotion and Motor Control",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 144,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-locomotion-motor-control.md"
      }
    },
    {
      "content": "Have questions about the content? Use our AI-powered chatbot to get instant answers based on the book material:\n\n<ChatbotWidget \n  bookId=\"physical-ai-robotics\" \n  chapterNumber={0} \n/>\n\n:::tip\nThe chatbot provides answers grounded in the book content with source references. Try asking questions about the concepts covered in this chapter.\n:::",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 5,
        "chapter_title": "Chapter 4: Locomotion and Motor Control",
        "section": "Ask Questions About This Chapter",
        "subsection": null,
        "page_number": null,
        "chunk_index": 7,
        "word_count": 46,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-locomotion-motor-control.md"
      }
    },
    {
      "content": "---\nsidebar_position: 1\ntitle: \"Chapter 1: ROS 2 Core Concepts\"\ndescription: ROS 2 architecture, DDS middleware, and QoS policies\ntags: [ros2, dds, qos, architecture]\n---\n\n# Chapter 1: ROS 2 Core Concepts",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 6,
        "chapter_title": "Chapter 1: ROS 2 Core Concepts",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 32,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-core-concepts.md"
      }
    },
    {
      "content": "1. Compare ROS 2 with ROS 1 and understand the motivations for redesign\n2. Understand the graph architecture of ROS 2 systems\n3. Configure Quality of Service (QoS) policies for reliable communication",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 6,
        "chapter_title": "Chapter 1: ROS 2 Core Concepts",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 32,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-core-concepts.md"
      }
    },
    {
      "content": "### Why ROS 2?\n\n**ROS 1 Limitations**:\n- Single point of failure (ROS Master)\n- No real-time support (Python/TCP-based)\n- Poor multi-robot support (namespace conflicts)\n- No built-in security\n- Linux-only production use\n\n**ROS 2 Improvements**:\n- **DDS (Data Distribution Service)**: Industry-standard middleware\n- **No master**: Peer-to-peer discovery\n- **Real-time**: Deterministic communication paths\n- **Multi-robot**: Isolated DDS domains\n- **Security**: DDS-Security (authentication, encryption)\n- **Cross-platform**: Windows, macOS, RTOS support\n\n### DDS Middleware\n\n**What is DDS?**\n- OMG (Object Management Group) standard for data-centric pub/sub\n- Used in military, aerospace, medical devices\n- Multiple vendors: Fast-DDS, Cyclone DDS, RTI Connext\n\n**Key Features**:\n- Discovery: Automatic participant detection\n- QoS: Fine-grained reliability/latency control\n- Data types: IDL (Interface Definition Language) for serialization\n\n### Migration Considerations\n\n**When to use ROS 2**:\n- New projects (default choice)\n- Multi-robot systems\n- Real-time control loops\n- Production deployments\n\n**When ROS 1 is acceptable**:\n- Legacy codebases (ros1_bridge exists)\n- Prototyping with mature ROS 1 packages\n- Research with limited time (some packages not yet ported)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 6,
        "chapter_title": "Chapter 1: ROS 2 Core Concepts",
        "section": "1.1 ROS 2 vs ROS 1",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 170,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-core-concepts.md"
      }
    },
    {
      "content": "```mermaid\ngraph TB\n    subgraph ros2_graph[\"ROS 2 Graph\"]\n        subgraph Nodes[\"Nodes (Processes)\"]\n            N1[Camera Driver]\n            N2[Image Processor]\n            N3[Object Detector]\n            N4[Motion Planner]\n            N5[Motor Controller]\n        end\n\n        subgraph Topics[\"Topics (Pub/Sub)\"]\n            T1[\"/camera/image<br/>sensor_msgs/Image\"]\n            T2[\"/objects<br/>vision_msgs/Detection2DArray\"]\n            T3[\"/cmd_vel<br/>geometry_msgs/Twist\"]\n        end\n\n        subgraph Services[\"Services (Req/Reply)\"]\n            S1[\"/capture_image\"]\n            S2[\"/reset_odometry\"]\n        end\n\n        subgraph Actions[\"Actions (Goal/Feedback/Result)\"]\n            A1[\"/navigate_to_pose\"]\n        end\n\n        N1 -->|publish| T1\n        T1 -->|subscribe| N2\n        N2 -->|publish| T2\n        T2 -->|subscribe| N3\n        N3 -.->|call| S1\n        N4 -->|publish| T3\n        T3 -->|subscribe| N5\n        N4 -.->|send goal| A1\n    end\n\n    style Nodes fill:#e1f5ff\n    style Topics fill:#ccffcc\n    style Services fill:#ffffcc\n    style Actions fill:#ffccff\n```\n\n**Figure 1.1**: ROS 2 graph architecture showing nodes communicating via topics (asynchronous), services (synchronous), and actions (goal-based). Topics use publish-subscribe, while services and actions use request-reply patterns.\n\n**ROS 2 Graph** = Nodes connected by topics, services, actions, and parameters\n\n### Nodes\n\n**Definition**: Independent processes that perform computation\n\n**Characteristics**:\n- Single responsibility (e.g., camera driver, motion planner)\n- Language-agnostic (C++, Python, Rust via rclcpp/rclpy/rclrust)\n- Composable: Multiple nodes in one process for efficiency\n\n**Example**:\n```bash\nros2 run demo_nodes_cpp talker  # Start publisher node\nros2 run demo_nodes_cpp listener # Start subscriber node\nros2 node list                   # See active nodes\n```\n\n### Topics\n\n**Pattern**: Publish-Subscribe (asynchronous, many-to-many)\n\n**Use Case**: Streaming data (sensor readings, odometry, images)\n\n**Message Types**:\n- Standard: `sensor_msgs/Image`, `geometry_msgs/Twist`\n- Custom: Define in `.msg` files\n\n**Example**:\n```bash\nros2 topic list\nros2 topic echo /chatter\nros2 topic hz /camera/image_raw  # Check publish frequency\n```\n\n### Services\n\n**Pattern**: Request-Reply (synchronous, one-to-one)\n\n**Use Case**: Discrete operations (reset odometry, capture image, load map)\n\n**Example**:\n```bash\nros2 service list\nros2 service call /add_two_ints example_interfaces/srv/AddTwoInts \"{a: 2, b: 3}\"\n```\n\n### Actions\n\n**Pattern**: Goal-Feedback-Result (asynchronous, preemptable)\n\n**Use Case**: Long-running tasks (navigate to waypoint, grasp object)\n\n**Components**:\n- Goal: Target state\n- Feedback: Progress updates\n- Result: Final outcome + success/failure\n\n**Example**:\n```bash\nros2 action list\nros2 action send_goal /fibonacci example_interfaces/action/Fibonacci \"{order: 5}\"\n```\n\n### Parameters\n\n**Definition**: Node configuration values (read at runtime)\n\n**Use Case**: Tuning without recompiling (PID gains, sensor IDs, file paths)\n\n**Example**:\n```bash\nros2 param list\nros2 param get /camera frame_rate\nros2 param set /camera frame_rate 60\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 6,
        "chapter_title": "Chapter 1: ROS 2 Core Concepts",
        "section": "1.2 Graph Architecture",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 331,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-core-concepts.md"
      }
    },
    {
      "content": "**Problem**: One size does not fit all communication patterns\n\n### Key QoS Policies\n\n**Reliability**:\n- `RELIABLE`: Guaranteed delivery (TCP-like) - Use for commands\n- `BEST_EFFORT`: No retransmission (UDP-like) - Use for sensors (lossy OK)\n\n**Durability**:\n- `TRANSIENT_LOCAL`: Late-joiners get last N messages - Use for maps, static data\n- `VOLATILE`: Only current messages - Use for real-time sensor streams\n\n**History**:\n- `KEEP_LAST(N)`: Store last N messages (default N=10)\n- `KEEP_ALL`: Store all (unbounded, risky)\n\n**Deadline**:\n- Max time between messages\n- Triggers callback if violated\n- Use for safety-critical loops\n\n**Lifespan**:\n- Max age of message before discarded\n- Prevents stale data (e.g., old odometry)\n\n### QoS Profiles (Presets)\n\n```python\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\n\n# Sensor data (lossy, real-time)\nsensor_qos = QoSProfile(\n    reliability=ReliabilityPolicy.BEST_EFFORT,\n    history=HistoryPolicy.KEEP_LAST,\n    depth=5\n)\n\n# Commands (reliable, no loss)\ncommand_qos = QoSProfile(\n    reliability=ReliabilityPolicy.RELIABLE,\n    history=HistoryPolicy.KEEP_LAST,\n    depth=10\n)\n```\n\n### QoS Compatibility\n\n**Publisher-Subscriber Matching**:\n- Reliability: RELIABLE publisher can match BEST_EFFORT subscriber (downgrade)\n- Durability: TRANSIENT_LOCAL publisher can match VOLATILE subscriber\n- Mismatch causes silent failure (check with `ros2 doctor`)\n\n**Best Practice**: Use standard profiles (`sensor_data`, `services_default`, `parameters`)\n\n```mermaid\ngraph TD\n    Start[Choose QoS Profile]\n    Start --> Q1{Data loss<br/>acceptable?}\n\n    Q1 -->|Yes<br/>Sensor data| BestEffort[BEST_EFFORT<br/>+ VOLATILE]\n    Q1 -->|No<br/>Commands| Reliable[RELIABLE]\n\n    Reliable --> Q2{Late joiners<br/>need history?}\n    Q2 -->|Yes<br/>Map/Config| Transient[TRANSIENT_LOCAL<br/>+ KEEP_LAST 10]\n    Q2 -->|No<br/>Real-time| Volatile[VOLATILE<br/>+ KEEP_LAST 10]\n\n    BestEffort --> Sensor[sensor_data_qos]\n    Transient --> Param[parameters_qos]\n    Volatile --> Cmd[services_default_qos]\n\n    style Sensor fill:#ccffcc\n    style Param fill:#ffffcc\n    style Cmd fill:#ffcccc\n```\n\n**Figure 1.2**: QoS policy decision tree. Choose BEST_EFFORT for high-frequency sensor data where occasional loss is acceptable. Use RELIABLE + TRANSIENT_LOCAL for configuration data that late-joining nodes need. Use RELIABLE + VOLATILE for commands that must arrive but don't need history.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 6,
        "chapter_title": "Chapter 1: ROS 2 Core Concepts",
        "section": "1.3 Quality of Service (QoS) Policies",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 273,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-core-concepts.md"
      }
    },
    {
      "content": "**ROS 2**: Modern redesign with DDS, real-time, multi-robot, security\n**Graph**: Nodes connected via topics (pub/sub), services (req/reply), actions (goals)\n**QoS**: Fine-grained control over reliability, latency, and durability",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 6,
        "chapter_title": "Chapter 1: ROS 2 Core Concepts",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 27,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-core-concepts.md"
      }
    },
    {
      "content": "**Exercise 1.1**: Compare DDS-based ROS 2 with ROS 1 (with ROS Master). List three failure scenarios where ROS 1 would fail but ROS 2 would continue operating.\n\n**Exercise 1.2**: Design QoS policies for the following use cases. Justify your choices:\n1. Robot odometry (50 Hz)\n2. Emergency stop command\n3. Static map loaded at startup\n4. Camera feed for object detection (30 Hz)\n\n**Exercise 1.3**: Create a simple ROS 2 package with a publisher and subscriber. The publisher should send a custom message containing robot battery status (voltage, current, percentage). The subscriber should log warnings when battery is below 20%.\n\n**Exercise 1.4**: Debug a QoS mismatch scenario: A camera publishes with `sensor_data_qos` (BEST_EFFORT), but a logger subscribes with `services_default_qos` (RELIABLE). Why does the subscriber not receive messages? How would you fix it?\n\n**Next**: Chapter 2 dives deeper into implementing nodes, topics, services, and actions.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 6,
        "chapter_title": "Chapter 1: ROS 2 Core Concepts",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 144,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-core-concepts.md"
      }
    },
    {
      "content": "---\nsidebar_position: 2\ntitle: \"Chapter 2: Nodes, Topics, Services, Actions\"\ndescription: ROS 2 communication patterns and node lifecycle\ntags: [ros2, nodes, topics, services, actions, lifecycle]\n---\n\n# Chapter 2: Nodes, Topics, Services, Actions",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 7,
        "chapter_title": "Chapter 2: Nodes, Topics, Services, Actions",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 33,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-nodes-topics-services-actions.md"
      }
    },
    {
      "content": "1. Implement ROS 2 nodes with proper lifecycle management\n2. Design pub/sub systems using topics and message types\n3. Implement synchronous services and asynchronous actions",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 7,
        "chapter_title": "Chapter 2: Nodes, Topics, Services, Actions",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 25,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-nodes-topics-services-actions.md"
      }
    },
    {
      "content": "### Managed vs Unmanaged Nodes\n\n**Unmanaged Node** (default):\n- Starts immediately in active state\n- No formal state transitions\n- Simple for prototyping\n\n**Managed Node** (LifecycleNode):\n- Explicit state machine: Unconfigured → Inactive → Active → Finalized\n- Allows controlled startup/shutdown\n- Required for production systems (ros2_control, Nav2)\n\n### Lifecycle States\n\n```\n┌─────────────┐\n│ Unconfigured│\n└──────┬──────┘\n       │ configure()\n       ▼\n┌─────────────┐\n│  Inactive   │\n└──────┬──────┘\n       │ activate()\n       ▼\n┌─────────────┐\n│   Active    │  ◄─── Normal operation\n└──────┬──────┘\n       │ deactivate()\n       ▼\n┌─────────────┐\n│  Inactive   │\n└──────┬──────┘\n       │ cleanup() / shutdown()\n       ▼\n┌─────────────┐\n│  Finalized  │\n└─────────────┘\n```\n\n**State Callbacks**:\n- `on_configure()`: Load parameters, allocate resources\n- `on_activate()`: Start publishing, open connections\n- `on_deactivate()`: Pause operation, keep resources\n- `on_cleanup()`: Release resources\n- `on_shutdown()`: Emergency stop\n\n**Example** (Python):\n```python\nfrom rclpy.lifecycle import LifecycleNode, LifecycleState\n\nclass CameraNode(LifecycleNode):\n    def on_configure(self, state: LifecycleState):\n        self.camera = Camera(self.get_parameter('device_id').value)\n        self.pub = self.create_lifecycle_publisher(Image, 'camera/image', 10)\n        return TransitionCallbackReturn.SUCCESS\n\n    def on_activate(self, state: LifecycleState):\n        self.camera.start()\n        self.timer = self.create_timer(0.033, self.publish_frame)  # 30 Hz\n        return super().on_activate(state)\n\n    def on_deactivate(self, state: LifecycleState):\n        self.timer.cancel()\n        self.camera.stop()\n        return super().on_deactivate(state)\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 7,
        "chapter_title": "Chapter 2: Nodes, Topics, Services, Actions",
        "section": "2.1 Node Lifecycle Management",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 169,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-nodes-topics-services-actions.md"
      }
    },
    {
      "content": "### Message Types\n\n**Standard Messages** (`ros2 interface list`):\n- `std_msgs`: Primitive types (Int32, Float64, String, Bool)\n- `geometry_msgs`: Pose, Twist, Transform, Wrench\n- `sensor_msgs`: Image, PointCloud2, LaserScan, Imu, JointState\n- `nav_msgs`: Odometry, Path, OccupancyGrid\n\n**Custom Messages**:\n```\n# my_robot_msgs/msg/BatteryStatus.msg\nfloat32 voltage\nfloat32 current\nfloat32 percentage\nuint8 STATUS_OK=0\nuint8 STATUS_LOW=1\nuint8 STATUS_CRITICAL=2\nuint8 status\n```\n\nBuild with `colcon build`, generates Python/C++ classes.\n\n### Publisher Example (C++)\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n\nclass VelocityPublisher : public rclcpp::Node {\npublic:\n    VelocityPublisher() : Node(\"velocity_publisher\") {\n        pub_ = this->create_publisher<geometry_msgs::msg::Twist>(\n            \"cmd_vel\", 10);\n        timer_ = this->create_wall_timer(\n            std::chrono::milliseconds(100),\n            std::bind(&VelocityPublisher::publish_cmd, this));\n    }\n\nprivate:\n    void publish_cmd() {\n        auto msg = geometry_msgs::msg::Twist();\n        msg.linear.x = 0.5;  // Forward at 0.5 m/s\n        msg.angular.z = 0.1; // Turn at 0.1 rad/s\n        pub_->publish(msg);\n    }\n\n    rclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr pub_;\n    rclcpp::TimerBase::SharedPtr timer_;\n};\n```\n\n### Subscriber Example (Python)\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\n\nclass ObstacleDetector(Node):\n    def __init__(self):\n        super().__init__('obstacle_detector')\n        self.sub = self.create_subscription(\n            LaserScan,\n            'scan',\n            self.scan_callback,\n            10)  # QoS depth\n\n    def scan_callback(self, msg: LaserScan):\n        min_distance = min(msg.ranges)\n        if min_distance < 0.5:\n            self.get_logger().warn(f'Obstacle at {min_distance:.2f}m!')\n```\n\n### Design Patterns\n\n**Throttling**:\n```python\nfrom rclpy.time import Duration\nself.last_publish = self.get_clock().now()\n\nif (self.get_clock().now() - self.last_publish) > Duration(seconds=1.0):\n    self.publisher.publish(msg)\n    self.last_publish = self.get_clock().now()\n```\n\n**Message Filtering**:\n```python\nfrom message_filters import Subscriber, TimeSynchronizer\n\nimage_sub = Subscriber(self, Image, '/camera/image')\ndepth_sub = Subscriber(self, Image, '/camera/depth')\nsync = TimeSynchronizer([image_sub, depth_sub], queue_size=10)\nsync.registerCallback(self.rgbd_callback)\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 7,
        "chapter_title": "Chapter 2: Nodes, Topics, Services, Actions",
        "section": "2.2 Publish-Subscribe Pattern (Topics)",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 218,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-nodes-topics-services-actions.md"
      }
    },
    {
      "content": "### When to Use Services\n\n**Services**: Discrete, short-duration tasks\n- Reset odometry\n- Capture a single image\n- Save map to file\n- Query robot state\n\n**Not for**: Long tasks (use actions), streaming (use topics)\n\n### Service Definition\n\n```\n# example_interfaces/srv/AddTwoInts.srv\nint64 a\nint64 b\n---\nint64 sum\n```\n\n### Service Server (Python)\n\n```python\nfrom example_interfaces.srv import AddTwoInts\n\nclass AdderService(Node):\n    def __init__(self):\n        super().__init__('adder')\n        self.srv = self.create_service(\n            AddTwoInts,\n            'add_two_ints',\n            self.add_callback)\n\n    def add_callback(self, request, response):\n        response.sum = request.a + request.b\n        self.get_logger().info(f'{request.a} + {request.b} = {response.sum}')\n        return response\n```\n\n### Service Client (C++)\n\n```cpp\nauto client = node->create_client<example_interfaces::srv::AddTwoInts>(\"add_two_ints\");\n\nauto request = std::make_shared<example_interfaces::srv::AddTwoInts::Request>();\nrequest->a = 5;\nrequest->b = 7;\n\nauto future = client->async_send_request(request);\n// Wait for response\nif (rclcpp::spin_until_future_complete(node, future) == rclcpp::FutureReturnCode::SUCCESS) {\n    auto result = future.get();\n    RCLCPP_INFO(node->get_logger(), \"Sum: %ld\", result->sum);\n}\n```\n\n### Timeout Handling\n\n```python\nimport rclpy\nfrom rclpy.client import Client\n\nclient = node.create_client(Trigger, '/reset_odometry')\nif not client.wait_for_service(timeout_sec=5.0):\n    node.get_logger().error('Service not available!')\n    return\n\nfuture = client.call_async(Trigger.Request())\nrclpy.spin_until_future_complete(node, future, timeout_sec=2.0)\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 7,
        "chapter_title": "Chapter 2: Nodes, Topics, Services, Actions",
        "section": "2.3 Request-Reply Pattern (Services)",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 157,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-nodes-topics-services-actions.md"
      }
    },
    {
      "content": "### When to Use Actions\n\n**Actions**: Long-running, preemptable tasks\n- Navigate to waypoint (can cancel mid-flight)\n- Pick and place object (feedback on grasp progress)\n- Trajectory execution (report completion percentage)\n\n**Advantages**:\n- Cancellable: Client can abort\n- Feedback: Progress updates\n- Result: Final outcome\n\n### Action Definition\n\n```\n# action/Fibonacci.action\n# Goal\nint32 order\n---\n# Result\nint32[] sequence\n---\n# Feedback\nint32[] partial_sequence\n```\n\n### Action Server (Python)\n\n```python\nfrom rclpy.action import ActionServer\nfrom example_interfaces.action import Fibonacci\n\nclass FibonacciServer(Node):\n    def __init__(self):\n        super().__init__('fibonacci_server')\n        self._action_server = ActionServer(\n            self,\n            Fibonacci,\n            'fibonacci',\n            self.execute_callback)\n\n    def execute_callback(self, goal_handle):\n        feedback = Fibonacci.Feedback()\n        feedback.partial_sequence = [0, 1]\n\n        for i in range(1, goal_handle.request.order):\n            # Check for cancellation\n            if goal_handle.is_cancel_requested:\n                goal_handle.canceled()\n                return Fibonacci.Result()\n\n            # Compute next number\n            feedback.partial_sequence.append(\n                feedback.partial_sequence[-1] + feedback.partial_sequence[-2])\n            goal_handle.publish_feedback(feedback)\n            time.sleep(0.5)  # Simulate work\n\n        goal_handle.succeed()\n        result = Fibonacci.Result()\n        result.sequence = feedback.partial_sequence\n        return result\n```\n\n### Action Client (C++)\n\n```cpp\nauto action_client = rclcpp_action::create_client<Fibonacci>(node, \"fibonacci\");\n\nauto goal_msg = Fibonacci::Goal();\ngoal_msg.order = 10;\n\nauto send_goal_options = rclcpp_action::Client<Fibonacci>::SendGoalOptions();\nsend_goal_options.feedback_callback = [](auto, const auto & feedback) {\n    std::cout << \"Feedback: \" << feedback->partial_sequence.back() << std::endl;\n};\nsend_goal_options.result_callback = [](const auto & result) {\n    std::cout << \"Final sequence size: \" << result.result->sequence.size() << std::endl;\n};\n\nauto goal_handle_future = action_client->async_send_goal(goal_msg, send_goal_options);\n```\n\n### Preemption (Cancellation)\n\n```python\n# Client cancels goal\nfuture = action_client.send_goal_async(goal)\ngoal_handle = future.result()\ncancel_future = goal_handle.cancel_goal_async()\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 7,
        "chapter_title": "Chapter 2: Nodes, Topics, Services, Actions",
        "section": "2.4 Goal-Based Pattern (Actions)",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 218,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-nodes-topics-services-actions.md"
      }
    },
    {
      "content": "**Lifecycle**: Managed nodes for controlled startup/shutdown\n**Topics**: Many-to-many, async, streaming data\n**Services**: One-to-one, sync, discrete requests\n**Actions**: Goal-feedback-result, async, preemptable",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 7,
        "chapter_title": "Chapter 2: Nodes, Topics, Services, Actions",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 20,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-nodes-topics-services-actions.md"
      }
    },
    {
      "content": "**Exercise 2.1**: Implement a LifecycleNode that controls an LED. The LED should turn on in the `on_activate()` callback and turn off in the `on_deactivate()` callback. Test state transitions using `ros2 lifecycle set /led_node`.\n\n**Exercise 2.2**: Create a service that calculates the distance between two 3D points. Define a custom service type with two `geometry_msgs/Point` as input and a `float64` distance as output. Implement the server in C++ and client in Python.\n\n**Exercise 2.3**: Implement an action server for a simple battery charging simulation. The goal is the target charge percentage, feedback is the current percentage (updated every second), and the result indicates whether charging succeeded or was preempted. Test cancellation mid-charging.\n\n**Exercise 2.4**: Synchronize two camera topics (RGB and depth) using `message_filters.TimeSynchronizer`. Process only pairs of messages with matching timestamps. Log a warning if synchronization fails for more than 1 second.\n\n**Next**: Chapter 3 covers coordinate transforms (TF2) and robot descriptions (URDF).",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 7,
        "chapter_title": "Chapter 2: Nodes, Topics, Services, Actions",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 7,
        "word_count": 152,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-nodes-topics-services-actions.md"
      }
    },
    {
      "content": "---\nsidebar_position: 3\ntitle: \"Chapter 3: TF and URDF\"\ndescription: Coordinate transforms and robot description formats\ntags: [ros2, tf2, urdf, xacro, robot-description]\n---\n\n# Chapter 3: TF and URDF",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 8,
        "chapter_title": "Chapter 3: TF and URDF",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 29,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-tf-urdf.md"
      }
    },
    {
      "content": "1. Use TF2 to manage coordinate transforms in robot systems\n2. Design robot descriptions using URDF and Xacro\n3. Apply best practices for modular robot descriptions",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 8,
        "chapter_title": "Chapter 3: TF and URDF",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 26,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-tf-urdf.md"
      }
    },
    {
      "content": "### Why TF2?\n\n**Problem**: Robots have many coordinate frames (world, base, sensors, end-effector)\n\n**Example**: To grasp an object detected by a camera:\n1. Camera sees object at (x, y, z) in `camera_frame`\n2. Need position in `base_link` for arm planning\n3. TF2 automatically transforms: `camera_frame` → `base_link`\n\n```mermaid\ngraph TD\n    map[map<br/>World fixed frame] --> odom[odom<br/>Local navigation]\n    odom --> base_link[base_link<br/>Robot center]\n\n    base_link --> base_footprint[base_footprint<br/>Ground projection]\n    base_link --> torso[torso]\n\n    torso --> head[head]\n    head --> camera_link[camera_link<br/>Camera optical frame]\n    head --> lidar_link[lidar_link]\n\n    torso --> left_shoulder[left_shoulder]\n    left_shoulder --> left_elbow[left_elbow]\n    left_elbow --> left_wrist[left_wrist]\n    left_wrist --> left_gripper[left_gripper]\n\n    torso --> right_shoulder[right_shoulder]\n    right_shoulder --> right_elbow[right_elbow]\n    right_elbow --> right_wrist[right_wrist]\n    right_wrist --> right_gripper[right_gripper]\n\n    base_link --> left_hip[left_hip]\n    left_hip --> left_knee[left_knee]\n    left_knee --> left_ankle[left_ankle]\n    left_ankle --> left_foot[left_foot]\n\n    base_link --> right_hip[right_hip]\n    right_hip --> right_knee[right_knee]\n    right_knee --> right_ankle[right_ankle]\n    right_ankle --> right_foot[right_foot]\n\n    style map fill:#ffffcc\n    style odom fill:#ccffff\n    style base_link fill:#ccffcc\n    style camera_link fill:#ffcccc\n```\n\n**Figure 3.1**: TF tree for a humanoid robot. The tree has a single root (`map`), with `odom` for local odometry and `base_link` as the robot's reference frame. Sensors (camera, lidar) and body parts (arms, legs) are child frames. TF2 automatically computes transforms between any two frames (e.g., `camera_link` to `left_gripper` for visual servoing).\n\n### TF2 Concepts\n\n**Frame**: Coordinate system (origin + orientation)\n\n**Transform**: Translation + rotation between two frames\n\n**Tree Structure**: Frames form a tree (one parent, multiple children)\n- Root: `world` or `map`\n- Mobile base: `odom` → `base_link`\n- Sensors: `base_link` → `camera_link`, `lidar_link`\n- Manipulator: `base_link` → `shoulder` → `elbow` → `wrist` → `end_effector`\n\n### TF2 API\n\n**Broadcasting Transforms** (Python):\n```python\nfrom tf2_ros import TransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\n\nclass OdometryPublisher(Node):\n    def __init__(self):\n        super().__init__('odometry_publisher')\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n    def publish_odometry(self, x, y, theta):\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'odom'\n        t.child_frame_id = 'base_link'\n        t.transform.translation.x = x\n        t.transform.translation.y = y\n        t.transform.translation.z = 0.0\n\n        # Convert yaw to quaternion\n        q = quaternion_from_euler(0, 0, theta)\n        t.transform.rotation.x = q[0]\n        t.transform.rotation.y = q[1]\n        t.transform.rotation.z = q[2]\n        t.transform.rotation.w = q[3]\n\n        self.tf_broadcaster.sendTransform(t)\n```\n\n**Looking Up Transforms** (C++):\n```cpp\n#include <tf2_ros/transform_listener.h>\n#include <tf2_ros/buffer.h>\n\nstd::shared_ptr<tf2_ros::Buffer> tf_buffer;\nstd::shared_ptr<tf2_ros::TransformListener> tf_listener;\n\ntf_buffer = std::make_shared<tf2_ros::Buffer>(node->get_clock());\ntf_listener = std::make_shared<tf2_ros::TransformListener>(*tf_buffer);\n\ntry {\n    auto transform = tf_buffer->lookupTransform(\n        \"base_link\", \"camera_link\",\n        tf2::TimePointZero);  // Latest available\n    // Use transform...\n} catch (tf2::TransformException &ex) {\n    RCLCPP_WARN(node->get_logger(), \"TF lookup failed: %s\", ex.what());\n}\n```\n\n### Time Travel\n\n**Problem**: Transforms change over time (robot moves)\n\n**Solution**: TF2 stores history (default: 10 seconds)\n\n```python\n# Get transform at specific time\npast_time = rclpy.time.Time(seconds=5.0)\ntransform = tf_buffer.lookup_transform(\n    'map', 'base_link',\n    past_time,\n    timeout=rclpy.duration.Duration(seconds=1.0))\n```\n\n**Use Case**: Localization - where was the robot when it saw this landmark?\n\n### Static vs Dynamic Transforms\n\n**Static**: Fixed (sensor mounted on robot)\n```bash\nros2 run tf2_ros static_transform_publisher 0 0 0.5 0 0 0 base_link camera_link\n```\n\n**Dynamic**: Changes (odometry, joint states)\n- Broadcast at sensor rate (IMU: 100Hz, odometry: 50Hz)\n\n### Debugging TF\n\n```bash\n# Visualize tree\nros2 run tf2_tools view_frames\n\n# Check specific transform\nros2 run tf2_ros tf2_echo base_link end_effector\n\n# View in RViz\nrviz2\n# Add TF display, set fixed frame to 'world'\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 8,
        "chapter_title": "Chapter 3: TF and URDF",
        "section": "3.1 Transform Trees (TF2)",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 489,
        "has_code_block": true,
        "has_math": false,
        "source_file": "03-tf-urdf.md"
      }
    },
    {
      "content": "### URDF (Unified Robot Description Format)\n\n**XML format** defining robot structure:\n- Links: Rigid bodies (visual, collision, inertial)\n- Joints: Connections (revolute, prismatic, fixed, continuous)\n\n**Simple Example**:\n```xml\n<?xml version=\"1.0\"?>\n<robot name=\"simple_robot\">\n  <link name=\"base_link\">\n    <visual>\n      <geometry>\n        <box size=\"0.6 0.4 0.2\"/>\n      </geometry>\n      <material name=\"blue\">\n        <color rgba=\"0 0 0.8 1\"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"0.6 0.4 0.2\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"10.0\"/>\n      <inertia ixx=\"0.1\" ixy=\"0\" ixz=\"0\" iyy=\"0.1\" iyz=\"0\" izz=\"0.1\"/>\n    </inertial>\n  </link>\n\n  <link name=\"wheel_left\">\n    <visual>\n      <geometry>\n        <cylinder radius=\"0.1\" length=\"0.05\"/>\n      </geometry>\n    </visual>\n    <inertial>\n      <mass value=\"0.5\"/>\n      <inertia ixx=\"0.01\" ixy=\"0\" ixz=\"0\" iyy=\"0.01\" iyz=\"0\" izz=\"0.01\"/>\n    </inertial>\n  </link>\n\n  <joint name=\"base_to_wheel_left\" type=\"continuous\">\n    <parent link=\"base_link\"/>\n    <child link=\"wheel_left\"/>\n    <origin xyz=\"0 0.25 -0.1\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 1 0\"/>\n  </joint>\n</robot>\n```\n\n**Joint Types**:\n- `fixed`: Welded (sensor mounts)\n- `revolute`: Hinge with limits (elbow: -90° to +90°)\n- `continuous`: Unlimited rotation (wheels)\n- `prismatic`: Linear slide (elevator, gripper)\n- `floating`: 6-DOF (rarely used, for simulating free-flying objects)\n\n### Xacro (XML Macros)\n\n**Problem**: URDF is verbose and repetitive\n\n**Solution**: Xacro adds variables, math, macros\n\n**Example**:\n```xml\n<?xml version=\"1.0\"?>\n<robot name=\"modular_robot\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\">\n  <!-- Parameters -->\n  <xacro:property name=\"wheel_radius\" value=\"0.1\"/>\n  <xacro:property name=\"wheel_width\" value=\"0.05\"/>\n  <xacro:property name=\"base_width\" value=\"0.4\"/>\n\n  <!-- Macro for wheels (reusable) -->\n  <xacro:macro name=\"wheel\" params=\"prefix x_pos y_pos\">\n    <link name=\"${prefix}_wheel\">\n      <visual>\n        <geometry>\n          <cylinder radius=\"${wheel_radius}\" length=\"${wheel_width}\"/>\n        </geometry>\n      </visual>\n      <inertial>\n        <mass value=\"0.5\"/>\n        <inertia ixx=\"0.01\" ixy=\"0\" ixz=\"0\" iyy=\"0.01\" iyz=\"0\" izz=\"0.01\"/>\n      </inertial>\n    </link>\n\n    <joint name=\"base_to_${prefix}_wheel\" type=\"continuous\">\n      <parent link=\"base_link\"/>\n      <child link=\"${prefix}_wheel\"/>\n      <origin xyz=\"${x_pos} ${y_pos} ${-wheel_radius}\" rpy=\"0 0 0\"/>\n      <axis xyz=\"0 1 0\"/>\n    </joint>\n  </xacro:macro>\n\n  <!-- Use macro -->\n  <xacro:wheel prefix=\"left\" x_pos=\"0\" y_pos=\"${base_width/2}\"/>\n  <xacro:wheel prefix=\"right\" x_pos=\"0\" y_pos=\"${-base_width/2}\"/>\n</robot>\n```\n\n**Convert to URDF**:\n```bash\nxacro robot.urdf.xacro > robot.urdf\n```\n\n### SDF (Simulation Description Format)\n\n**Gazebo-native format** (more expressive than URDF):\n- Supports closed kinematic chains\n- Plugin system\n- Nested models (multi-robot scenarios)\n\n**ROS 2 Integration**: `urdf_to_sdf` converter, or use URDF directly in Gazebo",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 8,
        "chapter_title": "Chapter 3: TF and URDF",
        "section": "3.2 Robot Description (URDF/Xacro, SDF)",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 293,
        "has_code_block": true,
        "has_math": true,
        "source_file": "03-tf-urdf.md"
      }
    },
    {
      "content": "### Modularity\n\n**Principle**: Separate robot into composable modules\n\n**Structure**:\n```\nrobot_description/\n├── urdf/\n│   ├── robot.urdf.xacro           # Top-level assembly\n│   ├── base/\n│   │   └── base.urdf.xacro        # Mobile base\n│   ├── sensors/\n│   │   ├── lidar.urdf.xacro\n│   │   └── camera.urdf.xacro\n│   └── manipulator/\n│       └── arm.urdf.xacro\n└── meshes/\n    ├── base_link.stl\n    └── gripper.stl\n```\n\n**Top-level File**:\n```xml\n<robot name=\"mobile_manipulator\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\">\n  <xacro:include filename=\"base/base.urdf.xacro\"/>\n  <xacro:include filename=\"sensors/lidar.urdf.xacro\"/>\n  <xacro:include filename=\"manipulator/arm.urdf.xacro\"/>\n\n  <!-- Assemble -->\n  <xacro:mobile_base prefix=\"\"/>\n  <xacro:lidar parent=\"base_link\" xyz=\"0.2 0 0.3\"/>\n  <xacro:robot_arm parent=\"base_link\" xyz=\"0 0 0.2\"/>\n</robot>\n```\n\n### Parameterization\n\n**Use properties for tunable values**:\n```xml\n<xacro:arg name=\"robot_mass\" default=\"50.0\"/>\n<xacro:arg name=\"wheel_diameter\" default=\"0.2\"/>\n<xacro:arg name=\"use_gpu_lidar\" default=\"false\"/>\n\n<mass value=\"$(arg robot_mass)\"/>\n```\n\n**Load with arguments**:\n```bash\nros2 launch robot_bringup robot.launch.py robot_mass:=60.0 use_gpu_lidar:=true\n```\n\n### Coordinate Frame Conventions\n\n**REP 103** (ROS coordinate frames):\n- X: Forward\n- Y: Left\n- Z: Up\n\n**REP 105** (frame names):\n- `map`: World frame (static, localization reference)\n- `odom`: Local odometry frame (continuous, drifts over time)\n- `base_link`: Robot center (origin for planning)\n- `base_footprint`: Projection of `base_link` on ground (Z=0)\n\n### Validation\n\n```bash\n# Check URDF for errors\ncheck_urdf robot.urdf\n\n# Visualize in RViz\nros2 launch urdf_tutorial display.launch.py model:=robot.urdf\n\n# Test joint limits\nros2 run joint_state_publisher_gui joint_state_publisher_gui\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 8,
        "chapter_title": "Chapter 3: TF and URDF",
        "section": "3.3 URDF Best Practices",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 195,
        "has_code_block": true,
        "has_math": true,
        "source_file": "03-tf-urdf.md"
      }
    },
    {
      "content": "**TF2**: Manages coordinate transforms, time-aware, tree structure\n**URDF**: Robot description (links, joints, visual, collision, inertial)\n**Xacro**: Modular, parameterized URDF with macros\n**Best Practices**: Modularity, parameterization, REP compliance",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 8,
        "chapter_title": "Chapter 3: TF and URDF",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 27,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-tf-urdf.md"
      }
    },
    {
      "content": "**Exercise 3.1**: Write a TF broadcaster that publishes the transform from `map` to `odom` based on a simulated robot position (x, y, theta). Update the transform at 50 Hz. Verify with `ros2 run tf2_ros tf2_echo map odom`.\n\n**Exercise 3.2**: Create a URDF for a simple 2-DOF robot arm with:\n- Base link (fixed to ground)\n- Shoulder joint (revolute, ±90°)\n- Elbow joint (revolute, ±120°)\n- End effector link\n\nUse Xacro to parameterize link lengths. Visualize in RViz with `joint_state_publisher_gui`.\n\n**Exercise 3.3**: Use TF2 to transform a point detected by a camera (in `camera_link` frame) to the robot's `base_link` frame. Given a point at (0.5, 0, 1.0) in camera coordinates, compute its position relative to the base. Account for the camera being mounted 0.3m above and 0.2m forward of the base.\n\n**Exercise 3.4**: Refactor a monolithic URDF into modular Xacro files. Separate the mobile base, sensors (camera, LiDAR), and manipulator into individual files. Create a top-level assembly file that includes all modules.\n\n**Next**: Chapter 4 integrates ROS 2 with simulation (Gazebo, launch files, development workflow).",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 8,
        "chapter_title": "Chapter 3: TF and URDF",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 175,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-tf-urdf.md"
      }
    },
    {
      "content": "---\nsidebar_position: 4\ntitle: \"Chapter 4: Simulation Pipeline\"\ndescription: Gazebo integration, launch files, and development workflow\ntags: [ros2, gazebo, launch, colcon, development]\n---\n\n# Chapter 4: Simulation Pipeline",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 9,
        "chapter_title": "Chapter 4: Simulation Pipeline",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 28,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-simulation-pipeline.md"
      }
    },
    {
      "content": "1. Integrate ROS 2 with Gazebo simulation\n2. Write Python launch files for complex system bringup\n3. Use colcon for building, testing, and debugging ROS 2 packages",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 9,
        "chapter_title": "Chapter 4: Simulation Pipeline",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 27,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-simulation-pipeline.md"
      }
    },
    {
      "content": "### Gazebo + ROS 2\n\n**Gazebo** (formerly Ignition, now Gazebo Sim):\n- Open-source 3D robot simulator\n- Physics engines: ODE, Bullet, DART\n- Sensor simulation: cameras, LiDAR, IMU, contact\n- Plugin system for custom behavior\n\n**ROS 2 Integration** (`ros_gz`):\n- Bridge: Translates Gazebo topics ↔ ROS 2 topics\n- Plugins: Sensors, actuators publish to ROS 2\n- URDF/SDF: Same robot description in simulation and hardware\n\n### ros2_control\n\n**Abstraction layer** for hardware interfaces:\n- Same controller code runs on simulation and real robot\n- Hardware interface plugins (Gazebo, real motors)\n- Standard controllers: `joint_trajectory_controller`, `diff_drive_controller`\n\n**Architecture**:\n```\n┌─────────────────────────────────────┐\n│     Controller Manager              │\n├─────────────────────────────────────┤\n│  joint_trajectory_controller        │\n│  diff_drive_controller              │\n└─────────────┬───────────────────────┘\n              │ ros2_control interface\n┌─────────────┴───────────────────────┐\n│  Hardware Interface                 │\n│  - GazeboSystem (sim)               │\n│  - RobotHardware (real)             │\n└─────────────────────────────────────┘\n```\n\n**URDF Configuration**:\n```xml\n<ros2_control name=\"robot_hardware\" type=\"system\">\n  <hardware>\n    <plugin>gazebo_ros2_control/GazeboSystem</plugin>\n  </hardware>\n\n  <joint name=\"left_wheel_joint\">\n    <command_interface name=\"velocity\"/>\n    <state_interface name=\"position\"/>\n    <state_interface name=\"velocity\"/>\n  </joint>\n\n  <joint name=\"right_wheel_joint\">\n    <command_interface name=\"velocity\"/>\n    <state_interface name=\"position\"/>\n    <state_interface name=\"velocity\"/>\n  </joint>\n</ros2_control>\n```\n\n**Controller Config** (`controllers.yaml`):\n```yaml\ncontroller_manager:\n  ros__parameters:\n    update_rate: 100  # Hz\n\n    diff_drive_controller:\n      type: diff_drive_controller/DiffDriveController\n\n    joint_state_broadcaster:\n      type: joint_state_broadcaster/JointStateBroadcaster\n\ndiff_drive_controller:\n  ros__parameters:\n    left_wheel_names: [\"left_wheel_joint\"]\n    right_wheel_names: [\"right_wheel_joint\"]\n    wheel_separation: 0.4\n    wheel_radius: 0.1\n    publish_rate: 50.0\n    base_frame_id: base_link\n```\n\n### Sensor Plugins\n\n**Camera** (URDF Gazebo plugin):\n```xml\n<gazebo reference=\"camera_link\">\n  <sensor name=\"camera\" type=\"camera\">\n    <update_rate>30</update_rate>\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n      </image>\n    </camera>\n    <plugin name=\"camera_driver\" filename=\"libgazebo_ros_camera.so\">\n      <ros>\n        <namespace>/robot</namespace>\n        <remapping>image_raw:=camera/image</remapping>\n        <remapping>camera_info:=camera/info</remapping>\n      </ros>\n    </plugin>\n  </sensor>\n</gazebo>\n```\n\n**LiDAR**:\n```xml\n<gazebo reference=\"lidar_link\">\n  <sensor name=\"lidar\" type=\"gpu_ray\">\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14</min_angle>\n          <max_angle>3.14</max_angle>\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n      </range>\n    </ray>\n    <plugin name=\"lidar_driver\" filename=\"libgazebo_ros_ray_sensor.so\">\n      <ros>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n    </plugin>\n  </sensor>\n</gazebo>\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 9,
        "chapter_title": "Chapter 4: Simulation Pipeline",
        "section": "4.1 Gazebo Integration",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 254,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-simulation-pipeline.md"
      }
    },
    {
      "content": "### Why Python Launch?\n\n**ROS 1 XML Launch** limitations:\n- No conditionals (if/else)\n- No loops\n- No programmatic configuration\n\n**ROS 2 Python Launch**:\n- Full Python: conditionals, functions, external configs\n- Type checking\n- Composable nodes (multiple nodes in one process)\n\n### Basic Launch File\n\n```python\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='demo_nodes_cpp',\n            executable='talker',\n            name='talker',\n            output='screen',\n            parameters=[{'use_sim_time': True}]\n        ),\n        Node(\n            package='demo_nodes_cpp',\n            executable='listener',\n            name='listener',\n            output='screen'\n        )\n    ])\n```\n\n**Run**:\n```bash\nros2 launch my_package demo.launch.py\n```\n\n### Advanced Features\n\n**Parameters from YAML**:\n```python\nfrom launch.substitutions import PathJoinSubstitution\nfrom launch_ros.substitutions import FindPackageShare\n\nconfig = PathJoinSubstitution([\n    FindPackageShare('my_robot'),\n    'config',\n    'params.yaml'\n])\n\nNode(\n    package='my_robot',\n    executable='controller',\n    parameters=[config]\n)\n```\n\n**Conditionals**:\n```python\nfrom launch.conditions import IfCondition\nfrom launch.substitutions import LaunchConfiguration\n\nNode(\n    package='rviz2',\n    executable='rviz2',\n    condition=IfCondition(LaunchConfiguration('use_rviz'))\n)\n```\n\n**Remapping**:\n```python\nNode(\n    package='image_tools',\n    executable='cam2image',\n    remappings=[\n        ('/image', '/camera/image_raw'),\n        ('/camera_info', '/camera/info')\n    ]\n)\n```\n\n**Composable Nodes** (lower latency):\n```python\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\nComposableNodeContainer(\n    name='camera_container',\n    namespace='',\n    package='rclcpp_components',\n    executable='component_container',\n    composable_node_descriptions=[\n        ComposableNode(\n            package='image_proc',\n            plugin='image_proc::RectifyNode',\n            name='rectify'\n        ),\n        ComposableNode(\n            package='depth_image_proc',\n            plugin='depth_image_proc::PointCloudXyzrgbNode',\n            name='pointcloud'\n        )\n    ]\n)\n```\n\n### Gazebo Launch Example\n\n```python\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Robot description\n    urdf_path = PathJoinSubstitution([\n        FindPackageShare('my_robot_description'),\n        'urdf', 'robot.urdf.xacro'\n    ])\n\n    # Gazebo launch\n    gazebo = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('ros_gz_sim'),\n                'launch', 'gz_sim.launch.py'\n            ])\n        ]),\n        launch_arguments={'gz_args': '-r empty.sdf'}.items()\n    )\n\n    # Spawn robot\n    spawn_robot = Node(\n        package='ros_gz_sim',\n        executable='create',\n        arguments=[\n            '-name', 'my_robot',\n            '-topic', 'robot_description',\n            '-x', '0.0', '-y', '0.0', '-z', '0.5'\n        ]\n    )\n\n    # Robot state publisher\n    robot_state_publisher = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        parameters=[{'robot_description': Command(['xacro ', urdf_path])}]\n    )\n\n    # Controller manager\n    controller_manager = Node(\n        package='controller_manager',\n        executable='ros2_control_node',\n        parameters=[\n            {'robot_description': Command(['xacro ', urdf_path])},\n            PathJoinSubstitution([\n                FindPackageShare('my_robot_bringup'),\n                'config', 'controllers.yaml'\n            ])\n        ]\n    )\n\n    return LaunchDescription([\n        gazebo,\n        robot_state_publisher,\n        spawn_robot,\n        controller_manager\n    ])\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 9,
        "chapter_title": "Chapter 4: Simulation Pipeline",
        "section": "4.2 Launch Files (Python Launch System)",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 292,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-simulation-pipeline.md"
      }
    },
    {
      "content": "### Colcon Build System\n\n**Colcon** (collective construction):\n- Successor to `catkin_make`, `catkin build`\n- Language-agnostic (Python, C++, Rust)\n- Parallel builds\n\n**Workspace Structure**:\n```\nros2_ws/\n├── src/                    # Source packages\n│   ├── my_robot_description/\n│   ├── my_robot_bringup/\n│   └── my_robot_control/\n├── build/                  # Compiled artifacts\n├── install/                # Install space (setup.bash here)\n└── log/                    # Build logs\n```\n\n**Build Commands**:\n```bash\n# Build all packages\ncolcon build\n\n# Build specific package\ncolcon build --packages-select my_robot_description\n\n# Build with debug symbols\ncolcon build --cmake-args -DCMAKE_BUILD_TYPE=Debug\n\n# Build in parallel (4 jobs)\ncolcon build --parallel-workers 4\n\n# Symlink Python files (no rebuild needed for changes)\ncolcon build --symlink-install\n```\n\n**Source Workspace**:\n```bash\nsource install/setup.bash  # Bash\nsource install/setup.zsh   # Zsh\n```\n\n### Testing\n\n**Unit Tests** (Python):\n```python\nimport unittest\nfrom my_package.my_module import add\n\nclass TestAdd(unittest.TestCase):\n    def test_add_positive(self):\n        self.assertEqual(add(2, 3), 5)\n\n    def test_add_negative(self):\n        self.assertEqual(add(-1, -1), -2)\n```\n\n**Integration Tests** (`launch_testing`):\n```python\nimport launch_testing\nimport pytest\n\n@pytest.mark.launch_test\ndef test_talker_listener():\n    # Launch nodes, check for expected output\n    pass\n```\n\n**Run Tests**:\n```bash\n# All tests\ncolcon test\n\n# Specific package\ncolcon test --packages-select my_package\n\n# View results\ncolcon test-result --all\n```\n\n### Debugging\n\n**GDB (C++)**:\n```bash\n# Launch with gdb\nros2 run --prefix 'gdb -ex run --args' my_package my_node\n```\n\n**Python Debugger**:\n```python\nimport pdb; pdb.set_trace()  # Breakpoint\n```\n\n**Logging Levels**:\n```bash\nros2 run my_package my_node --ros-args --log-level DEBUG\n```\n\n**Topic Introspection**:\n```bash\nros2 topic echo /scan\nros2 topic hz /camera/image_raw  # Check frequency\nros2 topic bw /camera/image_raw  # Check bandwidth\n```\n\n**rqt Tools**:\n```bash\nrqt_graph     # Visualize node graph\nrqt_console   # Log messages\nrqt_plot      # Plot numeric topics\nrqt_bag       # Record/playback data\n```\n\n### Best Practices\n\n1. **Version Control**: Use git, `.gitignore` build artifacts\n2. **Dependencies**: List in `package.xml`, use `rosdep install`\n3. **Documentation**: README per package, code comments\n4. **CI/CD**: GitHub Actions for automated builds/tests\n5. **Code Style**: `ament_lint` for C++, `flake8` for Python",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 9,
        "chapter_title": "Chapter 4: Simulation Pipeline",
        "section": "4.3 Development Workflow",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 310,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-simulation-pipeline.md"
      }
    },
    {
      "content": "**Gazebo**: Physics simulation with `ros2_control` abstraction\n**Launch Files**: Python-based, composable, conditional logic\n**Colcon**: Build system with parallel compilation, testing\n**Development**: Debugging tools, CI/CD, code quality checks",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 9,
        "chapter_title": "Chapter 4: Simulation Pipeline",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 26,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-simulation-pipeline.md"
      }
    },
    {
      "content": "**Exercise 4.1**: Configure `ros2_control` for a differential drive robot in Gazebo. Define hardware interfaces for left and right wheel velocity commands. Load the `diff_drive_controller` and test with `ros2 topic pub /cmd_vel`.\n\n**Exercise 4.2**: Write a Python launch file that:\n1. Launches Gazebo with a custom world file\n2. Spawns a robot from URDF\n3. Starts the robot_state_publisher\n4. Conditionally launches RViz if `use_rviz:=true`\n5. Accepts a `robot_name` argument\n\n**Exercise 4.3**: Add a camera sensor plugin to your robot's URDF. Configure it to publish to `/robot/camera/image_raw` at 30 Hz with 640x480 resolution. Verify the image stream in RViz.\n\n**Exercise 4.4**: Set up continuous integration (CI) for a ROS 2 workspace using GitHub Actions. The CI should:\n1. Build all packages with `colcon build`\n2. Run unit tests with `colcon test`\n3. Run static analysis (ament_lint)\n4. Fail if any step produces errors\n\n**Module 1 Complete!** Next modules cover Gazebo/Unity (Module 2), NVIDIA Isaac (Module 3), VLA models (Module 4), and capstone project (Module 5).",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 9,
        "chapter_title": "Chapter 4: Simulation Pipeline",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 163,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-simulation-pipeline.md"
      }
    },
    {
      "content": "---\nsidebar_position: 1\ntitle: \"Chapter 1: Digital Twin Basics\"\ndescription: Digital twin concepts, simulation fidelity, and sim-to-real gap\ntags: [digital-twin, simulation, fidelity, sim-to-real, domain-randomization]\n---\n\n# Chapter 1: Digital Twin Basics",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 10,
        "chapter_title": "Chapter 1: Digital Twin Basics",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 31,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-basics.md"
      }
    },
    {
      "content": "1. Understand the digital twin concept and its applications in robotics\n2. Analyze tradeoffs between simulation speed and physical accuracy\n3. Apply strategies to bridge the sim-to-real gap",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 10,
        "chapter_title": "Chapter 1: Digital Twin Basics",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 28,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-basics.md"
      }
    },
    {
      "content": "### Definition\n\n**Digital Twin**: A virtual replica of a physical system that mirrors its behavior, appearance, and dynamics in real-time or near-real-time.\n\n**In Robotics**:\n- Virtual robot model (URDF/SDF) matches physical robot\n- Simulated sensors produce data similar to real sensors\n- Control algorithms tested in simulation transfer to hardware\n- Environment models (warehouse, terrain) reflect real deployment sites\n\n### Historical Context\n\n**Origins**: NASA Apollo program (1960s) - physical duplicates for testing\n**Modern**: Software-based virtual replicas with continuous data synchronization\n\n**Evolution in Robotics**:\n- 2000s: Basic kinematic simulators (V-REP, Webots)\n- 2010s: Physics-based simulators (Gazebo, MuJoCo)\n- 2020s: GPU-accelerated, photorealistic (Isaac Sim, Unity Simulation)\n\n### Use Cases in Robotics\n\n**1. Development and Testing**:\n- Test algorithms before hardware exists\n- Validate safety-critical behaviors (emergency stops, collision avoidance)\n- Iterate rapidly (1000s of trials per day in simulation)\n\n**2. Training AI Models**:\n- Reinforcement learning (train policies in parallel simulations)\n- Synthetic data generation (object detection, segmentation)\n- Domain randomization (vary lighting, textures, physics for robustness)\n\n**3. Deployment Planning**:\n- Simulate warehouse layouts before construction\n- Test multi-robot coordination\n- Predict performance metrics (throughput, energy consumption)\n\n**4. Operator Training**:\n- Teleop practice without risking real robot\n- Emergency scenario training (fire, equipment failure)\n- VR-based interfaces for remote operation\n\n**5. Continuous Validation**:\n- Monitor real robot, replay scenarios in simulation\n- Detect anomalies (real vs simulated behavior divergence)\n- Predict failures before they occur\n\n```mermaid\ngraph TB\n    subgraph Physical[\"Physical System\"]\n        Robot[Real Robot]\n        Sensors[Sensors<br/>Camera, LiDAR, IMU]\n        Actuators[Actuators<br/>Motors, Grippers]\n    end\n\n    subgraph Digital[\"Digital Twin\"]\n        Model[Virtual Model<br/>URDF, Physics]\n        SimSensors[Simulated Sensors]\n        Controller[Control Algorithm]\n    end\n\n    subgraph Cloud[\"Cloud Services\"]\n        Analytics[Analytics<br/>Prediction, Optimization]\n        Storage[Data Storage<br/>Telemetry, Logs]\n        Training[ML Training<br/>Policies, Models]\n    end\n\n    Robot -->|Telemetry| Digital\n    Sensors -->|Real Data| Digital\n    Digital -->|Commands| Actuators\n    Digital -->|Predict Behavior| Robot\n\n    Digital <-->|Data Sync| Cloud\n    Cloud -->|Updated Model| Digital\n    Cloud -->|OTA Updates| Robot\n\n    style Physical fill:#ffe1cc\n    style Digital fill:#cce1ff\n    style Cloud fill:#ccffcc\n```\n\n**Figure 1.1**: Digital twin architecture showing bidirectional data flow between physical robot, virtual model, and cloud services. The digital twin continuously mirrors the physical system, enabling prediction, optimization, and OTA updates.\n\n### Digital Twin vs Simulation\n\n**Traditional Simulation**:\n- One-way: Design in simulation → deploy to hardware\n- Offline: No connection to real system\n\n**Digital Twin**:\n- Two-way: Real-time data from hardware → update simulation\n- Bidirectional sync: Simulation informs hardware decisions\n- Continuous: Mirrors physical system throughout lifecycle\n\n**Example**: Autonomous vehicle fleet\n- Traditional: Test in simulator, deploy car\n- Digital twin: Each car streams telemetry to its twin, simulation predicts maintenance needs, tests OTA updates",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 10,
        "chapter_title": "Chapter 1: Digital Twin Basics",
        "section": "1.1 Digital Twin Concept",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 413,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-basics.md"
      }
    },
    {
      "content": "**Fidelity**: How accurately the simulation represents reality\n\n### Dimensions of Fidelity\n\n**1. Kinematic Fidelity**:\n- Low: Simplified geometry (boxes, cylinders)\n- High: CAD meshes with sub-mm accuracy\n\n**2. Dynamic Fidelity**:\n- Low: Kinematic-only (no forces, instantaneous motion)\n- Medium: Rigid body dynamics (mass, inertia, collisions)\n- High: Deformable objects (cloth, fluids, soft robotics)\n\n**3. Sensor Fidelity**:\n- Low: Ray-casting for LiDAR, pin-hole camera\n- High: Ray-tracing with noise models, lens distortion, motion blur\n\n**4. Visual Fidelity**:\n- Low: Flat colors, no lighting\n- High: Physically-based rendering (PBR), global illumination, shadows\n\n**5. Computational Fidelity**:\n- Low: Simplified control loops\n- High: Hardware-in-the-loop (real embedded code)\n\n### Speed vs Accuracy\n\n**Fast Simulation** (low fidelity):\n- **Use case**: Reinforcement learning (need 1M+ episodes)\n- **Example**: Isaac Gym - 10,000 parallel robots at 100x real-time\n- **Tradeoff**: Simplified physics, less realistic sensor noise\n\n**Accurate Simulation** (high fidelity):\n- **Use case**: Hardware validation, safety testing\n- **Example**: Gazebo with DART physics - realistic contact dynamics\n- **Tradeoff**: Slower than real-time (0.1x-0.5x)\n\n**Real-Time Requirement**:\n- Human-in-loop: Must run ≥1x real-time (operator sees immediate feedback)\n- Batch training: Can run slower (overnight training runs)\n\n### Fidelity Selection Strategy\n\n```\n┌─────────────────────────────────────────────────┐\n│ High Fidelity (Slow)                           │\n│ - Safety validation                            │\n│ - Hardware-specific tuning (PID gains)         │\n│ - Operator training                            │\n└─────────────────────────────────────────────────┘\n                      ↕\n┌─────────────────────────────────────────────────┐\n│ Medium Fidelity (Real-time)                    │\n│ - Algorithm development                        │\n│ - Multi-robot coordination                     │\n│ - ROS 2 integration testing                    │\n└─────────────────────────────────────────────────┘\n                      ↕\n┌─────────────────────────────────────────────────┐\n│ Low Fidelity (Fast)                            │\n│ - Reinforcement learning                       │\n│ - Path planning in large spaces                │\n│ - Monte Carlo analysis (1000s of trials)       │\n└─────────────────────────────────────────────────┘\n```\n\n**Adaptive Fidelity**: Start low (explore design space), increase for final validation",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 10,
        "chapter_title": "Chapter 1: Digital Twin Basics",
        "section": "1.2 Simulation Fidelity Tradeoffs",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 283,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-basics.md"
      }
    },
    {
      "content": "**Problem**: Policies trained in simulation fail on real robots\n\n**Causes**:\n1. **Physics inaccuracies**: Contact dynamics, friction, deformation\n2. **Unmodeled effects**: Cable drag, actuator backlash, sensor drift\n3. **Simplified perception**: Perfect object poses in sim, noisy estimates in reality\n4. **Deterministic sim**: Real world has stochastic disturbances (wind, uneven floors)\n\n### Quantifying the Gap\n\n**Metrics**:\n- Task success rate: 95% in sim, 60% on hardware → 35% gap\n- Trajectory error: Mean deviation between sim and real execution\n- Perception accuracy: Object detection mAP difference\n\n**Example (Grasping)**:\n- Sim: 100% success (known object pose, perfect gripper control)\n- Real: 70% success (pose estimation errors, contact uncertainties)\n\n### Bridging Strategies\n\n**1. Domain Randomization**\n\n**Idea**: Vary simulation parameters so policy learns robust features\n\n**Randomize**:\n- **Physics**: Friction (0.3-0.9), object mass (±20%), joint damping\n- **Visual**: Lighting intensity, object textures, camera exposure\n- **Geometry**: Object sizes, positions, robot link lengths (within tolerances)\n- **Dynamics**: Action delays, sensor noise levels\n\n**Example (Cube stacking)**:\n```python\n# Randomize cube properties\ncube_mass = np.random.uniform(0.05, 0.15)  # kg\nfriction = np.random.uniform(0.4, 1.0)\ntexture = random.choice(textures)  # Different colors/patterns\n```\n\n**Result**: Policy sees diverse scenarios, generalizes to real world variability\n\n**2. System Identification**\n\n**Idea**: Measure real robot parameters, update simulation to match\n\n**Process**:\n1. Run calibration experiments on hardware (measure friction, inertia)\n2. Fit simulation parameters to match observed behavior\n3. Iteratively refine (residual minimization)\n\n**Tools**:\n- Motor torque calibration (apply known torque, measure angle)\n- Contact friction tests (sliding blocks on surfaces)\n- Sensor noise characterization (static environment, measure variance)\n\n**Limitation**: Time-consuming, requires hardware access\n\n**3. Sim-to-Real Transfer Learning**\n\n**Fine-tuning**:\n- Train policy in simulation (millions of samples)\n- Collect small real-world dataset (1000s of samples)\n- Fine-tune policy on real data (update last layers)\n\n**Advantage**: Leverages sim for exploration, real data for final polish\n\n**4. Reality Gap-Aware Training**\n\n**Adversarial Training**:\n- Train discriminator to detect sim vs real data\n- Policy learns features that work in both domains\n\n**Invariant Representations**:\n- Learn state representations invariant to sim/real differences\n- Example: Depth images (less affected by lighting than RGB)\n\n**5. Hybrid Approaches**\n\n**Hardware-in-the-Loop (HIL)**:\n- Real robot actuators + simulated environment\n- Or: Real sensors + simulated robot\n\n**Digital Twin Feedback**:\n- Run policy on real robot, stream data to sim\n- If real diverges from sim prediction → trigger safety stop, investigate\n\n### Best Practices\n\n1. **Start simple**: Test sim-to-real on simple tasks (move to point) before complex (manipulation)\n2. **Measure gap**: Log sim and real metrics, track convergence over development\n3. **Incremental reality**: Sim → sim with randomization → simplified hardware setup → full deployment\n4. **Safety margins**: If sim predicts 70% success, expect 50-60% on hardware\n5. **Iterative**: Sim → real → analyze failures → improve sim → repeat",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 10,
        "chapter_title": "Chapter 1: Digital Twin Basics",
        "section": "1.3 Sim-to-Real Gap",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 453,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-basics.md"
      }
    },
    {
      "content": "**Digital Twin**: Virtual replica for development, training, deployment planning, continuous validation\n**Fidelity Tradeoffs**: Fast (RL, Monte Carlo) ↔ Accurate (safety, tuning)\n**Sim-to-Real Gap**: Bridged via domain randomization, system ID, transfer learning, hybrid approaches",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 10,
        "chapter_title": "Chapter 1: Digital Twin Basics",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 33,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-basics.md"
      }
    },
    {
      "content": "**Exercise 1.1**: Design a digital twin system for a warehouse robot fleet. Specify:\n1. What data flows from physical robots to digital twins\n2. What predictions the twin makes (maintenance, performance)\n3. How the twin influences robot behavior (OTA updates, routing optimization)\n\n**Exercise 1.2**: For a pick-and-place robot, determine appropriate simulation fidelity levels for:\n1. Initial algorithm development (RL policy training)\n2. PID gain tuning for gripper force control\n3. Safety validation before hardware deployment\nJustify each choice with speed/accuracy tradeoffs.\n\n**Exercise 1.3**: Implement domain randomization for a cube stacking task. Randomize:\n- Cube mass (±20%)\n- Surface friction (0.4-1.0)\n- Lighting direction and intensity\nMeasure sim-to-real transfer by comparing success rates in simulation vs on real robot.\n\n**Exercise 1.4**: Conduct system identification for a mobile robot. Measure wheel friction by applying known torques and recording velocities. Update simulation parameters to match. Compare trajectories before and after calibration.\n\n**Next**: Chapter 2 covers Gazebo physics engines, contact modeling, and sensor simulation.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 10,
        "chapter_title": "Chapter 1: Digital Twin Basics",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 160,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-basics.md"
      }
    },
    {
      "content": "---\nsidebar_position: 2\ntitle: \"Chapter 2: Gazebo Physics\"\ndescription: Physics engines, contact modeling, and sensor simulation\ntags: [gazebo, physics, ode, bullet, dart, contact, sensors]\n---\n\n# Chapter 2: Gazebo Physics",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 11,
        "chapter_title": "Chapter 2: Gazebo Physics",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 30,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-gazebo-physics.md"
      }
    },
    {
      "content": "1. Compare physics engines (ODE, Bullet, DART) and select appropriate engine\n2. Configure contact parameters (friction, compliance) for realistic simulation\n3. Implement sensor plugins (camera, LiDAR, IMU) in Gazebo",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 11,
        "chapter_title": "Chapter 2: Gazebo Physics",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 29,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-gazebo-physics.md"
      }
    },
    {
      "content": "### Overview\n\n**Physics Engine**: Computes forces, collisions, and motion of rigid/soft bodies\n\n**Gazebo** supports multiple engines via plugin architecture:\n- **ODE** (Open Dynamics Engine): Default, stable, mature\n- **Bullet**: Fast, used in games and VR\n- **DART** (Dynamic Animation and Robotics Toolkit): Accurate contact, best for manipulation\n- **Simbody**: Biomechanics-focused (rarely used in robotics)\n\n### ODE (Open Dynamics Engine)\n\n**Strengths**:\n- Mature (20+ years of development)\n- Stable for wheeled robots and simple manipulation\n- Good documentation\n\n**Weaknesses**:\n- Less accurate contact resolution\n- Struggles with stacked objects (instability)\n- Slower convergence for complex contacts\n\n**Use Cases**:\n- Mobile robots (differential drive, Ackermann steering)\n- Prototyping (quick setup)\n- Educational simulations\n\n**Configuration**:\n```xml\n<physics type=\"ode\">\n  <max_step_size>0.001</max_step_size>\n  <real_time_factor>1.0</real_time_factor>\n  <real_time_update_rate>1000</real_time_update_rate>\n  <ode>\n    <solver>\n      <type>quick</type>\n      <iters>50</iters>\n      <sor>1.3</sor>\n    </solver>\n    <constraints>\n      <cfm>0.0</cfm>\n      <erp>0.2</erp>\n      <contact_max_correcting_vel>100.0</contact_max_correcting_vel>\n      <contact_surface_layer>0.001</contact_surface_layer>\n    </constraints>\n  </ode>\n</physics>\n```\n\n### Bullet\n\n**Strengths**:\n- Fast (optimized for real-time games)\n- Good for soft body dynamics (deformable objects)\n- Multi-threaded (can utilize multiple CPU cores)\n\n**Weaknesses**:\n- Less deterministic (minor numerical differences across runs)\n- Contact constraints less precise than DART\n\n**Use Cases**:\n- Fast simulation for reinforcement learning\n- Soft robotics (pneumatic actuators, flexible grippers)\n- Large-scale environments (100+ objects)\n\n**Configuration**:\n```xml\n<physics type=\"bullet\">\n  <max_step_size>0.001</max_step_size>\n  <bullet>\n    <solver>\n      <type>sequential_impulse</type>\n      <iters>50</iters>\n      <sor>1.3</sor>\n    </solver>\n    <constraints>\n      <cfm>0.0</cfm>\n      <erp>0.2</erp>\n      <split_impulse>true</split_impulse>\n    </constraints>\n  </bullet>\n</physics>\n```\n\n### DART\n\n**Strengths**:\n- Most accurate contact resolution (LCP solver)\n- Best for manipulation (grasping, assembly, contact-rich)\n- Deterministic (exact same results on repeated runs)\n\n**Weaknesses**:\n- Slower (prioritizes accuracy over speed)\n- More complex configuration\n\n**Use Cases**:\n- Manipulation tasks (pick-and-place, in-hand manipulation)\n- Legged locomotion (foot contacts critical)\n- Sim-to-real transfer (minimizes reality gap)\n\n**Configuration**:\n```xml\n<physics type=\"dart\">\n  <max_step_size>0.001</max_step_size>\n  <dart>\n    <solver>\n      <solver_type>dantzig</solver_type>\n    </solver>\n    <collision_detector>bullet</collision_detector>\n  </dart>\n</physics>\n```\n\n### Comparison Table\n\n| Feature | ODE | Bullet | DART |\n|---------|-----|--------|------|\n| **Speed** | Medium | Fast | Slow |\n| **Contact Accuracy** | Low | Medium | High |\n| **Stacking Stability** | Poor | Medium | Excellent |\n| **Soft Bodies** | No | Yes | Limited |\n| **Determinism** | Medium | Low | High |\n| **Best For** | Mobile robots | Fast RL | Manipulation |\n\n### Selection Guidelines\n\n**Choose ODE** if:\n- Mobile robot (no complex contacts)\n- Need quick setup with defaults\n- Educational use\n\n**Choose Bullet** if:\n- Need `>1x` real-time speed\n- Soft body simulation\n- Determinism not critical\n\n**Choose DART** if:\n- Manipulation or legged robots\n- Accuracy more important than speed\n- Sim-to-real transfer planned",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 11,
        "chapter_title": "Chapter 2: Gazebo Physics",
        "section": "2.1 Physics Engines",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 409,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-gazebo-physics.md"
      }
    },
    {
      "content": "### Contact Parameters\n\n**Contact** occurs when two collision geometries overlap\n\n**Key Parameters**:\n1. **Friction** (μ): Resistance to sliding\n2. **Restitution** (e): Bounciness (0 = inelastic, 1 = perfectly elastic)\n3. **Contact stiffness** (k): How \"hard\" the surface feels\n4. **Contact damping** (c): Energy dissipation during contact\n\n### Friction Models\n\n**Coulomb Friction**:\n- `F_friction ≤ μ * F_normal`\n- Static friction (μ_s) > dynamic friction (μ_d)\n\n**In Gazebo**:\n```xml\n<surface>\n  <friction>\n    <ode>\n      <mu>0.8</mu>    <!-- Coefficient of friction -->\n      <mu2>0.8</mu2>  <!-- Secondary direction (anisotropic) -->\n      <fdir1>1 0 0</fdir1>  <!-- Primary friction direction -->\n      <slip1>0.0</slip1>\n      <slip2>0.0</slip2>\n    </ode>\n  </friction>\n</surface>\n```\n\n**Material-Specific Values**:\n- Rubber on concrete: μ = 0.9-1.0\n- Metal on metal: μ = 0.15-0.3\n- Ice on ice: μ = 0.02-0.05\n- Gripper pad on plastic: μ = 0.6-0.8\n\n**Anisotropic Friction** (direction-dependent):\n- Example: Tank tracks (high lateral friction, low longitudinal)\n\n### Restitution (Bounciness)\n\n**Coefficient of Restitution** (e):\n- 0: Perfectly inelastic (object sticks)\n- 0.5: Loses half its velocity\n- 1: Perfectly elastic (bounces back to same height)\n\n**In Gazebo**:\n```xml\n<surface>\n  <bounce>\n    <restitution_coefficient>0.2</restitution_coefficient>\n    <threshold>0.01</threshold>  <!-- Min velocity for bounce -->\n  </bounce>\n</surface>\n```\n\n**Use Cases**:\n- Ball: e = 0.7-0.9\n- Cube on table: e = 0.1-0.3\n- Soft gripper: e = 0.0-0.1\n\n### Contact Compliance\n\n**Hard Contact** (default):\n- Instantaneous collision resolution\n- No penetration allowed\n- Can cause instability (jitter, explosions)\n\n**Soft Contact** (compliant):\n- Allow small penetration (spring-damper model)\n- More stable, especially for stacking\n- Adds compliance (objects \"squish\")\n\n**Configuration**:\n```xml\n<surface>\n  <contact>\n    <ode>\n      <kp>1e6</kp>     <!-- Contact stiffness (N/m) -->\n      <kd>100.0</kd>   <!-- Contact damping (N·s/m) -->\n      <max_vel>0.01</max_vel>  <!-- Max penetration velocity -->\n      <min_depth>0.001</min_depth>  <!-- Allowed penetration -->\n    </ode>\n  </contact>\n</surface>\n```\n\n**Tuning**:\n- High `kp` (stiff): Realistic but can jitter\n- Low `kp` (soft): Stable but objects sink\n- Rule of thumb: `kp = object_mass / desired_penetration`\n\n### Contact Instabilities\n\n**Problem**: Stacked objects vibrate or explode\n\n**Causes**:\n- Timestep too large (`max_step_size > 0.001`)\n- Insufficient solver iterations (`iters < 50`)\n- Conflicting contacts (box corners on uneven ground)\n\n**Solutions**:\n1. Reduce timestep: `max_step_size = 0.0005` (slower)\n2. Increase iterations: `iters = 100` (more accurate)\n3. Increase damping: `kd = 200` (more energy loss)\n4. Use DART engine (better contact solver)\n5. Simplify geometry (rounded corners instead of sharp edges)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 11,
        "chapter_title": "Chapter 2: Gazebo Physics",
        "section": "2.2 Contact Modeling",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 375,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-gazebo-physics.md"
      }
    },
    {
      "content": "### Camera\n\n**Types**:\n- **Monocular**: Single RGB camera\n- **Stereo**: Two cameras for depth\n- **Depth**: RGB-D (Kinect-style)\n\n**Plugin**:\n```xml\n<sensor name=\"camera\" type=\"camera\">\n  <update_rate>30</update_rate>\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>  <!-- 60 degrees -->\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>100</far>\n    </clip>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.007</stddev>  <!-- Per-channel noise -->\n    </noise>\n  </camera>\n  <plugin name=\"camera_controller\" filename=\"libgazebo_ros_camera.so\">\n    <ros>\n      <remapping>image_raw:=camera/image</remapping>\n    </ros>\n  </plugin>\n</sensor>\n```\n\n**Realism Enhancements**:\n- Lens distortion: Radial/tangential coefficients\n- Motion blur: Simulate fast camera motion\n- Rolling shutter: Row-by-row exposure (CMOS sensors)\n- Auto-exposure: Dynamic brightness adjustment\n\n### LiDAR (2D/3D)\n\n**2D LiDAR** (e.g., SICK, Hokuyo):\n```xml\n<sensor name=\"lidar\" type=\"gpu_ray\">\n  <update_rate>10</update_rate>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>720</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.01</stddev>\n    </noise>\n  </ray>\n  <plugin name=\"lidar_controller\" filename=\"libgazebo_ros_ray_sensor.so\">\n    <ros>\n      <remapping>~/out:=scan</remapping>\n    </ros>\n  </plugin>\n</sensor>\n```\n\n**3D LiDAR** (Velodyne, Ouster):\n- Add `<vertical>` scan parameters\n- Higher computational cost (100k+ points/sec)\n\n**GPU Acceleration**:\n- `type=\"gpu_ray\"` uses GPU ray-tracing\n- 10-100x faster than CPU version\n\n### IMU (Inertial Measurement Unit)\n\n**Measures**:\n- Linear acceleration (m/s²)\n- Angular velocity (rad/s)\n- (Optional) Orientation (from magnetometer)\n\n**Plugin**:\n```xml\n<sensor name=\"imu\" type=\"imu\">\n  <update_rate>100</update_rate>\n  <imu>\n    <angular_velocity>\n      <x>\n        <noise type=\"gaussian\">\n          <mean>0</mean>\n          <stddev>0.009</stddev>  <!-- Gyro noise (rad/s) -->\n          <bias_mean>0.00075</bias_mean>\n          <bias_stddev>0.0000008</bias_stddev>\n        </noise>\n      </x>\n      <!-- Repeat for y, z -->\n    </angular_velocity>\n    <linear_acceleration>\n      <x>\n        <noise type=\"gaussian\">\n          <mean>0</mean>\n          <stddev>0.017</stddev>  <!-- Accel noise (m/s²) -->\n          <bias_mean>0.1</bias_mean>\n          <bias_stddev>0.001</bias_stddev>\n        </noise>\n      </x>\n      <!-- Repeat for y, z -->\n    </linear_acceleration>\n  </imu>\n  <plugin name=\"imu_plugin\" filename=\"libgazebo_ros_imu_sensor.so\">\n    <ros>\n      <remapping>~/out:=imu/data</remapping>\n    </ros>\n  </plugin>\n</sensor>\n```\n\n**Noise Modeling**:\n- **White noise**: Random fluctuations\n- **Bias**: Constant offset (drifts over time)\n- **Bias instability**: Slow bias changes\n\n**Calibration**: Measure real IMU noise, configure sim to match\n\n### Joint Encoders\n\n**Measures**: Joint positions and velocities\n\n**Plugin** (via `libgazebo_ros2_control`):\n- Automatically publishes `sensor_msgs/JointState`\n- No explicit sensor tag needed (part of robot URDF)\n\n**Noise**:\n```xml\n<joint name=\"elbow_joint\">\n  <dynamics damping=\"0.1\"/>\n  <sensor_noise>0.001</sensor_noise>  <!-- Position noise (rad) -->\n</joint>\n```\n\n### Force/Torque Sensors\n\n**Measures**: Forces and torques at joints (e.g., wrist force sensor)\n\n**Plugin**:\n```xml\n<sensor name=\"force_torque\" type=\"force_torque\">\n  <update_rate>100</update_rate>\n  <force_torque>\n    <frame>child</frame>  <!-- Measure in child link frame -->\n    <measure_direction>child_to_parent</measure_direction>\n  </force_torque>\n  <plugin name=\"ft_plugin\" filename=\"libgazebo_ros_ft_sensor.so\"/>\n</sensor>\n```\n\n**Use Cases**: Compliant control, grasp force regulation\n\n```mermaid\ngraph TB\n    subgraph Gazebo[\"Gazebo Simulation\"]\n        World[World<br/>SDF File]\n        Physics[Physics Engine<br/>ODE/Bullet/DART]\n        Rendering[Rendering Engine<br/>Ogre2]\n    end\n\n    subgraph Plugins[\"Gazebo Plugins\"]\n        ModelPlugin[Model Plugin<br/>Control robots]\n        SensorPlugin[Sensor Plugin<br/>Camera, LiDAR]\n        WorldPlugin[World Plugin<br/>Environment logic]\n        SystemPlugin[System Plugin<br/>ROS 2 bridge]\n    end\n\n    subgraph ROS2[\"ROS 2\"]\n        Topics[Topics<br/>/cmd_vel, /scan]\n        Services[Services<br/>/spawn_entity]\n        Actions[Actions<br/>/navigate]\n    end\n\n    World --> Physics\n    Physics --> Rendering\n    \n    World --> ModelPlugin\n    World --> SensorPlugin\n    World --> WorldPlugin\n    World --> SystemPlugin\n\n    ModelPlugin -->|Publish| Topics\n    SensorPlugin -->|Publish| Topics\n    Topics -->|Subscribe| ModelPlugin\n    Services <--> SystemPlugin\n    Actions <--> SystemPlugin\n\n    style Gazebo fill:#cce1ff\n    style Plugins fill:#ffffcc\n    style ROS2 fill:#ccffcc\n```\n\n**Figure 2.1**: Gazebo plugin architecture. Plugins attach to world, models, or sensors and interface with ROS 2 for bidirectional communication. The physics engine computes dynamics, while plugins handle I/O and control logic.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 11,
        "chapter_title": "Chapter 2: Gazebo Physics",
        "section": "2.3 Sensor Simulation",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 457,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-gazebo-physics.md"
      }
    },
    {
      "content": "**Physics Engines**: ODE (stable), Bullet (fast), DART (accurate manipulation)\n**Contact**: Friction (μ), restitution (e), compliance (kp, kd) for realistic interactions\n**Sensors**: Camera, LiDAR, IMU, joint encoders with noise models for sim-to-real",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 11,
        "chapter_title": "Chapter 2: Gazebo Physics",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 31,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-gazebo-physics.md"
      }
    },
    {
      "content": "**Exercise 2.1**: Compare ODE, Bullet, and DART for a robot arm picking a fragile object. Run the same scenario in all three engines and measure:\n- Task success rate\n- Contact forces (should be gentle)\n- Simulation speed (real-time factor)\nWhich engine is best? Why?\n\n**Exercise 2.2**: Tune contact parameters for realistic box stacking. Start with default Gazebo settings (often unstable). Adjust kp (stiffness), kd (damping), friction (μ), and solver iterations until a 5-box stack is stable for 10 seconds. Document your final parameters.\n\n**Exercise 2.3**: Add sensor noise to a camera plugin to match real sensor specifications. If your camera has 0.007 stddev per-channel noise (from datasheet), configure Gazebo to match. Capture images, measure noise, and verify it matches.\n\n**Exercise 2.4**: Create a custom Gazebo plugin that publishes joint torques to ROS 2. The plugin should:\n- Read torques from ArticulationBody or physics engine\n- Publish to `/joint_torques` topic\n- Update at 100 Hz\nTest by commanding the robot and observing torque changes.\n\n**Next**: Chapter 3 covers Unity for high-fidelity rendering and animation.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 11,
        "chapter_title": "Chapter 2: Gazebo Physics",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 174,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-gazebo-physics.md"
      }
    },
    {
      "content": "---\nsidebar_position: 3\ntitle: \"Chapter 3: Unity Animation and Rendering\"\ndescription: Unity Robotics Hub, articulation bodies, and ML-Agents\ntags: [unity, robotics-hub, articulation-bodies, ml-agents, rendering]\n---\n\n# Chapter 3: Unity Animation and Rendering",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 12,
        "chapter_title": "Chapter 3: Unity Animation and Rendering",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 32,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-unity-animation.md"
      }
    },
    {
      "content": "1. Use Unity Robotics Hub to import URDF and connect to ROS 2\n2. Configure articulation bodies for realistic robot physics\n3. Compare Unity and Gazebo to select the appropriate tool",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 12,
        "chapter_title": "Chapter 3: Unity Animation and Rendering",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 31,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-unity-animation.md"
      }
    },
    {
      "content": "### Overview\n\n**Unity Robotics Hub**: Open-source package for integrating Unity with ROS/ROS 2\n\n**Components**:\n1. **URDF Importer**: Convert URDF → Unity GameObjects\n2. **ROS-TCP-Connector**: Bidirectional message passing (Unity ↔ ROS 2)\n3. **Visualizations**: Sensor data rendering (point clouds, markers)\n\n**Why Unity for Robotics?**\n- **Photorealistic rendering**: Real-time ray tracing, global illumination\n- **High-performance**: Optimized for games (60+ FPS with complex scenes)\n- **Cross-platform**: Desktop, mobile, VR/AR, cloud\n- **Asset ecosystem**: 3D models, environments (Unity Asset Store)\n- **ML-Agents**: Reinforcement learning framework (Unity ML-Agents)\n\n### Installation\n\n```bash\n# Unity Hub + Unity Editor (2021.3 LTS recommended)\n# Install via Unity Hub: https://unity.com/download\n\n# Add packages via Package Manager\n# 1. URDF Importer: com.unity.robotics.urdf-importer\n# 2. ROS-TCP-Connector: com.unity.robotics.ros-tcp-connector\n```\n\n**ROS 2 Side**:\n```bash\nsudo apt install ros-humble-ros-tcp-endpoint\nros2 run ros_tcp_endpoint default_server_endpoint --ros-args -p ROS_IP:=0.0.0.0\n```\n\n### URDF Import Workflow\n\n**Step 1: Prepare URDF**\n- Ensure mesh files (STL, OBJ, DAE) are referenced correctly\n- Use relative paths: `package://robot_description/meshes/base.stl`\n\n**Step 2: Import in Unity**\n```\nAssets → Import Robot from URDF\nSelect URDF file\nConfigure import settings:\n  - Axis Convention: Y-up (Unity) or Z-up (ROS)\n  - Mesh Decomposition: Convex (for collision)\n  - Material: Standard (PBR)\n```\n\n**Step 3: Articulation Body Conversion**\n- Unity auto-creates ArticulationBody components for joints\n- Hierarchy: Root (immovable) → links connected by articulations\n\n**Example Hierarchy**:\n```\nRobot_Root (ArticulationBody: Fixed)\n├── base_link\n│   ├── left_wheel (ArticulationBody: Continuous)\n│   ├── right_wheel (ArticulationBody: Continuous)\n│   └── camera_link (ArticulationBody: Fixed)\n```\n\n### ROS-TCP-Connector Setup\n\n**Unity Side** (C# script):\n```csharp\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Geometry;\n\npublic class RobotController : MonoBehaviour\n{\n    ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher<TwistMsg>(\"cmd_vel\");\n        ros.Subscribe<TwistMsg>(\"unity/cmd_vel\", OnCmdVel);\n    }\n\n    void OnCmdVel(TwistMsg msg)\n    {\n        // Apply velocity to robot\n        float linear = (float)msg.linear.x;\n        float angular = (float)msg.angular.z;\n        // ...control logic\n    }\n\n    void PublishOdometry()\n    {\n        var odom = new TwistMsg();\n        // ...populate odom\n        ros.Publish(\"odom\", odom);\n    }\n}\n```\n\n**ROS 2 Side**:\n```bash\n# Start TCP endpoint\nros2 run ros_tcp_endpoint default_server_endpoint\n\n# Publish to Unity\nros2 topic pub /unity/cmd_vel geometry_msgs/Twist \"{linear: {x: 0.5}, angular: {z: 0.1}}\"\n\n# Subscribe from Unity\nros2 topic echo /odom\n```\n\n**Message Types**: Auto-generated from ROS .msg files",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 12,
        "chapter_title": "Chapter 3: Unity Animation and Rendering",
        "section": "3.1 Unity Robotics Hub",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 348,
        "has_code_block": true,
        "has_math": false,
        "source_file": "03-unity-animation.md"
      }
    },
    {
      "content": "### Articulation Bodies\n\n**Unity Physics**: PhysX (NVIDIA's physics engine)\n\n**ArticulationBody**: Unity's component for robotic joints (replaces older Rigidbody chains)\n\n**Advantages**:\n- Reduced jitter (stable multi-body dynamics)\n- Direct joint control (position, velocity, force)\n- Better for high-DOF robots (humanoids, manipulators)\n\n**Joint Types**:\n- **Fixed**: Welded (sensor mounts)\n- **Revolute**: Hinge with limits\n- **Prismatic**: Linear slide\n- **Spherical**: Ball joint (3-DOF)\n\n**Configuration** (via Unity Inspector):\n```\nArticulationBody Component:\n  - Anchor Position/Rotation: Joint origin\n  - Joint Type: Revolute/Prismatic/Fixed\n  - X/Y/Z Motion: Locked/Limited/Free\n  - Limits: Min/Max angle (revolute), Min/Max distance (prismatic)\n  - Stiffness: PD controller gains\n  - Damping: Velocity damping\n```\n\n**Control Modes**:\n1. **Force**: Apply torque, physics computes motion\n2. **Acceleration**: Set target acceleration\n3. **Velocity**: PD controller to target velocity\n4. **Position**: PD controller to target angle\n\n**Example** (setting joint velocity):\n```csharp\nArticulationBody joint = GetComponent<ArticulationBody>();\nvar drive = joint.xDrive;\ndrive.target = 2.0f;  // rad/s (for revolute)\ndrive.targetVelocity = 2.0f;\njoint.xDrive = drive;\n```\n\n### High-Fidelity Rendering\n\n**Physically Based Rendering (PBR)**:\n- Metallic/Roughness workflow\n- Realistic materials (metal, plastic, rubber)\n- HDRP (High Definition Render Pipeline) for ray tracing\n\n**Lighting**:\n- **Directional Light**: Sunlight (parallel rays)\n- **Point Light**: Omnidirectional (lightbulb)\n- **Spot Light**: Cone-shaped (flashlight)\n- **Area Light**: Soft shadows (HDRP only)\n- **Global Illumination**: Indirect lighting (baked lightmaps)\n\n**Post-Processing**:\n- Bloom, lens flare, motion blur\n- Color grading, vignette\n- Depth of field (focus on robot, blur background)\n\n**Camera Simulation**:\n```csharp\n// Capture camera feed\nCamera cam = GetComponent<Camera>();\nRenderTexture rt = new RenderTexture(640, 480, 24);\ncam.targetTexture = rt;\ncam.Render();\n\n// Read pixels → publish to ROS\nTexture2D tex = new Texture2D(640, 480, TextureFormat.RGB24, false);\nRenderTexture.active = rt;\ntex.ReadPixels(new Rect(0, 0, 640, 480), 0, 0);\ntex.Apply();\n\n// Convert to sensor_msgs/Image\nbyte[] bytes = tex.GetRawTextureData();\n// ...publish via ROS-TCP-Connector\n```\n\n### Unity ML-Agents\n\n**ML-Agents**: Reinforcement learning framework for Unity\n\n**Architecture**:\n```\n┌─────────────────┐\n│ Python Trainer  │ (PyTorch/TensorFlow)\n└────────┬────────┘\n         │ RPC (grpc)\n┌────────┴────────┐\n│ Unity C# Agent  │ (Environment)\n└─────────────────┘\n```\n\n**Agent Example** (robot navigation):\n```csharp\nusing Unity.MLAgents;\nusing Unity.MLAgents.Sensors;\nusing Unity.MLAgents.Actuators;\n\npublic class RobotAgent : Agent\n{\n    public Transform target;\n\n    public override void OnEpisodeBegin()\n    {\n        // Reset robot position, randomize target\n        transform.position = new Vector3(0, 0, 0);\n        target.position = Random.insideUnitSphere * 5f;\n    }\n\n    public override void CollectObservations(VectorSensor sensor)\n    {\n        // State: robot pos, target pos, robot velocity\n        sensor.AddObservation(transform.position);\n        sensor.AddObservation(target.position);\n        sensor.AddObservation(GetComponent<Rigidbody>().velocity);\n    }\n\n    public override void OnActionReceived(ActionBuffers actions)\n    {\n        // Actions: linear/angular velocity\n        float linear = actions.ContinuousActions[0];\n        float angular = actions.ContinuousActions[1];\n\n        // Apply to robot\n        rb.AddForce(transform.forward * linear * speed);\n        rb.AddTorque(Vector3.up * angular * torque);\n\n        // Reward: distance to target\n        float dist = Vector3.Distance(transform.position, target.position);\n        if (dist < 0.5f) {\n            SetReward(1.0f);\n            EndEpisode();\n        } else {\n            SetReward(-0.01f * dist);\n        }\n    }\n}\n```\n\n**Training** (Python):\n```bash\nmlagents-learn config.yaml --run-id=robot_nav_01\n# Play in Unity Editor → agent learns\n```\n\n**Benefits**:\n- Parallel environments (1000s of robots in one scene)\n- Curriculum learning (progressively harder tasks)\n- Imitation learning (demonstrations → policy)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 12,
        "chapter_title": "Chapter 3: Unity Animation and Rendering",
        "section": "3.2 Animation and Rendering",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 478,
        "has_code_block": true,
        "has_math": false,
        "source_file": "03-unity-animation.md"
      }
    },
    {
      "content": "### When to Use Unity\n\n**Advantages**:\n- **Visuals**: Photorealistic (ray tracing, PBR)\n- **Performance**: Faster rendering (optimized for games)\n- **Cross-platform**: VR/AR support, mobile deployment\n- **Asset ecosystem**: Pre-built environments, objects\n- **ML-Agents**: Integrated RL framework\n\n**Use Cases**:\n1. **Synthetic data generation**: Train computer vision models\n2. **Human-robot interaction**: VR teleoperation, AR overlays\n3. **Marketing/demos**: Photorealistic videos for presentations\n4. **Large-scale RL**: 1000s of parallel agents\n5. **Mobile/AR robotics**: Visualize robot in real environment (ARCore/ARKit)\n\n### When to Use Gazebo\n\n**Advantages**:\n- **Physics accuracy**: DART engine for manipulation\n- **ROS 2 integration**: Native, no TCP bridge\n- **Sensor fidelity**: Realistic LiDAR, radar models\n- **Open-source**: Free, community-maintained\n- **Determinism**: Reproducible results (critical for research)\n\n**Use Cases**:\n1. **Contact-rich tasks**: Manipulation, grasping, assembly\n2. **Legged locomotion**: Accurate foot contacts\n3. **ROS 2 ecosystem**: Seamless integration with Nav2, MoveIt\n4. **Research**: Deterministic experiments, open-source transparency\n5. **Deployment testing**: Closest to hardware behavior\n\n### Comparison Table\n\n| Feature | Unity | Gazebo |\n|---------|-------|--------|\n| **Rendering** | Photorealistic | Basic |\n| **Physics Accuracy** | Medium | High (DART) |\n| **ROS 2 Integration** | TCP bridge | Native |\n| **Performance** | Fast (60+ FPS) | Medium |\n| **RL Support** | ML-Agents | External (Isaac, etc) |\n| **License** | Free (Personal) | Open-source |\n| **Best For** | Vision, VR/AR, ML | Manipulation, locomotion |\n\n### Hybrid Approach\n\n**Scenario**: Train vision policy in Unity, validate control in Gazebo\n\n**Workflow**:\n1. **Unity**: Generate synthetic images (domain randomization)\n2. **Train**: Object detection model (YOLOv8, Mask R-CNN)\n3. **Gazebo**: Deploy detector + motion planner\n4. **Validate**: Pick-and-place task with accurate physics\n5. **Transfer**: Real robot with fine-tuning\n\n**Example**: Amazon Robotics Challenge\n- Unity for vision training (synthetic shelves with varied products)\n- Gazebo for grasp planning (DART physics for contact)\n- Real robot deployment",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 12,
        "chapter_title": "Chapter 3: Unity Animation and Rendering",
        "section": "3.3 Unity vs Gazebo",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 301,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-unity-animation.md"
      }
    },
    {
      "content": "**Unity Robotics Hub**: URDF import, ROS-TCP-Connector for ROS 2 integration\n**Articulation Bodies**: PhysX-based robot physics with joint control\n**Unity vs Gazebo**: Unity (visuals, RL, VR) vs Gazebo (accuracy, ROS 2 native)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 12,
        "chapter_title": "Chapter 3: Unity Animation and Rendering",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 31,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-unity-animation.md"
      }
    },
    {
      "content": "**Exercise 3.1**: Import a robot URDF into Unity and configure ArticulationBody components for a 3-DOF arm (base rotation, shoulder, elbow). Set joint limits, stiffness, and damping. Verify the robot can reach a target position using position control.\n\n**Exercise 3.2**: Implement a Unity ML-Agents environment for robot navigation. The agent should:\n- Observe: Robot position, target position, obstacle positions\n- Act: Linear and angular velocity commands\n- Reward: +1 for reaching target, -0.01 per timestep, -1 for collision\nTrain with PPO for 1M steps and measure success rate.\n\n**Exercise 3.3**: Generate synthetic data for object detection. Create a Unity scene with 100 random objects (varying poses, lighting, backgrounds). Capture 10k images with bounding box annotations. Train YOLOv8 and measure mAP@0.5.\n\n**Exercise 3.4**: Compare Unity and Gazebo for a specific robot task (your choice). Measure:\n- Rendering performance (FPS)\n- Physics accuracy (e.g., contact force errors)\n- ROS 2 integration latency (message round-trip time)\n- Ease of use (subjective, but document setup time)\nPresent findings in a table with recommendations.\n\n**Next**: Chapter 4 integrates Gazebo and Unity with ROS 2 for multi-simulator workflows.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 12,
        "chapter_title": "Chapter 3: Unity Animation and Rendering",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 181,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-unity-animation.md"
      }
    },
    {
      "content": "---\nsidebar_position: 4\ntitle: \"Chapter 4: ROS 2 Integration\"\ndescription: Gazebo-ROS 2 bridge, Unity-ROS 2 integration, and multi-simulator workflows\ntags: [ros2, gazebo, unity, bridge, integration, multi-simulator]\n---\n\n# Chapter 4: ROS 2 Integration",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 13,
        "chapter_title": "Chapter 4: ROS 2 Integration",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 33,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-ros2-integration.md"
      }
    },
    {
      "content": "1. Configure Gazebo-ROS 2 bridge for seamless message passing\n2. Integrate Unity with ROS 2 using ROS-TCP-Connector\n3. Design multi-simulator workflows combining Gazebo and Unity",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 13,
        "chapter_title": "Chapter 4: ROS 2 Integration",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 25,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-ros2-integration.md"
      }
    },
    {
      "content": "### Architecture\n\n**ros_gz** (formerly ros_ign): Bridge between Gazebo and ROS 2\n\n**Components**:\n1. **ros_gz_bridge**: Message translator (Gazebo ↔ ROS 2)\n2. **ros_gz_sim**: Launch Gazebo from ROS 2\n3. **ros_gz_image**: Image transport optimization\n\n### Message Translation\n\n**Gazebo Topics** use Protobuf messages (e.g., `gz.msgs.Twist`)\n**ROS 2 Topics** use ROS messages (e.g., `geometry_msgs/Twist`)\n\n**Bridge**: Converts between formats automatically\n\n**Example Bridge Configuration**:\n```yaml\n# bridge_config.yaml\n- ros_topic_name: \"/cmd_vel\"\n  gz_topic_name: \"/model/robot/cmd_vel\"\n  ros_type_name: \"geometry_msgs/msg/Twist\"\n  gz_type_name: \"gz.msgs.Twist\"\n  direction: ROS_TO_GZ\n\n- ros_topic_name: \"/scan\"\n  gz_topic_name: \"/world/warehouse/model/robot/link/lidar_link/sensor/lidar/scan\"\n  ros_type_name: \"sensor_msgs/msg/LaserScan\"\n  gz_type_name: \"gz.msgs.LaserScan\"\n  direction: GZ_TO_ROS\n\n- ros_topic_name: \"/camera/image\"\n  gz_topic_name: \"/camera\"\n  ros_type_name: \"sensor_msgs/msg/Image\"\n  gz_type_name: \"gz.msgs.Image\"\n  direction: GZ_TO_ROS\n```\n\n**Launch Bridge**:\n```bash\nros2 run ros_gz_bridge parameter_bridge --ros-args -p config_file:=bridge_config.yaml\n```\n\n**Bidirectional** (ROS ↔ Gazebo):\n```yaml\ndirection: BIDIRECTIONAL\n```\n\n### Clock Synchronization\n\n**Problem**: Simulation time ≠ wall-clock time (pause, slow-motion, fast-forward)\n\n**Solution**: `/clock` topic with `use_sim_time:=true`\n\n**Gazebo publishes clock**:\n```xml\n<!-- In world SDF -->\n<plugin filename=\"libgz_ros2_control-system.so\" name=\"gz_ros2_control\">\n  <ros>\n    <namespace>/robot</namespace>\n    <argument>--ros-args</argument>\n    <argument>-p use_sim_time:=true</argument>\n  </ros>\n</plugin>\n```\n\n**ROS 2 nodes use sim time**:\n```bash\nros2 run my_package my_node --ros-args -p use_sim_time:=true\n```\n\n**Bridge automatically forwards /clock**:\n```bash\nros2 run ros_gz_bridge parameter_bridge /clock@rosgraph_msgs/msg/Clock[gz.msgs.Clock\n```\n\n### Plugin-Based Integration\n\n**Alternative**: Gazebo ROS 2 plugins (no bridge needed)\n\n**Camera Plugin**:\n```xml\n<sensor name=\"camera\" type=\"camera\">\n  <plugin filename=\"libgazebo_ros_camera.so\" name=\"camera_driver\">\n    <ros>\n      <namespace>/robot</namespace>\n      <remapping>image_raw:=camera/image</remapping>\n      <remapping>camera_info:=camera/info</remapping>\n    </ros>\n    <update_rate>30</update_rate>\n  </plugin>\n</sensor>\n```\n\n**Advantages**:\n- No separate bridge process\n- Lower latency (direct ROS 2 publishing)\n- Simpler launch files\n\n**Disadvantages**:\n- Plugin must exist for each sensor type\n- Less flexible than bridge (bridge can map any topic)\n\n### Debugging Bridge\n\n```bash\n# List Gazebo topics\ngz topic -l\n\n# Echo Gazebo topic\ngz topic -e -t /model/robot/cmd_vel\n\n# Check ROS 2 topics\nros2 topic list\n\n# Monitor data flow\nros2 topic echo /cmd_vel\ngz topic -e -t /model/robot/cmd_vel\n```\n\n**Common Issues**:\n- **Type mismatch**: ROS and Gazebo message types don't align\n- **Topic name errors**: Check exact topic names (case-sensitive)\n- **Clock not synced**: Nodes not using `use_sim_time:=true`",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 13,
        "chapter_title": "Chapter 4: ROS 2 Integration",
        "section": "4.1 Gazebo-ROS 2 Bridge",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 313,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-ros2-integration.md"
      }
    },
    {
      "content": "### ROS-TCP-Connector Architecture\n\n**Components**:\n1. **Unity Package**: `com.unity.robotics.ros-tcp-connector`\n2. **ROS 2 Package**: `ros_tcp_endpoint`\n\n**Communication**:\n```\nUnity (C#) ←→ TCP Socket ←→ ROS 2 (Python/C++)\n```\n\n**Advantages**:\n- No native ROS 2 dependency (Unity runs standalone)\n- Cross-platform (Windows, macOS, Linux, mobile)\n- Firewall-friendly (single TCP port)\n\n**Disadvantages**:\n- Extra latency (TCP serialization)\n- Not real-time (best-effort delivery)\n\n### Setup\n\n**Unity Side**:\n1. Install package: Window → Package Manager → Add by Git URL: `https://github.com/Unity-Technologies/ROS-TCP-Connector.git?path=/com.unity.robotics.ros-tcp-connector`\n2. Configure: Robotics → ROS Settings\n   - ROS IP Address: `127.0.0.1` (localhost) or remote IP\n   - ROS Port: `10000` (default)\n   - Protocol: ROS 2\n\n**ROS 2 Side**:\n```bash\nsudo apt install ros-humble-ros-tcp-endpoint\nsource /opt/ros/humble/setup.bash\nros2 run ros_tcp_endpoint default_server_endpoint --ros-args -p ROS_IP:=0.0.0.0\n```\n\n### Message Generation\n\n**Auto-generate C# classes from ROS .msg files**:\n\n```bash\n# In ROS 2 package\ncd ~/ros2_ws/src/my_msgs\nros2 run ros_tcp_endpoint msg_srv_gen.py --package my_msgs --output-dir ~/unity_project/Assets/RosMessages\n\n# Generates: Assets/RosMessages/my_msgs/msg/MyCustomMsg.cs\n```\n\n**Import in Unity**: Assets auto-refresh, C# classes ready to use\n\n### Publishing from Unity\n\n```csharp\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Geometry;\n\npublic class VelocityPublisher : MonoBehaviour\n{\n    ROSConnection ros;\n    public float publishRate = 10f;  // Hz\n    float timer = 0f;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher<TwistMsg>(\"cmd_vel\");\n    }\n\n    void Update()\n    {\n        timer += Time.deltaTime;\n        if (timer > 1f / publishRate)\n        {\n            var msg = new TwistMsg\n            {\n                linear = new Vector3Msg { x = 1.0, y = 0.0, z = 0.0 },\n                angular = new Vector3Msg { x = 0.0, y = 0.0, z = 0.5 }\n            };\n            ros.Publish(\"cmd_vel\", msg);\n            timer = 0f;\n        }\n    }\n}\n```\n\n### Subscribing in Unity\n\n```csharp\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\n\npublic class LaserScanSubscriber : MonoBehaviour\n{\n    void Start()\n    {\n        ROSConnection.GetOrCreateInstance().Subscribe<LaserScanMsg>(\"scan\", OnScanReceived);\n    }\n\n    void OnScanReceived(LaserScanMsg msg)\n    {\n        Debug.Log($\"Received {msg.ranges.Length} laser points\");\n\n        // Visualize in Unity (draw lines)\n        for (int i = 0; i < msg.ranges.Length; i++)\n        {\n            float angle = msg.angle_min + i * msg.angle_increment;\n            float range = msg.ranges[i];\n            Vector3 point = new Vector3(\n                range * Mathf.Cos(angle),\n                0,\n                range * Mathf.Sin(angle)\n            );\n            Debug.DrawRay(transform.position, point, Color.red, 0.1f);\n        }\n    }\n}\n```\n\n### Service Calls\n\n**Unity Client**:\n```csharp\nusing RosMessageTypes.Std;\n\npublic void CallService()\n{\n    var request = new TriggerRequest();\n    ROSConnection.GetOrCreateInstance().SendServiceMessage<TriggerResponse>(\n        \"reset_odometry\",\n        request,\n        OnServiceResponse\n    );\n}\n\nvoid OnServiceResponse(TriggerResponse response)\n{\n    if (response.success)\n        Debug.Log(\"Odometry reset successful\");\n}\n```\n\n**ROS 2 Server**:\n```python\nfrom std_srvs.srv import Trigger\n\ndef reset_callback(request, response):\n    # Reset odometry\n    response.success = True\n    response.message = \"Reset complete\"\n    return response\n\nnode.create_service(Trigger, 'reset_odometry', reset_callback)\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 13,
        "chapter_title": "Chapter 4: ROS 2 Integration",
        "section": "4.2 Unity-ROS 2 Integration",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 390,
        "has_code_block": true,
        "has_math": true,
        "source_file": "04-ros2-integration.md"
      }
    },
    {
      "content": "### Hybrid Simulation\n\n**Concept**: Use multiple simulators for different aspects\n\n**Example 1: Vision + Manipulation**\n- **Unity**: Generate synthetic RGB-D images (domain randomization)\n- **Gazebo**: Simulate grasping with DART physics\n- **Bridge**: Share object poses via ROS 2\n\n**Workflow**:\n1. Unity publishes camera images (`/camera/image`)\n2. Vision pipeline (ROS 2 node) detects objects\n3. Gazebo receives object poses, simulates grasp\n4. Grasp result sent back to Unity for visualization\n\n**Architecture**:\n```\n┌───────────┐         ┌──────────────┐         ┌───────────┐\n│  Unity    │ /camera │ ROS 2 Vision │ /object │  Gazebo   │\n│ (Visuals) ├────────>│   Pipeline   ├────────>│ (Physics) │\n└───────────┘         └──────────────┘         └───────────┘\n```\n\n### Parallel Simulations\n\n**Use Case**: Distributed training (RL with 1000s of agents)\n\n**Setup**:\n- Launch N Unity instances on different machines\n- Each publishes to unique namespace: `/robot_001/cmd_vel`, `/robot_002/cmd_vel`\n- Central ROS 2 trainer subscribes to all\n\n**Example**:\n```bash\n# Machine 1: Launch 10 Unity instances\nfor i in {1..10}; do\n    unity-server --namespace /robot_00$i &\ndone\n\n# Machine 2: Python trainer\nros2 run my_trainer ppo_trainer --num-robots 10\n```\n\n### Sequential Workflow\n\n**Use Case**: Asset pipeline (model → test → deploy)\n\n**Stages**:\n1. **CAD → URDF**: Design in SolidWorks, export URDF\n2. **Unity**: Visual validation (check mesh quality, textures)\n3. **Gazebo**: Physics validation (dynamics, collisions)\n4. **Real Hardware**: Deploy with ros2_control\n\n**Automation** (CI/CD):\n```yaml\n# GitHub Actions\n- name: Import URDF to Unity\n  run: unity-headless --import urdf/robot.urdf\n- name: Validate in Gazebo\n  run: ros2 launch gazebo validate_robot.launch.py\n- name: Build Docker image\n  run: docker build -t robot:latest .\n```\n\n### Cross-Simulator Verification\n\n**Scenario**: Ensure sim-to-real consistency\n\n**Method**:\n1. Run same controller in Unity, Gazebo, and hardware\n2. Record trajectories (joint angles, velocities)\n3. Compare:\n   - Unity vs Gazebo (physics consistency)\n   - Gazebo vs Hardware (sim-to-real gap)\n\n**Metrics**:\n- Mean trajectory error\n- Success rate (task completion)\n- Stability (oscillations, failures)\n\n**Example**:\n```python\n# Log data from all sources\nunity_traj = load_csv('unity_trajectory.csv')\ngazebo_traj = load_csv('gazebo_trajectory.csv')\nhardware_traj = load_csv('hardware_trajectory.csv')\n\n# Compare\nunity_gazebo_rmse = np.sqrt(np.mean((unity_traj - gazebo_traj)**2))\ngazebo_hardware_rmse = np.sqrt(np.mean((gazebo_traj - hardware_traj)**2))\n\nprint(f\"Unity-Gazebo error: {unity_gazebo_rmse}\")\nprint(f\"Gazebo-Hardware error: {gazebo_hardware_rmse}\")\n```\n\n### Best Practices\n\n1. **Namespace isolation**: Use unique namespaces per simulator (`/unity/`, `/gazebo/`)\n2. **Clock sync**: All sims should publish `/clock`, nodes use `use_sim_time`\n3. **Message versioning**: Pin ROS message package versions (avoid drift)\n4. **Graceful fallback**: If Unity fails, Gazebo continues (don't tightly couple)\n5. **Logging**: Record all data (rosbag2) for offline analysis\n\n```mermaid\nsequenceDiagram\n    participant Unity\n    participant TCP as ROS-TCP-Endpoint<br/>(Python/C++)\n    participant ROS2 as ROS 2 Network<br/>(DDS)\n\n    Note over Unity,ROS2: Initialization\n    Unity->>TCP: Connect (TCP Socket)\n    TCP->>Unity: Connection ACK\n\n    Note over Unity,ROS2: Publishing from Unity\n    Unity->>Unity: Create TwistMsg\n    Unity->>TCP: Serialize & Send (TCP)\n    TCP->>ROS2: Publish to /cmd_vel (DDS)\n\n    Note over Unity,ROS2: Subscribing in Unity\n    ROS2->>TCP: LaserScan on /scan (DDS)\n    TCP->>Unity: Serialize & Send (TCP)\n    Unity->>Unity: Deserialize LaserScanMsg\n    Unity->>Unity: Render in Scene\n\n    Note over Unity,ROS2: Service Call\n    Unity->>TCP: Service Request (Trigger)\n    TCP->>ROS2: Call /reset_odometry\n    ROS2->>TCP: Service Response\n    TCP->>Unity: Response (success)\n\n    Note over Unity,ROS2: Clock Sync\n    ROS2->>TCP: /clock tick (DDS)\n    TCP->>Unity: Simulation time\n    Unity->>Unity: Update Time.time\n```\n\n**Figure 4.1**: Unity-ROS 2 message flow via ROS-TCP-Connector. Unity communicates over TCP with the endpoint, which translates to/from ROS 2 DDS messages. Clock synchronization ensures Unity and ROS 2 nodes use the same simulation time.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 13,
        "chapter_title": "Chapter 4: ROS 2 Integration",
        "section": "4.3 Multi-Simulator Workflows",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 523,
        "has_code_block": true,
        "has_math": true,
        "source_file": "04-ros2-integration.md"
      }
    },
    {
      "content": "**Gazebo-ROS 2**: `ros_gz_bridge` for message translation, plugins for direct integration\n**Unity-ROS 2**: ROS-TCP-Connector over TCP, message generation from .msg files\n**Multi-Simulator**: Hybrid (Unity visuals + Gazebo physics), parallel (distributed RL), sequential (asset pipeline)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 13,
        "chapter_title": "Chapter 4: ROS 2 Integration",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 33,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-ros2-integration.md"
      }
    },
    {
      "content": "**Exercise 4.1**: Configure the Gazebo-ROS 2 bridge for a robot with camera, LiDAR, and IMU. Create a bridge config YAML that:\n- Publishes camera images at 30 Hz\n- Publishes LiDAR scans at 10 Hz\n- Publishes IMU data at 100 Hz\n- Subscribes to /cmd_vel for control\nVerify all topics with `ros2 topic hz`.\n\n**Exercise 4.2**: Synchronize Gazebo and ROS 2 clocks. Launch Gazebo in slow-motion (0.5x real-time) and verify a ROS 2 node receives `/clock` updates. Log timestamps and confirm they match simulation time, not wall-clock time.\n\n**Exercise 4.3**: Implement Unity-ROS 2 communication for a camera feed. Create a Unity script that:\n- Captures RGB images at 10 Hz\n- Publishes to `/unity/camera/image` as `sensor_msgs/Image`\n- Includes correct timestamp in header\nSubscribe in ROS 2 and visualize with `rqt_image_view`.\n\n**Exercise 4.4**: Design a hybrid simulation workflow. Use Unity for vision (object detection training with synthetic data) and Gazebo for manipulation (DART physics for grasping). The workflow should:\n1. Unity detects objects → publishes poses\n2. ROS 2 node plans grasp based on poses\n3. Gazebo executes grasp and reports success/failure\n4. Unity updates visualization\nMeasure end-to-end latency from detection to grasp completion.\n\n**Module 2 Complete!** Next modules cover NVIDIA Isaac (Module 3), VLA models (Module 4), and capstone project (Module 5).",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 13,
        "chapter_title": "Chapter 4: ROS 2 Integration",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 213,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-ros2-integration.md"
      }
    },
    {
      "content": "---\nsidebar_position: 1\ntitle: \"Chapter 1: NVIDIA Isaac Overview\"\ndescription: Isaac ecosystem, PhysX engine, and Replicator for synthetic data\ntags: [isaac-sim, isaac-ros, physx, replicator, synthetic-data]\n---\n\n# Chapter 1: NVIDIA Isaac Overview",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 14,
        "chapter_title": "Chapter 1: NVIDIA Isaac Overview",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 32,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-overview.md"
      }
    },
    {
      "content": "1. Understand the NVIDIA Isaac ecosystem and its components\n2. Leverage GPU-accelerated PhysX for scalable physics simulation\n3. Use Replicator for synthetic data generation",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 14,
        "chapter_title": "Chapter 1: NVIDIA Isaac Overview",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 24,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-overview.md"
      }
    },
    {
      "content": "```mermaid\ngraph TB\n    subgraph Cloud[\"NVIDIA Cloud\"]\n        Training[Model Training<br/>RTX GPUs]\n        Data[Synthetic Data<br/>Replicator]\n    end\n\n    subgraph IsaacSim[\"Isaac Sim (Simulation)\"]\n        PhysX[PhysX Engine<br/>GPU Physics]\n        RTX[RTX Rendering<br/>Photorealistic]\n        Replicator[Replicator<br/>Domain Randomization]\n        Python[Python API<br/>Automation]\n    end\n\n    subgraph IsaacROS[\"Isaac ROS (Deployment)\"]\n        Perception[Perception<br/>TensorRT, NITROS]\n        SLAM[Navigation<br/>Visual SLAM, Nvblox]\n        Manipulation[Manipulation<br/>DOPE, cuRobo]\n    end\n\n    subgraph Hardware[\"Robot Hardware\"]\n        Jetson[NVIDIA Jetson<br/>Edge AI]\n        Sensors[Cameras, LiDAR<br/>IMU]\n        Actuators[Motors, Grippers]\n    end\n\n    Cloud --> IsaacSim\n    IsaacSim -->|Train Policies| Training\n    IsaacSim -->|Generate Data| Data\n    Data -->|Train Models| Training\n    Training -->|Deploy| IsaacROS\n\n    IsaacROS --> Jetson\n    Sensors --> Perception\n    Perception --> SLAM\n    SLAM --> Manipulation\n    Manipulation --> Actuators\n\n    style Cloud fill:#ccffcc\n    style IsaacSim fill:#cce1ff\n    style IsaacROS fill:#ffffcc\n    style Hardware fill:#ffe1cc\n```\n\n**Figure 1.1**: NVIDIA Isaac ecosystem showing the complete workflow from simulation (Isaac Sim) to deployment (Isaac ROS on Jetson hardware). Synthetic data and trained models flow from cloud/sim to edge devices.\n\n### Overview\n\n**NVIDIA Isaac**: End-to-end platform for AI-powered robotics, from simulation to deployment.\n\n**Core Components**:\n1. **Isaac Sim**: GPU-accelerated robot simulation (Omniverse-based)\n2. **Isaac ROS**: ROS 2 packages for perception, navigation, manipulation (hardware-accelerated)\n3. **Isaac Manipulator**: Pre-trained models and workflows for robotic arms\n4. **Isaac AMR**: Autonomous mobile robot stack (SLAM, navigation, fleet management)\n\n### Isaac Sim\n\n**Built on NVIDIA Omniverse**: Collaborative 3D design platform using USD (Universal Scene Description)\n\n**Key Features**:\n- **PhysX 5**: GPU physics (1000s of parallel robots)\n- **RTX Ray Tracing**: Photorealistic rendering for vision training\n- **Replicator**: Synthetic data generation with domain randomization\n- **ROS/ROS 2 Bridge**: Seamless integration\n- **Python API**: Scripting for automation\n\n**Use Cases**:\n- Train RL policies (Isaac Gym style, but with full rendering)\n- Generate synthetic datasets (object detection, segmentation)\n- Test navigation algorithms in virtual warehouses\n- Validate manipulation before hardware deployment\n\n**System Requirements**:\n- GPU: NVIDIA RTX (2000 series or newer)\n- VRAM: 8GB minimum, 16GB+ recommended\n- OS: Ubuntu 20.04/22.04, Windows 10/11\n\n### Isaac ROS\n\n**Hardware-Accelerated ROS 2 Packages** (run on Jetson or x86+GPU)\n\n**Key Packages**:\n- **isaac_ros_dnn_inference**: GPU-accelerated deep learning (TensorRT)\n- **isaac_ros_image_proc**: Image processing (debayering, rectification) on GPU\n- **isaac_ros_visual_slam**: Real-time visual SLAM (GPU-optimized)\n- **isaac_ros_object_detection**: Object detection (DOPE, CenterPose)\n- **isaac_ros_nvblox**: 3D reconstruction and mapping (GPU voxel hashing)\n\n**Advantages**:\n- **Performance**: 10-100x faster than CPU (e.g., SegFormer: 5 FPS CPU → 50 FPS GPU)\n- **NITROS**: Zero-copy message passing (no serialization overhead)\n- **Jetson Optimized**: Runs on Jetson Orin (edge deployment)\n\n**Example**: Visual SLAM at 30 Hz with 1MP camera (vs 5 Hz CPU)\n\n### Isaac Manipulator\n\n**Pre-trained Models** for pick-and-place:\n- Grasp detection (6-DOF grasps)\n- Object pose estimation (DOPE for known objects)\n- Motion planning (cuRobo - GPU-accelerated trajectory optimization)\n\n**Workflows**:\n1. Train object detector in Isaac Sim (synthetic data)\n2. Deploy on Jetson with Isaac ROS\n3. Plan grasps with cuRobo (collision-free, optimized)\n\n### Isaac AMR\n\n**Autonomous Mobile Robot Stack**:\n- **Visual SLAM**: isaac_ros_visual_slam (GPU-accelerated ORB-SLAM3)\n- **LiDAR SLAM**: Gmapping, Cartographer integration\n- **Path Planning**: Nav2 integration (GPU-accelerated costmaps)\n- **Fleet Management**: Multi-robot coordination\n\n**Deployment**: Warehouse robots, delivery bots, inspection drones",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 14,
        "chapter_title": "Chapter 1: NVIDIA Isaac Overview",
        "section": "1.1 Isaac Ecosystem",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 476,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-overview.md"
      }
    },
    {
      "content": "### GPU-Accelerated Physics\n\n**PhysX 5**: NVIDIA's physics engine (used in games, robotics, VFX)\n\n**Key Features**:\n- **GPU Acceleration**: Simulate 1000s of robots in parallel\n- **Rigid Body Dynamics**: Articulations, contacts, collisions\n- **Soft Bodies**: Deformable objects (limited in Isaac Sim)\n- **Determinism**: Reproducible results (critical for RL)\n\n**Scalability**:\n- Single robot: 1000 FPS (faster than real-time)\n- 1000 robots: 10-100 FPS (parallel RL training)\n- Trade-off: Accuracy vs throughput\n\n### Comparison with Other Engines\n\n| Feature | PhysX (Isaac) | DART (Gazebo) | MuJoCo |\n|---------|---------------|---------------|--------|\n| **GPU Acceleration** | Yes | No | No |\n| **Parallel Robots** | 1000s | 1 | 1 |\n| **Contact Accuracy** | High | Very High | Very High |\n| **Speed (single robot)** | Very Fast | Medium | Fast |\n| **Best For** | RL, synthetic data | Manipulation | Control research |\n\n**When to use PhysX/Isaac Sim**:\n- Reinforcement learning (need >1M samples)\n- Synthetic data at scale (100k images)\n- Multi-robot scenarios (fleet simulation)\n\n**When to use DART/Gazebo**:\n- High-precision manipulation (assembly, surgery)\n- Hardware validation (closest to real physics)\n\n### PhysX Configuration\n\n**In Isaac Sim**:\n```python\nfrom omni.isaac.core import World\n\n# Create world with PhysX\nworld = World(physics_dt=1/60, rendering_dt=1/60)\n\n# Get PhysX scene\nphysx_scene = world.get_physics_context().get_physics_scene()\n\n# Configure solver\nphysx_scene.set_solver_type(\"TGS\")  # Temporal Gauss-Seidel (accurate)\nphysx_scene.set_solver_position_iteration_count(4)\nphysx_scene.set_solver_velocity_iteration_count(1)\n\n# Enable GPU dynamics\nphysx_scene.enable_gpu_dynamics(flag=True)\n```\n\n**Parameters**:\n- `physics_dt`: Simulation timestep (1/60 = 16.7ms)\n- `solver_type`: TGS (accurate) vs PGS (fast)\n- `position_iterations`: Higher = more accurate contacts (slower)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 14,
        "chapter_title": "Chapter 1: NVIDIA Isaac Overview",
        "section": "1.2 PhysX Engine",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 248,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-overview.md"
      }
    },
    {
      "content": "### Synthetic Data Generation\n\n**Replicator**: Procedural data generation framework in Isaac Sim\n\n**Problem**: Real-world data is expensive (labeling, collection, diversity)\n\n**Solution**: Generate 100k+ labeled images in simulation\n- Randomize: Object poses, lighting, backgrounds, camera angles\n- Auto-label: Bounding boxes, segmentation masks, depth\n\n### Domain Randomization\n\n**Randomize Visual Appearance**:\n```python\nimport omni.replicator.core as rep\n\n# Randomize object textures\nwith rep.trigger.on_frame():\n    rep.randomizer.texture(\n        objects=rep.get.prims(path_pattern=\"/World/Objects/*\"),\n        textures=rep.utils.get_textures(\"textures/\"),\n        project_uvw=True\n    )\n```\n\n**Randomize Lighting**:\n```python\nwith rep.trigger.on_frame():\n    rep.randomizer.light(\n        lights=rep.get.prims(semantics=[(\"class\", \"light\")]),\n        intensity=(500, 3000),\n        temperature=(3000, 6500),  # Color temperature (K)\n        angle=(0, 180)\n    )\n```\n\n**Randomize Camera**:\n```python\nwith rep.trigger.on_frame():\n    rep.randomizer.camera_position(\n        cameras=rep.get.prims(semantics=[(\"class\", \"camera\")]),\n        position_range=((-5, 5), (1, 3), (-5, 5))\n    )\n```\n\n### Data Collection Workflow\n\n**1. Setup Scene**:\n```python\nimport omni.isaac.core.utils.stage as stage_utils\nfrom omni.isaac.core.objects import DynamicCuboid\n\n# Load environment (warehouse, kitchen)\nstage_utils.add_reference_to_stage(usd_path=\"warehouse.usd\", prim_path=\"/World/Warehouse\")\n\n# Add objects to detect\nfor i in range(10):\n    DynamicCuboid(\n        prim_path=f\"/World/Objects/cube_{i}\",\n        size=0.1,\n        position=(i * 0.2, 0, 0.5)\n    )\n```\n\n**2. Configure Replicator**:\n```python\n# Camera setup\ncamera = rep.create.camera(position=(0, 2, 5), look_at=(0, 0, 0))\n\n# Annotators (what data to capture)\nrp = rep.create.render_product(camera, resolution=(640, 480))\nwriter = rep.WriterRegistry.get(\"BasicWriter\")\n\nwriter.initialize(\n    output_dir=\"output/synthetic_data\",\n    rgb=True,\n    bounding_box_2d_tight=True,  # Tight bounding boxes\n    semantic_segmentation=True,\n    distance_to_camera=True  # Depth\n)\nwriter.attach([rp])\n```\n\n**3. Run Data Generation**:\n```python\nrep.orchestrator.run(num_frames=10000)\n# Generates 10k images with labels\n```\n\n**Output**: `output/synthetic_data/`\n- `rgb_*.png`: RGB images\n- `bounding_box_2d_tight_*.json`: Bounding boxes (COCO format)\n- `semantic_segmentation_*.png`: Segmentation masks\n- `distance_to_camera_*.npy`: Depth maps\n\n### Use Cases\n\n**Object Detection**:\n- Train YOLOv8 on 50k synthetic images\n- Fine-tune on 1k real images\n- Deploy with Isaac ROS\n\n**Grasp Planning**:\n- Generate depth images of objects\n- Train grasp pose estimator (6-DOF)\n- Deploy on robot arm\n\n**Navigation**:\n- Randomize warehouse layouts\n- Train semantic segmentation (floor, obstacles, shelves)\n- Deploy visual navigation policy\n\n### Best Practices\n\n1. **Start with real data**: Collect 1k real images, measure baseline\n2. **Match distributions**: Ensure synthetic data resembles real (lighting, backgrounds)\n3. **Incremental randomization**: Start simple, add complexity\n4. **Validate**: Test on real robot frequently (avoid overfitting to sim)\n5. **Mix real + synthetic**: Best results with 80% synthetic + 20% real",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 14,
        "chapter_title": "Chapter 1: NVIDIA Isaac Overview",
        "section": "1.3 Replicator",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 334,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-overview.md"
      }
    },
    {
      "content": "**Exercise 1.1**: Isaac Sim Installation and Setup\n- Install Isaac Sim via NVIDIA Omniverse Launcher\n- Verify GPU support with `nvidia-smi`\n- Run the Jetbot sample scene and measure FPS\n- Document system specs and performance\n\n**Exercise 1.2**: PhysX Configuration Experiment\n- Create a scene with 10 identical robots\n- Measure simulation FPS with different solver settings (TGS vs PGS, different iteration counts)\n- Compare GPU vs CPU physics performance\n- Plot results and analyze tradeoffs\n\n**Exercise 1.3**: Synthetic Data Generation with Replicator\n- Set up a scene with 5 different objects on a table\n- Implement visual randomization (textures, lighting, camera poses)\n- Generate 1000 labeled images (bounding boxes + segmentation)\n- Verify output format (COCO JSON) and visualize samples\n\n**Exercise 1.4**: Isaac ROS Deployment\n- Install Isaac ROS packages on Jetson Orin or x86+GPU\n- Convert a YOLOv8 model to TensorRT engine\n- Run inference on camera feed and measure FPS\n- Compare with CPU-only ROS 2 implementation",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 14,
        "chapter_title": "Chapter 1: NVIDIA Isaac Overview",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 160,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-overview.md"
      }
    },
    {
      "content": "**Isaac Ecosystem**: Sim (physics, rendering), ROS (perception, navigation), Manipulator/AMR (pre-trained models)\n**PhysX**: GPU-accelerated, 1000s of parallel robots, RL-ready\n**Replicator**: Synthetic data generation with domain randomization for vision ML\n\n**Next**: Chapter 2 covers Isaac ROS perception and planning packages.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 14,
        "chapter_title": "Chapter 1: NVIDIA Isaac Overview",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 38,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-overview.md"
      }
    },
    {
      "content": "---\nsidebar_position: 2\ntitle: \"Chapter 2: Isaac ROS Perception and Planning\"\ndescription: Hardware-accelerated perception, SLAM, and manipulation with Isaac ROS\ntags: [isaac-ros, perception, slam, manipulation, nitros]\n---\n\n# Chapter 2: Isaac ROS Perception and Planning",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 15,
        "chapter_title": "Chapter 2: Isaac ROS Perception and Planning",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 35,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-perception-planning.md"
      }
    },
    {
      "content": "1. Implement GPU-accelerated perception with Isaac ROS\n2. Deploy visual SLAM for autonomous navigation\n3. Integrate motion planning for manipulation tasks\n\n```mermaid\ngraph LR\n    Camera[Camera<br/>sensor_msgs/Image] -->|NITROS| Debayer[Debayer<br/>GPU]\n    Debayer -->|NITROS| Rectify[Rectify<br/>GPU]\n    Rectify -->|NITROS| DNN[DNN Inference<br/>TensorRT]\n    DNN --> Detections[Detections<br/>vision_msgs]\n\n    Depth[Depth Camera] -->|NITROS| Nvblox[Nvblox<br/>3D Mapping]\n    Nvblox --> Costmap[Costmap<br/>nav_msgs]\n    Costmap --> Nav2[Nav2 Planner]\n\n    IMU[IMU] --> VSLAM[Visual SLAM]\n    Camera -->|NITROS| VSLAM\n    VSLAM --> Odometry[Odometry<br/>nav_msgs]\n\n    style Camera fill:#ffe1cc\n    style DNN fill:#ccffcc\n    style Nvblox fill:#cce1ff\n    style VSLAM fill:#ffffcc\n```\n\n**Figure 2.1**: Isaac ROS perception pipeline showing zero-copy NITROS communication between GPU-accelerated nodes. All image processing stays on GPU for minimal latency.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 15,
        "chapter_title": "Chapter 2: Isaac ROS Perception and Planning",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 94,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-perception-planning.md"
      }
    },
    {
      "content": "### DNN Inference (TensorRT)\n\n**isaac_ros_dnn_inference**: GPU-accelerated deep learning\n\n**Workflow**:\n1. Train model (PyTorch, TensorFlow)\n2. Convert to ONNX\n3. Optimize with TensorRT (FP16/INT8 quantization)\n4. Deploy with Isaac ROS\n\n**Example** (Object Detection):\n```python\n# Convert PyTorch to ONNX\nimport torch\nmodel = torch.load('yolov8.pth')\ntorch.onnx.export(model, dummy_input, 'yolov8.onnx')\n\n# Optimize with TensorRT (command line)\ntrtexec --onnx=yolov8.onnx --saveEngine=yolov8.engine --fp16\n```\n\n**ROS 2 Node**:\n```python\nfrom isaac_ros_tensor_rt import TensorRTNode\n\ntensorrt_node = TensorRTNode(\n    model_file_path='yolov8.engine',\n    engine_file_path='yolov8.engine',\n    input_tensor_names=['images'],\n    input_binding_names=['images'],\n    output_tensor_names=['output0'],\n    output_binding_names=['output0']\n)\n```\n\n**Performance**:\n- YOLOv8: 5 FPS (CPU) → 60 FPS (GPU/TensorRT)\n- SegFormer: 2 FPS (CPU) → 30 FPS (GPU)\n\n### NITROS (Zero-Copy Messaging)\n\n**Problem**: ROS 2 message serialization overhead (10-50ms latency)\n\n**NITROS**: GPU-to-GPU message passing (no CPU copy)\n\n**Benefits**:\n- Lower latency (1-5ms)\n- Higher throughput (100+ FPS pipelines)\n- Reduced CPU load\n\n**Example**: Camera → Debayer → Rectify → DNN\n- Standard ROS 2: 50ms latency\n- NITROS: 10ms latency",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 15,
        "chapter_title": "Chapter 2: Isaac ROS Perception and Planning",
        "section": "2.1 Isaac ROS Perception",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 147,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-perception-planning.md"
      }
    },
    {
      "content": "### Visual SLAM\n\n**isaac_ros_visual_slam**: GPU-accelerated ORB-SLAM3\n\n**Features**:\n- Real-time: 30 Hz with 1MP camera\n- Stereo or monocular\n- Loop closure detection\n- Map persistence (save/load)\n\n**Setup**:\n```bash\nros2 launch isaac_ros_visual_slam isaac_ros_visual_slam.launch.py\n```\n\n**Subscribed Topics**:\n- `/camera/image_raw` (sensor_msgs/Image)\n- `/camera/camera_info` (sensor_msgs/CameraInfo)\n- `/imu` (sensor_msgs/Imu) - optional\n\n**Published Topics**:\n- `/visual_slam/tracking/odometry` (nav_msgs/Odometry)\n- `/visual_slam/tracking/vo_pose` (geometry_msgs/PoseStamped)\n- `/visual_slam/vis/observations_cloud` (sensor_msgs/PointCloud2)\n\n**Use Cases**:\n- Indoor navigation without LiDAR\n- Drone SLAM (lightweight, visual-only)\n- AR/VR tracking\n\n### Nvblox (3D Mapping)\n\n**isaac_ros_nvblox**: GPU voxel hashing for 3D reconstruction\n\n**Features**:\n- Real-time mapping (30 Hz with depth camera)\n- ESDF (Euclidean Signed Distance Field) for path planning\n- Mesh output for visualization\n\n**Pipeline**:\n1. Depth camera → Point cloud\n2. Nvblox integrates into voxel map\n3. ESDF computed on GPU\n4. Planner queries ESDF for collision-free paths\n\n**Example**:\n```bash\nros2 launch isaac_ros_nvblox isaac_ros_nvblox.launch.py\n```\n\n**Integration with Nav2**:\n- Nvblox publishes costmap\n- Nav2 uses for obstacle avoidance",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 15,
        "chapter_title": "Chapter 2: Isaac ROS Perception and Planning",
        "section": "2.2 Isaac ROS Navigation",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 151,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-perception-planning.md"
      }
    },
    {
      "content": "### Object Pose Estimation (DOPE)\n\n**DOPE** (Deep Object Pose Estimation): 6-DOF pose from RGB\n\n**Workflow**:\n1. Train DOPE on synthetic data (Isaac Sim)\n2. Deploy with Isaac ROS on Jetson\n3. Get object pose → plan grasp\n\n**Training in Isaac Sim**:\n```python\n# Generate synthetic data with Replicator\nimport omni.replicator.core as rep\n\n# Randomize object pose\nwith rep.trigger.on_frame():\n    rep.randomizer.scatter_3d(\n        objects=rep.get.prims(path_pattern=\"/World/Objects/*\"),\n        surface_prims=\"/World/Ground\",\n        check_for_collisions=True\n    )\n```\n\n**Inference**:\n```bash\nros2 launch isaac_ros_dope isaac_ros_dope.launch.py\n```\n\n**Output**: `geometry_msgs/PoseStamped` for each detected object\n\n### Motion Planning (cuRobo)\n\n**cuRobo**: GPU-accelerated motion planning\n\n**Features**:\n- Trajectory optimization (100-1000x faster than MoveIt)\n- Collision checking on GPU\n- Multi-arm support\n\n**Example**:\n```python\nfrom curobo.wrap.reacher import MotionGenConfig, MotionGen\n\n# Load robot\nconfig = MotionGenConfig.from_urdf(\"robot.urdf\")\nmotion_gen = MotionGen(config)\n\n# Plan to goal pose\nresult = motion_gen.plan_single(\n    goal_pose=target_pose,\n    start_state=current_joint_state\n)\n\n# Execute trajectory\nfor waypoint in result.trajectory:\n    robot.set_joint_positions(waypoint)\n```\n\n**Performance**: 10ms planning time (vs 100ms+ for OMPL/MoveIt)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 15,
        "chapter_title": "Chapter 2: Isaac ROS Perception and Planning",
        "section": "2.3 Isaac ROS Manipulation",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 145,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-perception-planning.md"
      }
    },
    {
      "content": "**Exercise 2.1**: TensorRT Model Conversion Pipeline\n- Train a simple object detection model (YOLOv8 or MobileNet-SSD) on COCO dataset\n- Convert model from PyTorch → ONNX → TensorRT engine\n- Benchmark inference speed (FP32, FP16, INT8) on target hardware\n- Document accuracy vs speed tradeoffs\n\n**Exercise 2.2**: NITROS Performance Analysis\n- Set up a perception pipeline: Camera → Debayer → Rectify → DNN Inference\n- Measure latency with standard ROS 2 messages vs NITROS\n- Profile CPU/GPU usage with both approaches\n- Create performance comparison chart\n\n**Exercise 2.3**: Visual SLAM Deployment\n- Deploy Isaac ROS Visual SLAM on a mobile robot or test rig\n- Collect odometry data while navigating a structured environment\n- Evaluate drift over time and loop closure performance\n- Compare with wheel odometry and sensor fusion results\n\n**Exercise 2.4**: cuRobo Motion Planning\n- Set up cuRobo for a robotic arm (e.g., UR5, Franka Emika)\n- Define collision geometry for workspace obstacles\n- Plan trajectories to 10 random goal poses\n- Measure planning time and compare with OMPL/MoveIt",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 15,
        "chapter_title": "Chapter 2: Isaac ROS Perception and Planning",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 171,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-perception-planning.md"
      }
    },
    {
      "content": "**Perception**: TensorRT for DNN inference, NITROS for zero-copy, 10-100x speedup\n**Navigation**: Visual SLAM (30 Hz), Nvblox (real-time 3D mapping), Nav2 integration\n**Manipulation**: DOPE (6-DOF pose), cuRobo (GPU motion planning)\n\n**Next**: Chapter 3 covers sim-to-real transfer strategies with Isaac Sim.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 15,
        "chapter_title": "Chapter 2: Isaac ROS Perception and Planning",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 39,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-perception-planning.md"
      }
    },
    {
      "content": "---\nsidebar_position: 3\ntitle: \"Chapter 3: Sim-to-Real Transfer\"\ndescription: Domain randomization, system identification, and transfer learning in Isaac\ntags: [sim-to-real, domain-randomization, isaac-sim, transfer-learning]\n---\n\n# Chapter 3: Sim-to-Real Transfer",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 16,
        "chapter_title": "Chapter 3: Sim-to-Real Transfer",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 29,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-sim-to-real.md"
      }
    },
    {
      "content": "1. Apply domain randomization techniques in Isaac Sim\n2. Conduct system identification workflows\n3. Implement transfer learning strategies for robot policies\n\n```mermaid\ngraph TB\n    subgraph Simulation[\"Isaac Sim Training\"]\n        DR[Domain Randomization<br/>Textures, Lighting, Physics]\n        Train[Train Policy<br/>PPO, 1M steps]\n        Validate[Validate in Sim<br/>Success rate]\n    end\n\n    subgraph Transfer[\"Transfer Process\"]\n        SysID[System Identification<br/>Measure real params]\n        FineTune[Fine-Tune Policy<br/>Real robot, 10k steps]\n    end\n\n    subgraph Deployment[\"Real Robot\"]\n        Deploy[Deploy Policy]\n        Monitor[Monitor Performance]\n        Feedback[Collect Failure Cases]\n    end\n\n    DR --> Train\n    Train --> Validate\n    Validate -->|Good| Transfer\n    Validate -->|Poor| DR\n\n    SysID --> FineTune\n    FineTune --> Deploy\n    Deploy --> Monitor\n    Monitor -->|Failures| Feedback\n    Feedback -->|Update Sim| DR\n\n    style Simulation fill:#cce1ff\n    style Transfer fill:#ffffcc\n    style Deployment fill:#ccffcc\n```\n\n**Figure 3.1**: Sim-to-real workflow showing iterative process: train in simulation with domain randomization, transfer to real robot with system ID and fine-tuning, monitor performance and update simulation based on failures.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 16,
        "chapter_title": "Chapter 3: Sim-to-Real Transfer",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 133,
        "has_code_block": true,
        "has_math": false,
        "source_file": "03-sim-to-real.md"
      }
    },
    {
      "content": "### Visual Randomization\n\n**Randomize Textures**:\n```python\nimport omni.replicator.core as rep\n\n# Randomize object materials\nwith rep.trigger.on_frame():\n    rep.randomizer.materials(\n        objects=rep.get.prims(path_pattern=\"/World/Objects/*\"),\n        materials=rep.utils.get_materials(\"materials/\"),\n        project_uvw=True\n    )\n```\n\n**Randomize Lighting**:\n```python\n# Random light positions and intensities\nwith rep.trigger.on_frame():\n    rep.randomizer.light(\n        lights=rep.get.prims(semantics=[(\"class\", \"light\")]),\n        intensity=(500, 3000),\n        temperature=(2500, 7500),\n        position=((-10, 10), (2, 5), (-10, 10))\n    )\n```\n\n**Camera Randomization**:\n```python\n# Random camera pose, FOV, exposure\nwith rep.trigger.on_frame():\n    rep.randomizer.camera(\n        cameras=rep.get.prims(semantics=[(\"class\", \"camera\")]),\n        focal_length=(18, 85),  # mm\n        f_stop=(1.4, 16),\n        focus_distance=(0.5, 10)\n    )\n```\n\n### Physics Randomization\n\n**Object Properties**:\n```python\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom pxr import UsdPhysics\n\ncube_prim = get_prim_at_path(\"/World/Cube\")\nphysics_api = UsdPhysics.RigidBodyAPI.Apply(cube_prim)\n\n# Randomize mass\nimport random\nmass = random.uniform(0.05, 0.15)  # kg\nphysics_api.CreateMassAttr(mass)\n```\n\n**Friction**:\n```python\nphysics_material = UsdPhysics.MaterialAPI.Apply(cube_prim)\nphysics_material.CreateStaticFrictionAttr(random.uniform(0.3, 0.9))\nphysics_material.CreateDynamicFrictionAttr(random.uniform(0.2, 0.8))\n```\n\n**Joint Dynamics**:\n```python\n# Randomize joint damping/stiffness\njoint = robot.get_articulation().get_joints_by_name(\"elbow_joint\")[0]\njoint.set_joint_damping(random.uniform(0.1, 1.0))\njoint.set_joint_stiffness(random.uniform(50, 200))\n```\n\n### Structured Randomization\n\n**Curriculum Learning**: Progressively harder randomization\n\n```python\nclass DomainRandomizer:\n    def __init__(self, difficulty=0.0):\n        self.difficulty = difficulty  # 0.0 (easy) to 1.0 (hard)\n\n    def randomize_mass(self, nominal_mass):\n        variation = 0.05 + 0.45 * self.difficulty  # 5% to 50% variation\n        return nominal_mass * random.uniform(1 - variation, 1 + variation)\n\n    def randomize_friction(self):\n        if self.difficulty < 0.5:\n            return random.uniform(0.6, 0.9)  # Easy: high friction\n        else:\n            return random.uniform(0.1, 0.9)  # Hard: full range\n\n# Increase difficulty during training\nrandomizer = DomainRandomizer(difficulty=0.0)\nfor epoch in range(100):\n    randomizer.difficulty = min(1.0, epoch / 100.0)\n    # Train policy with current difficulty\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 16,
        "chapter_title": "Chapter 3: Sim-to-Real Transfer",
        "section": "3.1 Domain Randomization in Isaac Sim",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 223,
        "has_code_block": true,
        "has_math": false,
        "source_file": "03-sim-to-real.md"
      }
    },
    {
      "content": "### Parameter Estimation\n\n**Workflow**:\n1. Run experiments on real robot (collect data)\n2. Fit simulation parameters to match observations\n3. Validate: Run same experiment in sim, compare\n\n**Example** (Friction Identification):\n\n**Step 1**: Real robot data collection\n```python\n# Apply known torques, measure joint velocities\ntorques = [0.1, 0.2, 0.3, 0.5, 1.0]  # N·m\nmeasured_velocities = []  # rad/s\n\nfor torque in torques:\n    robot.apply_joint_torque(\"elbow\", torque)\n    time.sleep(1.0)  # Let reach steady state\n    vel = robot.get_joint_velocity(\"elbow\")\n    measured_velocities.append(vel)\n```\n\n**Step 2**: Simulation parameter fitting\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef simulate_joint(friction_coeff, torque):\n    # Simple friction model: torque = friction * velocity\n    # At steady state: applied_torque = friction * velocity\n    return torque / friction_coeff\n\ndef error_function(friction_coeff):\n    errors = []\n    for i, torque in enumerate(torques):\n        sim_vel = simulate_joint(friction_coeff, torque)\n        real_vel = measured_velocities[i]\n        errors.append((sim_vel - real_vel) ** 2)\n    return np.mean(errors)\n\n# Optimize\nresult = minimize(error_function, x0=0.1, bounds=[(0.01, 1.0)])\noptimal_friction = result.x[0]\n```\n\n**Step 3**: Update Isaac Sim\n```python\njoint.set_joint_damping(optimal_friction)\n```\n\n### Automated Calibration\n\n**Loop**: Real robot → Sim → Compare → Adjust\n\n```python\nclass SystemIDWorkflow:\n    def __init__(self, real_robot, sim_robot):\n        self.real = real_robot\n        self.sim = sim_robot\n\n    def run_experiment(self, commands):\n        # Run on real robot\n        real_data = self.real.execute(commands)\n        # Run in sim\n        sim_data = self.sim.execute(commands)\n        return real_data, sim_data\n\n    def calibrate(self, param_ranges, n_iterations=10):\n        best_params = None\n        best_error = float('inf')\n\n        for i in range(n_iterations):\n            # Sample parameters\n            params = {k: random.uniform(*v) for k, v in param_ranges.items()}\n            self.sim.set_parameters(params)\n\n            # Run experiments\n            real_data, sim_data = self.run_experiment(test_commands)\n\n            # Compute error\n            error = np.linalg.norm(real_data - sim_data)\n\n            if error < best_error:\n                best_error = error\n                best_params = params\n\n        return best_params\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 16,
        "chapter_title": "Chapter 3: Sim-to-Real Transfer",
        "section": "3.2 System Identification Workflows",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 259,
        "has_code_block": true,
        "has_math": false,
        "source_file": "03-sim-to-real.md"
      }
    },
    {
      "content": "### Pre-training in Simulation\n\n**Workflow**:\n1. Train policy in Isaac Sim (millions of samples)\n2. Fine-tune on real robot (1000s of samples)\n3. Deploy\n\n**Example** (Grasping):\n```python\n# 1. Train in simulation\npolicy = PPO(\"MlpPolicy\", env=isaac_sim_env)\npolicy.learn(total_timesteps=1_000_000)\npolicy.save(\"policy_sim.zip\")\n\n# 2. Fine-tune on real robot\npolicy = PPO.load(\"policy_sim.zip\")\npolicy.set_env(real_robot_env)\npolicy.learn(total_timesteps=10_000)  # Fine-tune\npolicy.save(\"policy_real.zip\")\n```\n\n**Advantages**:\n- Leverage sim for exploration\n- Real robot data only for final polish\n- Faster convergence (warm start from sim)\n\n### Sim-to-Real Adaptation\n\n**Progressive Networks**: Separate columns for sim and real\n```python\nclass ProgressivePolicy(nn.Module):\n    def __init__(self):\n        self.sim_column = nn.Sequential(...)  # Frozen after sim training\n        self.real_column = nn.Sequential(...)  # Trainable\n\n    def forward(self, state):\n        sim_features = self.sim_column(state)\n        real_features = self.real_column(state)\n        combined = torch.cat([sim_features, real_features], dim=-1)\n        return self.policy_head(combined)\n```\n\n**Domain Adversarial Training**: Learn domain-invariant features\n```python\n# Discriminator predicts sim vs real\ndiscriminator = DomainClassifier(input_dim=128)\n\n# Policy learns to fool discriminator\npolicy_loss = task_loss - 0.1 * discriminator_loss\n```\n\n### Residual Learning\n\n**Idea**: Policy = Sim policy + Residual (learned on real robot)\n\n```python\nclass ResidualPolicy:\n    def __init__(self, sim_policy_path):\n        self.sim_policy = load_policy(sim_policy_path)\n        self.sim_policy.freeze()  # Don't update\n\n        self.residual = nn.Sequential(\n            nn.Linear(obs_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, action_dim)\n        )\n\n    def get_action(self, obs):\n        sim_action = self.sim_policy(obs)\n        residual = self.residual(obs)\n        return sim_action + 0.1 * residual  # Small correction\n```\n\n**Train**: Only `residual` on real robot, keep `sim_policy` frozen\n\n**Advantages**:\n- Sim policy handles coarse behavior\n- Residual corrects for sim-to-real gap\n- Safer (residual bounded)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 16,
        "chapter_title": "Chapter 3: Sim-to-Real Transfer",
        "section": "3.3 Transfer Learning Strategies",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 231,
        "has_code_block": true,
        "has_math": false,
        "source_file": "03-sim-to-real.md"
      }
    },
    {
      "content": "**Exercise 3.1**: Visual Domain Randomization for Object Detection\n- Create an Isaac Sim scene with 10 household objects\n- Implement texture, lighting, and camera randomization with Replicator\n- Generate 5000 synthetic images with bounding box annotations\n- Train YOLOv8 on synthetic data and test on real images\n\n**Exercise 3.2**: Physics Parameter Identification\n- Set up a simple pendulum in both Isaac Sim and real hardware\n- Apply known torques and measure joint velocities on real system\n- Use optimization to fit simulation parameters (damping, friction)\n- Validate by comparing real and sim trajectories\n\n**Exercise 3.3**: Curriculum-Based Domain Randomization\n- Implement a curriculum randomizer for a robot grasping task\n- Start with minimal randomization (5% mass variation, high friction)\n- Progressively increase difficulty over training epochs\n- Plot success rate vs difficulty level\n\n**Exercise 3.4**: Residual Policy Learning\n- Train a base policy for robot navigation in Isaac Sim (1M steps)\n- Fine-tune with residual learning on real robot (10k steps)\n- Compare performance with: (a) sim-only, (b) real-only, (c) residual approach\n- Analyze which behaviors come from sim vs residual",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 16,
        "chapter_title": "Chapter 3: Sim-to-Real Transfer",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 180,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-sim-to-real.md"
      }
    },
    {
      "content": "**Domain Randomization**: Visual (textures, lighting), physics (mass, friction), curriculum learning\n**System ID**: Parameter estimation from real data, automated calibration loops\n**Transfer Learning**: Pre-training in sim, progressive networks, residual policies\n\n**Next**: Chapter 4 covers control loops with RL and imitation learning in Isaac.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 16,
        "chapter_title": "Chapter 3: Sim-to-Real Transfer",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 42,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-sim-to-real.md"
      }
    },
    {
      "content": "---\nsidebar_position: 4\ntitle: \"Chapter 4: Control Loops\"\ndescription: Reinforcement learning, imitation learning, and hybrid control with Isaac\ntags: [reinforcement-learning, imitation-learning, isaac-gym, control]\n---\n\n# Chapter 4: Control Loops",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 17,
        "chapter_title": "Chapter 4: Control Loops",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 29,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-control-loops.md"
      }
    },
    {
      "content": "1. Train RL policies in Isaac Gym for parallel robot learning\n2. Implement imitation learning pipelines from demonstrations\n3. Design hybrid control architectures combining classical and learning-based methods",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 17,
        "chapter_title": "Chapter 4: Control Loops",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 28,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-control-loops.md"
      }
    },
    {
      "content": "### Isaac Gym Integration\n\n**Isaac Gym**: GPU-accelerated RL environment (part of Isaac Sim)\n\n**Features**:\n- **Parallel Environments**: 1000s of robots simultaneously\n- **Throughput**: 100k steps/second (vs 100 steps/second single robot)\n- **GPU Tensors**: Observations/actions stay on GPU (no CPU transfer)\n\n**Setup**:\n```python\nfrom omni.isaac.gym.vec_env import VecEnvBase\n\nclass HumanoidEnv(VecEnvBase):\n    def __init__(self, cfg, sim_device, graphics_device_id, headless):\n        self.num_envs = cfg[\"env\"][\"numEnvs\"]  # e.g., 4096\n        self.num_obs = 108  # Observation dim\n        self.num_actions = 21  # Action dim (joint targets)\n\n        super().__init__(cfg, sim_device, graphics_device_id, headless)\n\n    def create_sim(self):\n        # Create Isaac Sim physics scene\n        self.sim = gymapi.acquire_sim()\n        # ... setup\n\n    def reset(self):\n        # Reset 4096 robots in parallel\n        self.gym.set_actor_root_state_tensor(self.sim, self.root_states)\n        return self.obs\n\n    def step(self, actions):\n        # Apply actions to all robots\n        self.gym.set_dof_position_target_tensor(self.sim, actions)\n\n        # Step physics (all robots in parallel)\n        self.gym.simulate(self.sim)\n\n        # Compute observations and rewards (on GPU)\n        self.obs = self.compute_observations()\n        self.rew = self.compute_rewards()\n        self.done = self.compute_dones()\n\n        return self.obs, self.rew, self.done, {}\n```\n\n### Training with PPO\n\n**Algorithm**: Proximal Policy Optimization (stable, sample-efficient)\n\n```python\nfrom rl_games.algos_torch import torch_ext\nfrom rl_games.common import env_configurations, vecenv\n\n# Register environment\nvecenv.register('IsaacHumanoid', lambda cfg: HumanoidEnv(cfg))\n\n# Training config\nconfig = {\n    'params': {\n        'seed': 42,\n        'algo': {\n            'name': 'a2c_continuous'\n        },\n        'model': {\n            'name': 'continuous_a2c_logstd'\n        },\n        'network': {\n            'name': 'actor_critic',\n            'separate': False,\n            'mlp': {\n                'units': [256, 256, 128],\n                'activation': 'elu'\n            }\n        },\n        'config': {\n            'name': 'HumanoidWalk',\n            'env_name': 'IsaacHumanoid',\n            'num_actors': 4096,\n            'num_steps_per_env': 16,\n            'minibatch_size': 32768,\n            'learning_rate': 3e-4,\n            'horizon_length': 16,\n            'gamma': 0.99,\n            'lam': 0.95\n        }\n    }\n}\n\n# Train\nrunner = Runner()\nrunner.load(config)\nrunner.run({'train': True, 'play': False, 'checkpoint': 'runs/'})\n```\n\n**Training Time**: 1 hour to achieve walking (vs 24 hours single robot)\n\n### Reward Design\n\n**Sparse** vs **Dense** rewards:\n\n```python\ndef compute_rewards(self):\n    # Dense reward (guide learning)\n    forward_vel = self.root_states[:, 7]  # x velocity\n    upright = self.root_states[:, 3]  # z-component of quaternion\n    energy = torch.sum(torch.abs(self.joint_torques), dim=-1)\n\n    reward = (\n        2.0 * forward_vel  # Encourage forward motion\n        + 1.0 * upright  # Stay upright\n        - 0.001 * energy  # Minimize energy\n        - 5.0 * self.fallen  # Penalty for falling\n    )\n\n    return reward\n```\n\n**Termination Conditions**:\n```python\ndef compute_dones(self):\n    # Fallen: torso below threshold\n    fallen = self.root_states[:, 2] < 0.3  # z-position\n\n    # Out of bounds\n    out_of_bounds = torch.abs(self.root_states[:, :2]) > 10.0  # x, y\n\n    # Timeout\n    timeout = self.progress_buf >= self.max_episode_length\n\n    return fallen | out_of_bounds.any(dim=-1) | timeout\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 17,
        "chapter_title": "Chapter 4: Control Loops",
        "section": "4.1 Reinforcement Learning in Isaac Gym",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 371,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-control-loops.md"
      }
    },
    {
      "content": "### Behavior Cloning\n\n**Idea**: Learn policy from expert demonstrations\n\n**Workflow**:\n1. Collect expert demos (teleop, scripted, or human)\n2. Train policy: `π(a|s) ≈ expert(s)`\n3. Deploy\n\n**Data Collection**:\n```python\n# Teleoperate robot, log state-action pairs\ndemos = []\nwhile teleoperating:\n    state = robot.get_state()  # Joint positions, velocities\n    action = gamepad.get_input()  # Desired joint velocities\n    demos.append((state, action))\n\n# Save dataset\nnp.save('demos.npy', demos)\n```\n\n**Training**:\n```python\nimport torch\nimport torch.nn as nn\n\nclass BehaviorClonePolicy(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim)\n        )\n\n    def forward(self, state):\n        return self.net(state)\n\n# Load demos\ndemos = np.load('demos.npy', allow_pickle=True)\nstates = torch.tensor([d[0] for d in demos])\nactions = torch.tensor([d[1] for d in demos])\n\n# Train\npolicy = BehaviorClonePolicy(state_dim=108, action_dim=21)\noptimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)\n\nfor epoch in range(1000):\n    pred_actions = policy(states)\n    loss = nn.MSELoss()(pred_actions, actions)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n**Limitation**: Distribution mismatch (policy sees states expert never visited)\n\n### DAgger (Dataset Aggregation)\n\n**Idea**: Iteratively query expert, expand dataset\n\n```python\n# Round 1: Train on initial demos\npolicy.train(expert_demos)\n\nfor i in range(10):  # 10 rounds\n    # Deploy policy, collect rollouts\n    rollouts = []\n    for episode in range(100):\n        state = env.reset()\n        for t in range(horizon):\n            action = policy(state)  # Policy action\n            expert_action = expert(state)  # Query expert\n            rollouts.append((state, expert_action))  # Use expert action\n            state, _, done, _ = env.step(action)\n            if done:\n                break\n\n    # Augment dataset\n    expert_demos.extend(rollouts)\n\n    # Retrain\n    policy.train(expert_demos)\n```\n\n**Advantage**: Policy learns to recover from its own mistakes\n\n### Generative Adversarial Imitation Learning (GAIL)\n\n**Idea**: Learn reward function from demos, then RL\n\n```python\n# Discriminator: real (expert) vs fake (policy)\ndiscriminator = nn.Sequential(\n    nn.Linear(state_dim + action_dim, 256),\n    nn.ReLU(),\n    nn.Linear(256, 1),\n    nn.Sigmoid()\n)\n\n# Train discriminator\nfor epoch in range(epochs):\n    # Expert data\n    expert_s, expert_a = sample_expert_demos()\n    expert_score = discriminator(torch.cat([expert_s, expert_a], dim=-1))\n\n    # Policy data\n    policy_s, policy_a = sample_policy_rollouts()\n    policy_score = discriminator(torch.cat([policy_s, policy_a], dim=-1))\n\n    # Binary cross-entropy\n    loss = -torch.mean(torch.log(expert_score) + torch.log(1 - policy_score))\n    # Update discriminator\n\n# Use discriminator as reward for RL\nreward = -torch.log(discriminator(s, a))  # High reward for expert-like behavior\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 17,
        "chapter_title": "Chapter 4: Control Loops",
        "section": "4.2 Imitation Learning Pipelines",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 334,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-control-loops.md"
      }
    },
    {
      "content": "### Classical + Learning\n\n**Idea**: Classical control for stability, learning for adaptation\n\n**Example** (Quadruped Locomotion):\n```python\nclass HybridController:\n    def __init__(self):\n        self.classical = PIDController()  # Joint-level PD control\n        self.learned = NeuralPolicy()  # Learned foothold planner\n\n    def control(self, state):\n        # Learned: High-level planning\n        target_footholds = self.learned.plan_footholds(state)\n\n        # Classical: Low-level tracking\n        joint_targets = self.classical.inverse_kinematics(target_footholds)\n\n        return joint_targets\n```\n\n**Advantages**:\n- Stability guarantees from classical\n- Adaptability from learning\n- Interpretable (can inspect classical component)\n\n### Residual RL\n\n**Idea**: Learning corrects classical controller\n\n```python\nclass ResidualController:\n    def __init__(self):\n        self.classical = PDController(kp=100, kd=10)\n        self.residual = NeuralNetwork()\n\n    def control(self, state, target):\n        # Classical baseline\n        classical_action = self.classical.control(state, target)\n\n        # Learned correction\n        residual_action = self.residual(state, target)\n\n        # Combined (bound residual)\n        return classical_action + 0.1 * residual_action\n```\n\n**Training**: RL trains `residual`, `classical` frozen\n\n**Use Case**: Fine-tune PID gains per environment (rough terrain, slopes)\n\n### Hierarchical Control\n\n**High-level** (slow): Task planning (learned)\n**Mid-level** (medium): Motion primitives (classical or learned)\n**Low-level** (fast): Joint control (PD)\n\n```python\nclass HierarchicalController:\n    def __init__(self):\n        self.high_level = TaskPlanner()  # Outputs: \"walk\", \"grasp\", \"stand\"\n        self.mid_level = MotionPrimitives()  # Outputs: joint trajectories\n        self.low_level = PDController()  # Tracks trajectories\n\n    def control(self, state, goal):\n        # High-level: Decide what to do (10 Hz)\n        task = self.high_level.plan(state, goal)\n\n        # Mid-level: Generate trajectory (100 Hz)\n        trajectory = self.mid_level.execute(task, state)\n\n        # Low-level: Track trajectory (1000 Hz)\n        torques = self.low_level.track(trajectory, state)\n\n        return torques\n```\n\n**Example**: Humanoid navigation\n- High-level: A* path planning\n- Mid-level: Footstep planner (learned)\n- Low-level: Joint PD control",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 17,
        "chapter_title": "Chapter 4: Control Loops",
        "section": "4.3 Hybrid Control Architectures",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 238,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-control-loops.md"
      }
    },
    {
      "content": "**Exercise 4.1**: RL Training in Isaac Gym\n- Implement a cartpole balancing task in Isaac Gym (4096 parallel environments)\n- Train a PPO policy for 1000 episodes\n- Experiment with reward shaping (balance angle, velocity penalties)\n- Plot learning curves and analyze convergence\n\n**Exercise 4.2**: Behavior Cloning for Grasping\n- Collect 500 expert demonstrations of pick-and-place in Isaac Sim (teleoperation or scripted)\n- Train behavior cloning policy (vision → gripper pose)\n- Evaluate on test objects not seen during training\n- Measure success rate and failure modes\n\n**Exercise 4.3**: DAgger Interactive Learning\n- Start with initial BC policy from Exercise 4.2\n- Implement DAgger: deploy policy, collect expert corrections, retrain\n- Run 5 iterations of DAgger\n- Plot success rate vs iteration and compare with pure BC\n\n**Exercise 4.4**: Hybrid Residual Controller\n- Implement a PD controller for quadruped standing balance\n- Add learned residual correction for terrain adaptation\n- Train residual policy on randomized terrain in Isaac Sim\n- Compare performance: PD-only vs PD+residual on slopes/stairs",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 17,
        "chapter_title": "Chapter 4: Control Loops",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 167,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-control-loops.md"
      }
    },
    {
      "content": "**RL in Isaac Gym**: Parallel training (1000s of robots), PPO for stability, reward engineering\n**Imitation Learning**: Behavior cloning (fast, brittle), DAgger (iterative), GAIL (adversarial)\n**Hybrid Control**: Classical + learning, residual RL, hierarchical architectures\n\n**Module 3 Complete!** Next modules cover VLA models (Module 4) and capstone project (Module 5).",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 17,
        "chapter_title": "Chapter 4: Control Loops",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 48,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-control-loops.md"
      }
    },
    {
      "content": "---\nsidebar_position: 1\ntitle: \"Chapter 1: VLA Fundamentals\"\ndescription: Vision-Language-Action models, transformer architectures, and training paradigms\ntags: [vla, transformers, foundation-models, multimodal, robotics]\n---\n\n# Chapter 1: VLA Fundamentals",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 18,
        "chapter_title": "Chapter 1: VLA Fundamentals",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 28,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-fundamentals.md"
      }
    },
    {
      "content": "1. Understand Vision-Language-Action (VLA) models and their role in robotics\n2. Grasp transformer architectures adapted for multimodal robot learning\n3. Compare training paradigms: imitation learning, RL, and hybrid approaches",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 18,
        "chapter_title": "Chapter 1: VLA Fundamentals",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 29,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-fundamentals.md"
      }
    },
    {
      "content": "### What are VLA Models?\n\n**Vision-Language-Action (VLA)** models are foundation models that map visual observations and natural language instructions to robot actions.\n\n**Core Capability**: \"Pick up the red mug\" + camera image → gripper motion\n\n**Key Innovation**: Generalization across tasks, objects, and environments without task-specific retraining.\n\n### Historical Context\n\n**2017-2020**: Task-specific policies\n- Separate models for grasping, navigation, manipulation\n- Trained from scratch per task (100k+ demonstrations)\n\n**2021-2022**: Language-conditioned policies\n- CLIPort (2021): Language + vision for pick-and-place\n- BC-Z (2022): Multi-task behavior cloning with language goals\n\n**2023-Present**: Foundation models for robotics\n- RT-1 (2023): 130k demonstrations, 700 tasks\n- RT-2 (2023): Vision-language model → robot actions\n- OpenVLA (2024): Open-source 7B parameter VLA\n- π₀ (Pi-Zero, 2024): Generalist robot policy\n\n### Key Research Papers\n\n1. **RT-1** (Brohan et al., 2023): \"RT-1: Robotics Transformer for Real-World Control at Scale\"\n   - Transformer for robot control (not just vision/language)\n   - 130k robot trajectories across 700 tasks\n   - 97% success on seen tasks, 62% on novel tasks\n\n2. **RT-2** (Brohan et al., 2023): \"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control\"\n   - Pretrained vision-language model (PaLI-X) → robot actions\n   - Leverages internet-scale vision-language data\n   - Emergent capabilities: reasoning about object properties, physics\n\n3. **OpenVLA** (Kim et al., 2024): \"OpenVLA: An Open-Source Vision-Language-Action Model\"\n   - 7B parameter model trained on Open X-Embodiment dataset\n   - 970k robot trajectories, 22 robot platforms\n   - Apache 2.0 license (commercial-friendly)\n\n4. **Octo** (Ghosh et al., 2024): \"Octo: An Open-Source Generalist Robot Policy\"\n   - Diffusion-based action prediction\n   - 800k trajectories from 9 robot types\n   - Fine-tunes with `<1000` demonstrations\n\n### VLA Model Taxonomy\n\n```mermaid\ngraph TB\n    VLA[VLA Models]\n\n    VLA --> Closed[Closed-Source]\n    VLA --> Open[Open-Source]\n\n    Closed --> RT1[RT-1<br/>Google]\n    Closed --> RT2[RT-2<br/>Google]\n    Closed --> PiZero[π₀<br/>Physical Intelligence]\n\n    Open --> OpenVLA[OpenVLA<br/>7B params]\n    Open --> Octo[Octo<br/>Diffusion]\n    Open --> SmolVLA[SmolVLA<br/>2B params]\n\n    style VLA fill:#e1f5ff\n    style Closed fill:#ffe1e1\n    style Open fill:#e1ffe1\n```\n\n**Figure 1.1**: VLA model landscape showing closed-source (industry) and open-source (academic/community) approaches.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 18,
        "chapter_title": "Chapter 1: VLA Fundamentals",
        "section": "1.1 Vision-Language-Action Models",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 318,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-fundamentals.md"
      }
    },
    {
      "content": "### From Language to Actions\n\n**Standard Transformer** (BERT, GPT):\n- Input: Text tokens\n- Output: Text tokens (next word prediction)\n\n**VLA Transformer**:\n- Input: Image tokens + language tokens + proprioception (joint angles, gripper state)\n- Output: Action tokens (joint velocities, gripper commands)\n\n### Architecture Components\n\n**1. Vision Encoder**:\n```python\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Vision Transformer (ViT) for image encoding\nprocessor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\nvision_encoder = AutoModel.from_pretrained(\"google/vit-base-patch16-224\")\n\n# Process camera image\nimage_tokens = vision_encoder(pixel_values=processor(images, return_tensors=\"pt\").pixel_values)\n# Output: (batch, 196, 768) - 196 patches, 768-dim embeddings\n```\n\n**2. Language Encoder**:\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\n# T5 for language encoding\ntokenizer = AutoTokenizer.from_pretrained(\"google/t5-v1_1-base\")\nlanguage_encoder = AutoModel.from_pretrained(\"google/t5-v1_1-base\")\n\n# Encode instruction\ninstruction = \"Pick up the red cup\"\nlanguage_tokens = language_encoder(**tokenizer(instruction, return_tensors=\"pt\"))\n# Output: (batch, seq_len, 768)\n```\n\n**3. Multimodal Fusion**:\n```python\nimport torch\nimport torch.nn as nn\n\nclass MultimodalFusion(nn.Module):\n    def __init__(self, dim=768):\n        super().__init__()\n        self.cross_attention = nn.MultiheadAttention(dim, num_heads=12)\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, image_tokens, language_tokens):\n        # Cross-attention: language queries, image keys/values\n        fused, _ = self.cross_attention(\n            query=language_tokens,\n            key=image_tokens,\n            value=image_tokens\n        )\n        return self.norm(fused + language_tokens)  # Residual connection\n```\n\n**4. Action Head**:\n```python\nclass ActionHead(nn.Module):\n    def __init__(self, input_dim=768, action_dim=7):\n        super().__init__()\n        self.head = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim)\n        )\n\n    def forward(self, fused_tokens):\n        # Pool tokens (mean or CLS token)\n        pooled = fused_tokens.mean(dim=1)  # (batch, 768)\n        actions = self.head(pooled)  # (batch, 7) - e.g., 6-DOF arm + gripper\n        return actions\n```\n\n### Full VLA Forward Pass\n\n```python\nclass SimpleVLA(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.vision_encoder = AutoModel.from_pretrained(\"google/vit-base-patch16-224\")\n        self.language_encoder = AutoModel.from_pretrained(\"google/t5-v1_1-base\")\n        self.fusion = MultimodalFusion(dim=768)\n        self.action_head = ActionHead(input_dim=768, action_dim=7)\n\n    def forward(self, images, instructions, proprioception=None):\n        # Encode vision\n        image_tokens = self.vision_encoder(pixel_values=images).last_hidden_state\n\n        # Encode language\n        language_tokens = self.language_encoder(**instructions).last_hidden_state\n\n        # Fuse modalities\n        fused = self.fusion(image_tokens, language_tokens)\n\n        # Predict actions\n        actions = self.action_head(fused)\n\n        return actions  # (batch, action_dim)\n```\n\n### Attention Mechanisms\n\n**Self-Attention** (within modality):\n- Vision: Spatial relationships between image patches\n- Language: Dependencies between words\n\n**Cross-Attention** (across modalities):\n- Language queries image: \"Where is the red cup?\"\n- Image queries language: \"What object am I looking at?\"\n\n**Causal Attention** (for autoregressive action prediction):\n- Action at time `t` depends only on past actions (t-1, t-2, ...)\n- Prevents \"looking into the future\" during training",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 18,
        "chapter_title": "Chapter 1: VLA Fundamentals",
        "section": "1.2 Transformer Architectures for Robotics",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 354,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-fundamentals.md"
      }
    },
    {
      "content": "### Imitation Learning (Behavior Cloning)\n\n**Objective**: Learn policy π(a|s, g) from expert demonstrations\n\n**Data**: (observation, language goal, action) tuples\n- Example: (image of table, \"pick up cup\", gripper_close)\n\n**Loss Function** (supervised learning):\n```python\nimport torch.nn.functional as F\n\ndef imitation_loss(predicted_actions, expert_actions):\n    # Mean squared error for continuous actions\n    return F.mse_loss(predicted_actions, expert_actions)\n```\n\n**Advantages**:\n- Simple to implement\n- Stable training (no exploration needed)\n- Leverages human expertise\n\n**Disadvantages**:\n- Requires large demonstration datasets (10k-1M)\n- Distributional shift: Policy sees states expert never visited\n- No error correction (drift accumulates)\n\n### Reinforcement Learning\n\n**Objective**: Maximize cumulative reward ∑ γᵗ r(sₜ, aₜ)\n\n**Environment**: Robot simulator or real hardware\n**Reward**: Task success (1.0) or failure (0.0), shaped rewards\n\n**Example** (PPO training loop):\n```python\nfrom stable_baselines3 import PPO\n\n# VLA policy wrapped for RL\npolicy = VLAPolicyWrapper(vla_model)\n\n# PPO trainer\nagent = PPO(\"MultiInputPolicy\", env, policy=policy, verbose=1)\n\n# Train\nagent.learn(total_timesteps=1_000_000)\n```\n\n**Advantages**:\n- Learns from trial and error (no demonstrations needed)\n- Optimizes directly for task success\n- Handles novel states via exploration\n\n**Disadvantages**:\n- Sample inefficient (millions of steps)\n- Sim-to-real gap (simulator ≠ reality)\n- Reward engineering challenging\n\n### Hybrid: Pretrain + Fine-tune\n\n**Best of both worlds**:\n1. **Pretrain** with imitation learning on large dataset (100k-1M demos)\n2. **Fine-tune** with RL on specific task (1k-10k steps)\n\n**Workflow**:\n```python\n# Step 1: Pretrain on demonstrations\nvla_model.train_behavior_cloning(demo_dataset, epochs=100)\n\n# Step 2: Fine-tune with RL\nvla_policy = VLAPolicyWrapper(vla_model)\nppo = PPO(\"MultiInputPolicy\", env, policy=vla_policy)\nppo.learn(total_timesteps=10_000)  # Much faster than RL from scratch\n```\n\n**Example**: RT-2 pretrained on web data (vision-language), fine-tuned on robot tasks\n\n### Training Data Sources\n\n**Open X-Embodiment Dataset** (2023):\n- 1M+ robot trajectories\n- 22 robot types (arms, mobile manipulators, quadrupeds)\n- 527 skills across diverse tasks\n- CC BY 4.0 license\n\n**Key Datasets**:\n- **Bridge V2**: 60k trajectories, kitchen tasks\n- **FrankaPlay**: 50k trajectories, Franka Emika arm\n- **DROID**: 76k trajectories, 564 skills\n- **Language-Table**: Simulated tabletop manipulation",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 18,
        "chapter_title": "Chapter 1: VLA Fundamentals",
        "section": "1.3 Training Paradigms",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 314,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-fundamentals.md"
      }
    },
    {
      "content": "**Exercise 1.1**: VLA Model Paper Review\n- Read RT-2 paper (Brohan et al., 2023)\n- Summarize: (a) Architecture, (b) Training data, (c) Emergent capabilities\n- Compare with RT-1: What changed? Why does it generalize better?\n\n**Exercise 1.2**: Transformer Forward Pass\n- Implement a minimal VLA model using the code above\n- Input: 224x224 image + \"pick up cup\" instruction\n- Output: 7-dim action (6-DOF pose + gripper)\n- Count total parameters and estimate FLOPs\n\n**Exercise 1.3**: Imitation Learning Baseline\n- Load the Bridge V2 dataset (or subset)\n- Train behavior cloning model (ViT + T5 + action head)\n- Evaluate on held-out tasks\n- Plot training loss and validation success rate\n\n**Exercise 1.4**: Hybrid Training Experiment\n- Train baseline: (a) BC only, (b) RL only, (c) BC → RL fine-tuning\n- Compare sample efficiency (timesteps to 80% success)\n- Analyze which approach works best for your task",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 18,
        "chapter_title": "Chapter 1: VLA Fundamentals",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 147,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-fundamentals.md"
      }
    },
    {
      "content": "**VLA Models**: Foundation models mapping vision + language → actions, enabling generalization across tasks\n**Transformers for Robotics**: ViT (vision) + T5 (language) + cross-attention (fusion) + action head\n**Training Paradigms**: Imitation learning (simple, stable), RL (optimal, sample-intensive), hybrid (best results)\n\n**Next**: Chapter 2 covers RT-2, OpenVLA, and SmolVLA architectures and deployment.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 18,
        "chapter_title": "Chapter 1: VLA Fundamentals",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 51,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-fundamentals.md"
      }
    },
    {
      "content": "---\nsidebar_position: 2\ntitle: \"Chapter 2: RT-2 and Open-Source VLA Models\"\ndescription: RT-2 architecture, OpenVLA, and SmolVLA for edge deployment\ntags: [rt-2, openvla, smolvla, vision-language-action, robotics]\n---\n\n# Chapter 2: RT-2 and Open-Source VLA Models",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 19,
        "chapter_title": "Chapter 2: RT-2 and Open-Source VLA Models",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 35,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-rt2-models.md"
      }
    },
    {
      "content": "1. Understand RT-2 architecture and how it leverages web-scale pretraining\n2. Deploy OpenVLA for open-source robot control\n3. Use SmolVLA for resource-constrained edge devices",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 19,
        "chapter_title": "Chapter 2: RT-2 and Open-Source VLA Models",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 24,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-rt2-models.md"
      }
    },
    {
      "content": "### Core Innovation\n\n**RT-2** (Robotics Transformer 2): Vision-language model adapted for robot control\n\n**Key Insight**: Pretrained vision-language models (VLMs) contain rich world knowledge that transfers to robotics.\n\n**Architecture Pipeline**:\n\n```mermaid\ngraph LR\n    Image[Robot Camera<br/>224x224 RGB] --> VLM[PaLI-X VLM<br/>55B params]\n    Text[Language Instruction<br/>pick red apple] --> VLM\n\n    VLM --> Tokens[Action Tokens<br/>discretized]\n    Tokens --> Detokenize[Detokenize]\n    Detokenize --> Actions[Continuous Actions<br/>7-DOF + gripper]\n\n    Actions --> Robot[Robot Execution]\n\n    style VLM fill:#e1f5ff\n    style Actions fill:#ffe1e1\n```\n\n**Figure 2.1**: RT-2 pipeline showing how a pretrained vision-language model (PaLI-X) is fine-tuned to output discretized action tokens instead of text.\n\n### PaLI-X Base Model\n\n**PaLI-X** (Pathways Language and Image model):\n- **Vision Encoder**: ViT-22B (22 billion parameters)\n- **Language Decoder**: UL2 (32B parameters)\n- **Total**: 55B parameters\n\n**Pretraining**:\n- WebLI dataset: 10B image-text pairs from the web\n- Tasks: Image captioning, VQA, object detection (in text form)\n\n**Example Capabilities**:\n- Input: Image of apple + \"What color is this fruit?\"\n- Output: \"Red\"\n\n### Action Tokenization\n\n**Problem**: PaLI-X outputs text, but robots need continuous actions (joint angles, velocities)\n\n**Solution**: Discretize action space into 256 bins per dimension\n\n**Example** (7-DOF arm):\n```python\nimport numpy as np\n\ndef tokenize_action(continuous_action, num_bins=256):\n    \"\"\"\n    Convert continuous action to discrete tokens\n\n    Args:\n        continuous_action: (7,) array in [-1, 1]\n        num_bins: Number of discrete bins (vocabulary size)\n\n    Returns:\n        tokens: (7,) array of integers in [0, num_bins-1]\n    \"\"\"\n    # Clip to valid range\n    clipped = np.clip(continuous_action, -1, 1)\n\n    # Map [-1, 1] → [0, num_bins-1]\n    tokens = ((clipped + 1) / 2 * (num_bins - 1)).astype(int)\n\n    return tokens\n\n# Example\naction = np.array([0.5, -0.3, 0.8, 0.0, -0.5, 0.2, 1.0])  # Continuous\ntokens = tokenize_action(action)\nprint(tokens)  # [191, 89, 230, 128, 64, 153, 255]\n```\n\n**Detokenization** (inference):\n```python\ndef detokenize_action(tokens, num_bins=256):\n    \"\"\"\n    Convert discrete tokens back to continuous actions\n\n    Args:\n        tokens: (7,) array of integers in [0, num_bins-1]\n\n    Returns:\n        continuous_action: (7,) array in [-1, 1]\n    \"\"\"\n    # Map [0, num_bins-1] → [-1, 1]\n    continuous = (tokens / (num_bins - 1)) * 2 - 1\n\n    return continuous.astype(np.float32)\n\n# Example\ncontinuous = detokenize_action(tokens)\nprint(continuous)  # Approximates original action\n```\n\n### RT-2 Training Process\n\n**Phase 1**: Pretrain PaLI-X on web data (done by Google)\n- 10B image-text pairs\n- Generalist vision-language understanding\n\n**Phase 2**: Co-fine-tune on robotics + web data\n```python\n# Pseudo-code for RT-2 training\nfor batch in dataloader:\n    # Mix robot data (80%) and web data (20%)\n    if random.random() < 0.8:\n        # Robot trajectory\n        images, instructions, actions = batch\n        # Tokenize actions\n        action_tokens = tokenize_action(actions)\n        # Forward pass: predict action tokens\n        predicted_tokens = model(images, instructions)\n        loss = cross_entropy_loss(predicted_tokens, action_tokens)\n    else:\n        # Web VQA data (maintain VLM capabilities)\n        images, questions, answers = web_batch\n        predicted_text = model(images, questions)\n        loss = cross_entropy_loss(predicted_text, answers)\n\n    loss.backward()\n    optimizer.step()\n```\n\n**Key Benefit**: Model retains visual reasoning from pretraining while learning robot control\n\n### RT-2 Performance\n\n**Benchmark** (Google Robot Lab, 2023):\n- **RT-1** (trained from scratch): 62% success on novel tasks\n- **RT-2-PaLI-X**: 90% success on novel tasks\n- **Emergent reasoning**: \"Pick up the extinct animal\" → picks dinosaur toy\n\n**Generalization Examples**:\n- Novel objects: Identifies \"heaviest object\" without explicit training\n- Spatial reasoning: \"Move apple to the left of banana\"\n- Affordances: \"Pick up object for hammering\" → selects hammer",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 19,
        "chapter_title": "Chapter 2: RT-2 and Open-Source VLA Models",
        "section": "2.1 RT-2 Architecture",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 522,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-rt2-models.md"
      }
    },
    {
      "content": "### Open-Source VLA\n\n**OpenVLA** (2024): First open-source, production-ready VLA model\n\n**Key Stats**:\n- **Parameters**: 7B (similar to Llama 2-7B)\n- **Training Data**: Open X-Embodiment (970k trajectories, 22 robots)\n- **License**: Apache 2.0 (commercial use allowed)\n- **Performance**: Matches RT-2 on Open X-Embodiment tasks\n\n### Architecture\n\n**Base Model**: Prismatic VLM (7B)\n- **Vision**: DINOv2 (ViT-L/14) - 300M params\n- **Language**: Llama 2-7B - 7B params\n- **Fusion**: Gated cross-attention\n\n**Action Head**: MLP (2-layer, 256 hidden)\n\n### Installation and Setup\n\n```bash\n# Install OpenVLA\npip install openvla\n\n# Download pretrained model (7B params, ~14GB)\npython -m openvla.download --model openvla-7b\n```\n\n### Inference Example\n\n```python\nfrom openvla import OpenVLA\nimport torch\nfrom PIL import Image\n\n# Load model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = OpenVLA.from_pretrained(\"openvla-7b\").to(device)\n\n# Prepare inputs\nimage = Image.open(\"robot_camera.jpg\")  # 224x224 RGB\ninstruction = \"pick up the red cup\"\n\n# Predict action\nwith torch.no_grad():\n    action = model.predict_action(\n        image=image,\n        instruction=instruction,\n        unnormalize=True  # Convert to real robot action space\n    )\n\nprint(action)  # (7,) - e.g., [x, y, z, roll, pitch, yaw, gripper]\n```\n\n### Fine-Tuning on Custom Data\n\n```python\nfrom openvla import OpenVLA\nfrom openvla.data import RobotDataset\nfrom torch.utils.data import DataLoader\n\n# Load pretrained model\nmodel = OpenVLA.from_pretrained(\"openvla-7b\")\n\n# Prepare your dataset (RLDS format)\ndataset = RobotDataset(data_path=\"my_robot_data/\", task=\"pick_and_place\")\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n# Fine-tune\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\nfor epoch in range(10):\n    for batch in dataloader:\n        images, instructions, actions = batch\n\n        # Forward pass\n        predicted_actions = model(images, instructions)\n        loss = model.compute_loss(predicted_actions, actions)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n# Save fine-tuned model\nmodel.save_pretrained(\"openvla-finetuned-pickplace\")\n```\n\n### Open X-Embodiment Dataset\n\n**Coverage**:\n- **22 robot types**: Franka, UR5, Kinova, mobile manipulators\n- **527 skills**: Grasping, placing, pushing, opening, closing\n- **Diverse environments**: Labs, kitchens, warehouses\n\n**Loading Example**:\n```python\nfrom openvla.data import load_oxe_dataset\n\n# Load Bridge V2 dataset (kitchen tasks)\ndataset = load_oxe_dataset(\"bridge_v2\", split=\"train\")\n\n# Inspect sample\nsample = dataset[0]\nprint(sample.keys())  # ['image', 'instruction', 'action', 'robot_type']\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 19,
        "chapter_title": "Chapter 2: RT-2 and Open-Source VLA Models",
        "section": "2.2 OpenVLA",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 318,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-rt2-models.md"
      }
    },
    {
      "content": "### Small-Scale VLA for Edge Deployment\n\n**SmolVLA**: Lightweight VLA optimized for edge devices (Jetson, Raspberry Pi)\n\n**Key Stats**:\n- **Parameters**: 2B (vs 7B for OpenVLA)\n- **Latency**: 15ms inference (Jetson Orin) vs 50ms for OpenVLA\n- **Accuracy**: 85% of OpenVLA performance with 30% params\n\n### Architecture Optimizations\n\n1. **Smaller Vision Encoder**: DINOv2-S (22M params) vs DINOv2-L (300M)\n2. **Distilled Language Model**: Llama 2-2B (distilled from 7B)\n3. **Quantization**: INT8 weights (4x memory reduction)\n4. **Pruning**: 30% of least important weights removed\n\n### Installation\n\n```bash\n# Install SmolVLA\npip install smolvla\n\n# Download quantized model (INT8, ~2GB)\npython -m smolvla.download --model smolvla-2b-int8\n```\n\n### Deployment on Jetson Orin\n\n```python\nfrom smolvla import SmolVLA\nimport torch\n\n# Load quantized model\nmodel = SmolVLA.from_pretrained(\n    \"smolvla-2b-int8\",\n    device=\"cuda\",\n    quantization=\"int8\"  # Use TensorRT INT8 kernels\n)\n\n# Inference (optimized for low latency)\n@torch.inference_mode()\ndef predict_action_fast(image, instruction):\n    return model.predict_action(\n        image=image,\n        instruction=instruction,\n        use_kv_cache=True  # Cache attention keys/values\n    )\n\n# Benchmark\nimport time\ntimes = []\nfor _ in range(100):\n    start = time.time()\n    action = predict_action_fast(test_image, \"pick cup\")\n    times.append(time.time() - start)\n\nprint(f\"Latency: {np.mean(times)*1000:.1f}ms ± {np.std(times)*1000:.1f}ms\")\n# Expected: ~15ms on Jetson Orin\n```\n\n### Model Comparison\n\n| Model | Params | Memory | Latency (Jetson Orin) | Success Rate |\n|-------|--------|--------|----------------------|--------------|\n| **RT-2** | 55B | 110GB | N/A (cloud only) | 90% |\n| **OpenVLA** | 7B | 14GB | 50ms | 88% |\n| **SmolVLA** | 2B | 2GB (INT8) | 15ms | 75% |\n| **SmolVLA-distilled** | 2B | 2GB | 15ms | 80% |\n\n### When to Use Each Model\n\n**RT-2** (cloud deployment):\n- Highest accuracy needed\n- Cloud compute available\n- Complex reasoning tasks\n\n**OpenVLA** (workstation/server):\n- Open-source requirement\n- GPU server available (A100, RTX 4090)\n- Fine-tuning on custom data\n\n**SmolVLA** (edge deployment):\n- Real-time control (`<20ms` latency)\n- Jetson Orin, Raspberry Pi 5\n- Battery-powered robots",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 19,
        "chapter_title": "Chapter 2: RT-2 and Open-Source VLA Models",
        "section": "2.3 SmolVLA",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 303,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-rt2-models.md"
      }
    },
    {
      "content": "**Exercise 2.1**: RT-2 Action Tokenization\n- Implement action tokenization with 128 bins (instead of 256)\n- Measure quantization error vs number of bins (64, 128, 256, 512)\n- Plot error vs bins and recommend optimal bin count\n\n**Exercise 2.2**: OpenVLA Inference\n- Install OpenVLA and run inference on sample images\n- Test generalization: \"pick the largest object\", \"move left\"\n- Compare predicted actions with ground truth on Bridge V2\n\n**Exercise 2.3**: Fine-Tuning OpenVLA\n- Collect 100 demonstrations of a custom task (or use simulation)\n- Fine-tune OpenVLA for 10 epochs\n- Evaluate on held-out test set (success rate)\n\n**Exercise 2.4**: SmolVLA Edge Deployment\n- Deploy SmolVLA on Jetson Orin or Raspberry Pi 5\n- Benchmark latency with different batch sizes (1, 4, 8)\n- Profile memory usage and identify bottlenecks",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 19,
        "chapter_title": "Chapter 2: RT-2 and Open-Source VLA Models",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 130,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-rt2-models.md"
      }
    },
    {
      "content": "**RT-2**: 55B parameter VLM adapted for robotics, achieves 90% success via web-scale pretraining\n**OpenVLA**: Open-source 7B VLA matching RT-2 performance, Apache 2.0 license\n**SmolVLA**: 2B lightweight VLA for edge deployment, 15ms latency on Jetson\n\n**Next**: Chapter 3 covers integrating VLA policies with ROS 2 and safety wrappers.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 19,
        "chapter_title": "Chapter 2: RT-2 and Open-Source VLA Models",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 47,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-rt2-models.md"
      }
    },
    {
      "content": "---\nsidebar_position: 3\ntitle: \"Chapter 3: Policy Integration\"\ndescription: Integrating VLA policies with ROS 2, safety wrappers, and real-world deployment\ntags: [policy-integration, ros2, safety, deployment, robotics]\n---\n\n# Chapter 3: Policy Integration",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 20,
        "chapter_title": "Chapter 3: Policy Integration",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 32,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-policy-integration.md"
      }
    },
    {
      "content": "1. Integrate VLA policies with ROS 2 robot systems\n2. Implement safety wrappers for collision avoidance and workspace limits\n3. Address real-world deployment challenges (latency, failure modes, monitoring)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 20,
        "chapter_title": "Chapter 3: Policy Integration",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 28,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-policy-integration.md"
      }
    },
    {
      "content": "### Action Space Design\n\n**VLA Output**: Normalized actions in [-1, 1] per dimension\n\n**Robot Input**: Task-specific action spaces\n\n**Common Action Spaces**:\n\n1. **End-Effector Pose (6-DOF + Gripper)**:\n   ```python\n   action = [x, y, z, roll, pitch, yaw, gripper_state]  # (7,)\n   # x, y, z: Position in meters (workspace frame)\n   # roll, pitch, yaw: Orientation in radians\n   # gripper_state: 0 = open, 1 = closed\n   ```\n\n2. **Joint Velocities** (7-DOF arm):\n   ```python\n   action = [j1_vel, j2_vel, ..., j7_vel, gripper_vel]  # (8,)\n   # Joint velocities in rad/s\n   ```\n\n3. **Delta End-Effector** (relative motion):\n   ```python\n   action = [dx, dy, dz, droll, dpitch, dyaw, gripper]  # (7,)\n   # Small incremental changes (e.g., ±5cm, ±10°)\n   ```\n\n### Action Denormalization\n\n**Mapping [-1, 1] → Real Units**:\n\n```python\nimport numpy as np\n\nclass ActionDenormalizer:\n    def __init__(self, action_space):\n        \"\"\"\n        Args:\n            action_space: Dict with 'low' and 'high' bounds\n\n        Example:\n            action_space = {\n                'low': np.array([-0.5, -0.5, 0.0, -3.14, -1.57, -3.14, 0.0]),\n                'high': np.array([0.5, 0.5, 0.5, 3.14, 1.57, 3.14, 1.0])\n            }\n        \"\"\"\n        self.low = np.array(action_space['low'])\n        self.high = np.array(action_space['high'])\n\n    def denormalize(self, normalized_action):\n        \"\"\"\n        Convert [-1, 1] → [low, high]\n\n        Args:\n            normalized_action: (n,) array in [-1, 1]\n\n        Returns:\n            real_action: (n,) array in [low, high]\n        \"\"\"\n        # Clip to [-1, 1]\n        clipped = np.clip(normalized_action, -1, 1)\n\n        # Linear mapping\n        real_action = self.low + (clipped + 1) / 2 * (self.high - self.low)\n\n        return real_action\n\n# Example usage\naction_space = {\n    'low': np.array([-0.5, -0.5, 0.0, -3.14, -1.57, -3.14, 0.0]),\n    'high': np.array([0.5, 0.5, 0.5, 3.14, 1.57, 3.14, 1.0])\n}\ndenormalizer = ActionDenormalizer(action_space)\n\n# VLA output\nvla_action = np.array([0.2, -0.5, 0.8, 0.0, 0.3, -0.1, 1.0])\n\n# Denormalize\nrobot_action = denormalizer.denormalize(vla_action)\nprint(robot_action)\n# [0.1, -0.375, 0.4, 0.0, -0.314, 2.984, 1.0]\n```\n\n### ROS 2 Policy Node\n\n**Architecture**:\n\n```mermaid\ngraph LR\n    Camera[\"/camera/image_raw<br/>sensor_msgs/Image\"] --> PolicyNode[VLA Policy Node]\n    Instruction[\"/task/instruction<br/>std_msgs/String\"] --> PolicyNode\n\n    PolicyNode --> Action[\"/action/command<br/>control_msgs/JointTrajectoryPoint\"]\n\n    Action --> Controller[Robot Controller<br/>ros2_control]\n    Controller --> Robot[Robot Hardware]\n\n    style PolicyNode fill:#e1f5ff\n    style Controller fill:#ffe1e1\n```\n\n**Figure 3.1**: ROS 2 integration showing VLA policy node consuming camera and instruction topics, publishing action commands to robot controller.\n\n**Implementation**:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom control_msgs.msg import JointTrajectoryPoint\nfrom cv_bridge import CvBridge\nimport torch\nfrom openvla import OpenVLA\n\nclass VLAPolicyNode(Node):\n    def __init__(self):\n        super().__init__('vla_policy_node')\n\n        # Load VLA model\n        self.model = OpenVLA.from_pretrained(\"openvla-7b\").to(\"cuda\")\n        self.model.eval()\n\n        # ROS utilities\n        self.bridge = CvBridge()\n        self.current_image = None\n        self.current_instruction = \"pick up the cup\"  # Default\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n        self.instruction_sub = self.create_subscription(\n            String,\n            '/task/instruction',\n            self.instruction_callback,\n            10\n        )\n\n        # Publisher\n        self.action_pub = self.create_publisher(\n            JointTrajectoryPoint,\n            '/action/command',\n            10\n        )\n\n        # Policy loop (10 Hz)\n        self.timer = self.create_timer(0.1, self.policy_loop)\n\n        self.get_logger().info(\"VLA Policy Node started\")\n\n    def image_callback(self, msg):\n        # Convert ROS Image to OpenCV format\n        self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')\n\n    def instruction_callback(self, msg):\n        self.current_instruction = msg.data\n        self.get_logger().info(f\"New instruction: {self.current_instruction}\")\n\n    def policy_loop(self):\n        if self.current_image is None:\n            return\n\n        # Predict action\n        with torch.no_grad():\n            action = self.model.predict_action(\n                image=self.current_image,\n                instruction=self.current_instruction,\n                unnormalize=True\n            )\n\n        # Publish action\n        msg = JointTrajectoryPoint()\n        msg.positions = action[:7].tolist()  # Joint positions or EE pose\n        msg.time_from_start.sec = 0\n        msg.time_from_start.nanosec = 100_000_000  # 100ms\n\n        self.action_pub.publish(msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VLAPolicyNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n### Control Frequencies\n\n**Policy Frequency**: 10 Hz (typical for VLA models)\n- Inference time: 20-50ms (OpenVLA on GPU)\n- Remaining budget: 50-80ms for processing\n\n**Robot Control Frequency**: 100-1000 Hz (joint-level control)\n- High-level commands (10 Hz) → Low-level tracking (1000 Hz)\n\n**Bridging Frequencies**:\n```python\nclass FrequencyBridge:\n    def __init__(self, policy_hz=10, control_hz=1000):\n        self.policy_hz = policy_hz\n        self.control_hz = control_hz\n        self.steps_per_action = control_hz // policy_hz  # 100 steps\n\n    def interpolate_action(self, prev_action, next_action, step):\n        \"\"\"\n        Linear interpolation between actions\n\n        Args:\n            prev_action: Previous VLA output\n            next_action: Current VLA output\n            step: Current step in [0, steps_per_action-1]\n\n        Returns:\n            interpolated_action: Smooth action for this timestep\n        \"\"\"\n        alpha = step / self.steps_per_action\n        return (1 - alpha) * prev_action + alpha * next_action\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 20,
        "chapter_title": "Chapter 3: Policy Integration",
        "section": "3.1 Policy-to-Robot Interface",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 621,
        "has_code_block": true,
        "has_math": false,
        "source_file": "03-policy-integration.md"
      }
    },
    {
      "content": "### Workspace Limits\n\n**Problem**: VLA may command unsafe positions (outside reachable workspace)\n\n**Solution**: Clip actions to safe bounds\n\n```python\nclass WorkspaceSafetyWrapper:\n    def __init__(self, workspace_bounds):\n        \"\"\"\n        Args:\n            workspace_bounds: Dict with 'position' and 'orientation' limits\n\n        Example:\n            bounds = {\n                'position': {'min': [-0.5, -0.5, 0.0], 'max': [0.5, 0.5, 0.6]},\n                'orientation': {'min': [-3.14, -1.57, -3.14], 'max': [3.14, 1.57, 3.14]}\n            }\n        \"\"\"\n        self.pos_min = np.array(workspace_bounds['position']['min'])\n        self.pos_max = np.array(workspace_bounds['position']['max'])\n        self.ori_min = np.array(workspace_bounds['orientation']['min'])\n        self.ori_max = np.array(workspace_bounds['orientation']['max'])\n\n    def clip_action(self, action):\n        \"\"\"\n        Clip action to workspace bounds\n\n        Args:\n            action: (7,) [x, y, z, roll, pitch, yaw, gripper]\n\n        Returns:\n            clipped_action: Safe action within bounds\n        \"\"\"\n        safe_action = action.copy()\n\n        # Clip position (xyz)\n        safe_action[:3] = np.clip(action[:3], self.pos_min, self.pos_max)\n\n        # Clip orientation (rpy)\n        safe_action[3:6] = np.clip(action[3:6], self.ori_min, self.ori_max)\n\n        # Gripper unchanged\n        safe_action[6] = np.clip(action[6], 0, 1)\n\n        return safe_action\n```\n\n### Collision Avoidance\n\n**Real-time collision checking** with robot environment:\n\n```python\nimport pybullet as p\n\nclass CollisionChecker:\n    def __init__(self, robot_urdf, obstacle_urdfs):\n        \"\"\"\n        Initialize PyBullet for collision checking\n\n        Args:\n            robot_urdf: Path to robot URDF\n            obstacle_urdfs: List of paths to obstacle URDFs\n        \"\"\"\n        p.connect(p.DIRECT)  # Headless\n        self.robot_id = p.loadURDF(robot_urdf)\n        self.obstacle_ids = [p.loadURDF(urdf) for urdf in obstacle_urdfs]\n\n    def check_collision(self, joint_positions):\n        \"\"\"\n        Check if joint configuration causes collision\n\n        Args:\n            joint_positions: (n,) array of joint angles\n\n        Returns:\n            is_collision: True if collision detected\n        \"\"\"\n        # Set robot state\n        for i, pos in enumerate(joint_positions):\n            p.resetJointState(self.robot_id, i, pos)\n\n        # Check collisions\n        for obstacle_id in self.obstacle_ids:\n            contacts = p.getContactPoints(self.robot_id, obstacle_id)\n            if len(contacts) > 0:\n                return True  # Collision\n\n        return False  # Safe\n\n    def filter_safe_action(self, current_joints, target_action, num_checks=10):\n        \"\"\"\n        Filter action to avoid collisions via interpolation\n\n        Args:\n            current_joints: Current joint positions\n            target_action: Desired joint positions from VLA\n            num_checks: Number of intermediate points to check\n\n        Returns:\n            safe_action: Closest collision-free action\n        \"\"\"\n        for i in range(num_checks + 1):\n            alpha = i / num_checks\n            interpolated = (1 - alpha) * current_joints + alpha * target_action\n\n            if self.check_collision(interpolated):\n                # Collision detected, return previous safe point\n                safe_alpha = max(0, (i - 1) / num_checks)\n                return (1 - safe_alpha) * current_joints + safe_alpha * target_action\n\n        return target_action  # All checks passed\n```\n\n### Emergency Stop\n\n**Conditions for E-stop**:\n1. Force/torque limits exceeded (gripper crushing object)\n2. Joint limits violated\n3. Communication timeout (policy node crashed)\n\n```python\nclass EmergencyStop:\n    def __init__(self, force_limit=50.0, timeout=0.5):\n        \"\"\"\n        Args:\n            force_limit: Max force in Newtons\n            timeout: Max time without policy update (seconds)\n        \"\"\"\n        self.force_limit = force_limit\n        self.timeout = timeout\n        self.last_update_time = time.time()\n\n    def check_safety(self, force_reading, joint_positions, joint_limits):\n        \"\"\"\n        Returns:\n            (is_safe, reason)\n        \"\"\"\n        # Check force\n        if np.any(np.abs(force_reading) > self.force_limit):\n            return False, \"Force limit exceeded\"\n\n        # Check joint limits\n        if np.any(joint_positions < joint_limits['min']) or \\\n           np.any(joint_positions > joint_limits['max']):\n            return False, \"Joint limit violated\"\n\n        # Check timeout\n        if time.time() - self.last_update_time > self.timeout:\n            return False, \"Policy timeout\"\n\n        return True, \"OK\"\n\n    def update_timestamp(self):\n        self.last_update_time = time.time()\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 20,
        "chapter_title": "Chapter 3: Policy Integration",
        "section": "3.2 Safety Wrappers",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 448,
        "has_code_block": true,
        "has_math": false,
        "source_file": "03-policy-integration.md"
      }
    },
    {
      "content": "### Latency Budget\n\n**Total Latency**: Perception → Action\n- **Camera capture**: 16ms (60 FPS)\n- **Encoding/preprocessing**: 5ms\n- **VLA inference**: 20-50ms (GPU), 100ms (CPU)\n- **Action denormalization**: `<1ms`\n- **ROS 2 messaging**: 1-5ms\n- **Robot control**: 1ms\n- **Total**: 50-80ms (12-20 Hz control loop)\n\n**Latency Optimization**:\n```python\nimport torch\n\n# 1. Use FP16 inference (2x speedup)\nmodel = model.half()  # Convert to FP16\n\n# 2. TensorRT compilation (3-5x speedup)\nimport torch_tensorrt\n\ntrt_model = torch_tensorrt.compile(\n    model,\n    inputs=[torch_tensorrt.Input((1, 3, 224, 224))],\n    enabled_precisions={torch.float16}\n)\n\n# 3. Batch processing (if multiple cameras)\nactions = model(images_batch)  # Process all cameras at once\n```\n\n### Failure Modes and Recovery\n\n**Common Failures**:\n\n1. **Perception Failure**: Object not detected\n   ```python\n   def handle_perception_failure(confidence):\n       if confidence < 0.5:\n           # Request human demonstration\n           return \"teleop_mode\"\n   ```\n\n2. **Grasp Failure**: Object slipped\n   ```python\n   def detect_grasp_failure(gripper_force, expected_weight):\n       if gripper_force < 0.5 * expected_weight:\n           # Retry grasp with tighter grip\n           return \"retry_grasp\"\n   ```\n\n3. **Policy Uncertainty**: Low confidence action\n   ```python\n   def check_policy_confidence(action_distribution):\n       entropy = -torch.sum(action_distribution * torch.log(action_distribution))\n       if entropy > 2.0:  # High uncertainty\n           return \"request_human_help\"\n   ```\n\n### Monitoring and Logging\n\n**Real-time Metrics**:\n\n```python\nclass PolicyMonitor:\n    def __init__(self):\n        self.metrics = {\n            'inference_time': [],\n            'action_magnitude': [],\n            'safety_violations': 0,\n            'success_rate': []\n        }\n\n    def log_inference(self, inference_time, action):\n        self.metrics['inference_time'].append(inference_time)\n        self.metrics['action_magnitude'].append(np.linalg.norm(action))\n\n    def log_safety_violation(self):\n        self.metrics['safety_violations'] += 1\n\n    def log_task_outcome(self, success):\n        self.metrics['success_rate'].append(1.0 if success else 0.0)\n\n    def get_summary(self):\n        return {\n            'avg_latency_ms': np.mean(self.metrics['inference_time']) * 1000,\n            'p95_latency_ms': np.percentile(self.metrics['inference_time'], 95) * 1000,\n            'safety_violations': self.metrics['safety_violations'],\n            'success_rate': np.mean(self.metrics['success_rate'][-100:])  # Last 100 tasks\n        }\n```\n\n### Sim-to-Real Validation\n\n**Checklist before real-world deployment**:\n\n1. ✅ Test in simulation (Isaac Sim, Gazebo) with domain randomization\n2. ✅ Validate safety wrappers with edge cases\n3. ✅ Benchmark latency on target hardware (Jetson, workstation)\n4. ✅ Collect initial demonstrations on real robot (fine-tune if needed)\n5. ✅ Start with simple tasks (pick-and-place known objects)\n6. ✅ Human supervisor ready for e-stop (first 100 runs)\n7. ✅ Log all failures and update policy/safety rules",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 20,
        "chapter_title": "Chapter 3: Policy Integration",
        "section": "3.3 Real-World Deployment Considerations",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 312,
        "has_code_block": true,
        "has_math": false,
        "source_file": "03-policy-integration.md"
      }
    },
    {
      "content": "**Exercise 3.1**: ROS 2 Policy Integration\n- Implement VLA policy node in ROS 2 (code above)\n- Connect to simulated robot (Gazebo or Isaac Sim)\n- Test latency: measure end-to-end time from image → action execution\n- Optimize to achieve `<50ms` latency\n\n**Exercise 3.2**: Safety Wrapper Implementation\n- Implement workspace limits and collision checking\n- Test with intentionally unsafe actions (outside bounds, collision paths)\n- Measure false positive rate (safe actions rejected) and false negative rate (unsafe actions allowed)\n\n**Exercise 3.3**: Failure Mode Analysis\n- Simulate 3 failure modes: perception failure, grasp failure, policy uncertainty\n- Implement recovery strategies for each\n- Measure recovery success rate\n\n**Exercise 3.4**: Real-World Deployment\n- Deploy VLA policy on real robot (if available) or high-fidelity sim\n- Run 50 pick-and-place tasks\n- Log: success rate, latency, safety violations, failure modes\n- Create deployment report with recommendations",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 20,
        "chapter_title": "Chapter 3: Policy Integration",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 142,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-policy-integration.md"
      }
    },
    {
      "content": "**Policy-Robot Interface**: Action denormalization, ROS 2 integration, frequency bridging (10 Hz → 1000 Hz)\n**Safety Wrappers**: Workspace limits, collision avoidance, emergency stop\n**Deployment**: Latency optimization (`<50ms`), failure recovery, monitoring, sim-to-real validation\n\n**Next**: Chapter 4 covers humanoid-specific skills (manipulation, locomotion, multi-task learning).",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 20,
        "chapter_title": "Chapter 3: Policy Integration",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 41,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-policy-integration.md"
      }
    },
    {
      "content": "---\nsidebar_position: 4\ntitle: \"Chapter 4: Humanoid Skills with VLA\"\ndescription: Manipulation, locomotion, and multi-task learning for humanoid robots\ntags: [humanoid, manipulation, locomotion, multi-task, vla]\n---\n\n# Chapter 4: Humanoid Skills with VLA",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 21,
        "chapter_title": "Chapter 4: Humanoid Skills with VLA",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 33,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-humanoid-skills.md"
      }
    },
    {
      "content": "1. Apply VLA models to manipulation tasks (grasping, dexterous manipulation)\n2. Integrate VLA with locomotion policies for mobile manipulation\n3. Implement multi-task learning and skill composition for humanoid robots",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 21,
        "chapter_title": "Chapter 4: Humanoid Skills with VLA",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 29,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-humanoid-skills.md"
      }
    },
    {
      "content": "### Grasping with VLA\n\n**Task**: \"Grasp the red mug from the table\"\n\n**VLA Output**: 6-DOF gripper pose + gripper state\n\n**Pipeline**:\n\n```mermaid\ngraph LR\n    Image[Camera Image<br/>RGB-D] --> VLA[VLA Model<br/>OpenVLA]\n    Instruction[\"Instruction<br/>'grasp red mug'\"] --> VLA\n\n    VLA --> Pose[Gripper Pose<br/>x,y,z,r,p,y]\n    VLA --> GripperState[Gripper State<br/>open/close]\n\n    Pose --> IK[Inverse Kinematics]\n    IK --> JointCmd[Joint Commands]\n\n    JointCmd --> Controller[Arm Controller]\n    GripperState --> Controller\n\n    Controller --> Robot[Humanoid Arm]\n\n    style VLA fill:#e1f5ff\n    style IK fill:#ffe1e1\n```\n\n**Figure 4.1**: VLA-based grasping pipeline showing conversion from end-effector pose to joint commands via inverse kinematics.\n\n**Implementation**:\n\n```python\nimport numpy as np\nfrom openvla import OpenVLA\nfrom scipy.spatial.transform import Rotation\n\nclass VLAGrasper:\n    def __init__(self, robot_interface):\n        self.vla = OpenVLA.from_pretrained(\"openvla-7b\").to(\"cuda\")\n        self.robot = robot_interface\n\n    def grasp_object(self, image, instruction):\n        \"\"\"\n        Execute grasp using VLA\n\n        Args:\n            image: RGB image (224, 224, 3)\n            instruction: str, e.g., \"grasp the red mug\"\n\n        Returns:\n            success: bool\n        \"\"\"\n        # 1. Predict grasp pose with VLA\n        action = self.vla.predict_action(image, instruction)\n        pose = action[:6]  # x, y, z, roll, pitch, yaw\n        gripper_cmd = action[6]  # 0-1 (open-close)\n\n        # 2. Pre-grasp pose (10cm above target)\n        pregrasp_pose = pose.copy()\n        pregrasp_pose[2] += 0.1  # Lift z by 10cm\n\n        # 3. Execute motion sequence\n        # Step 1: Move to pre-grasp\n        self.robot.move_to_pose(pregrasp_pose, speed=0.2)\n\n        # Step 2: Open gripper\n        self.robot.set_gripper(0.0)  # Fully open\n\n        # Step 3: Move to grasp pose\n        self.robot.move_to_pose(pose, speed=0.1)\n\n        # Step 4: Close gripper\n        self.robot.set_gripper(gripper_cmd)\n\n        # Step 5: Lift object\n        lift_pose = pose.copy()\n        lift_pose[2] += 0.15\n        self.robot.move_to_pose(lift_pose, speed=0.1)\n\n        # 6. Check grasp success (force sensor)\n        force = self.robot.get_gripper_force()\n        success = force > 1.0  # Object held\n\n        return success\n```\n\n### Dexterous Manipulation\n\n**Task**: \"Open the drawer\" (requires force control, multi-step reasoning)\n\n**Challenges**:\n1. **Contact-rich**: Gripper must maintain contact with handle\n2. **Force control**: Pull with appropriate force (not too hard/soft)\n3. **Multi-step**: (1) Grasp handle, (2) Pull, (3) Release\n\n**VLA + Impedance Control**:\n\n```python\nclass DexterousManipulator:\n    def __init__(self, vla_model, robot):\n        self.vla = vla_model\n        self.robot = robot\n\n    def open_drawer(self, image, instruction=\"open the drawer\"):\n        \"\"\"\n        Complex manipulation with force feedback\n\n        Strategy:\n        1. VLA predicts grasp pose and pull direction\n        2. Impedance controller executes pull with force limits\n        \"\"\"\n        # VLA prediction\n        action = self.vla.predict_action(image, instruction)\n        handle_pose = action[:6]\n        pull_direction = action[6:9]  # 3D vector (typically [-1, 0, 0])\n\n        # Grasp handle\n        self.robot.move_to_pose(handle_pose)\n        self.robot.set_gripper(1.0)  # Close\n\n        # Impedance control for pulling\n        # Low stiffness in pull direction, high in others\n        stiffness = np.array([100, 1000, 1000, 100, 100, 100])  # [x, y, z, rx, ry, rz]\n        damping = 2 * np.sqrt(stiffness)  # Critical damping\n\n        # Pull for 2 seconds or until drawer opens\n        start_time = time.time()\n        while time.time() - start_time < 2.0:\n            # Current pose\n            current_pose = self.robot.get_ee_pose()\n\n            # Desired pose (incremental pull)\n            desired_pose = current_pose + 0.01 * pull_direction\n\n            # Impedance control law\n            pose_error = desired_pose - current_pose\n            force_cmd = stiffness * pose_error - damping * self.robot.get_ee_velocity()\n\n            # Execute\n            self.robot.apply_ee_force(force_cmd)\n\n            # Check if drawer opened (depth camera feedback)\n            if self.check_drawer_open(image_stream):\n                break\n\n        # Release handle\n        self.robot.set_gripper(0.0)\n\n        return self.check_drawer_open(self.robot.get_camera_image())\n```\n\n### Tool Use\n\n**Task**: \"Use the hammer to hit the nail\"\n\n**VLA Capabilities**:\n- Affordance reasoning: Identifies hammer as tool for hammering\n- Grasp planning: Grips hammer handle (not head)\n- Motion planning: Swinging motion to strike nail\n\n```python\ndef use_tool(image, instruction=\"use the hammer\"):\n    # VLA predicts:\n    # 1. Tool to grasp (bounding box + grasp pose)\n    # 2. Tool-use motion (trajectory waypoints)\n\n    action = vla.predict_action(image, instruction)\n\n    # Parse action\n    tool_grasp_pose = action[:6]\n    motion_waypoints = action[6:].reshape(-1, 6)  # N waypoints\n\n    # Execute\n    robot.move_to_pose(tool_grasp_pose)\n    robot.set_gripper(1.0)\n\n    for waypoint in motion_waypoints:\n        robot.move_to_pose(waypoint, speed=0.3)\n\n    robot.set_gripper(0.0)  # Release tool\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 21,
        "chapter_title": "Chapter 4: Humanoid Skills with VLA",
        "section": "4.1 Manipulation Skills",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 573,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-humanoid-skills.md"
      }
    },
    {
      "content": "### VLA for Navigation\n\n**Task**: \"Walk to the kitchen\"\n\n**Challenge**: VLA trained on manipulation data may not generalize to locomotion\n\n**Solution**: Hierarchical control\n- **High-level** (VLA): Goal selection (\"go to kitchen\")\n- **Low-level** (RL policy): Footstep planning and balance\n\n```python\nclass HierarchicalNavigator:\n    def __init__(self, vla_high_level, rl_low_level):\n        self.vla = vla_high_level  # Trained on navigation tasks\n        self.rl_policy = rl_low_level  # Trained in Isaac Gym\n\n    def navigate_to_goal(self, image, instruction=\"go to the kitchen\"):\n        \"\"\"\n        VLA: Image → Waypoints\n        RL: Waypoints → Joint torques\n        \"\"\"\n        # VLA predicts waypoint path\n        waypoints = self.vla.predict_waypoints(image, instruction)\n        # waypoints: [(x1, y1), (x2, y2), ..., (xn, yn)]\n\n        # RL policy tracks waypoints\n        for waypoint in waypoints:\n            while not self.reached_waypoint(waypoint):\n                # Get robot state\n                state = self.get_state()  # Joint angles, IMU, foot contacts\n\n                # RL policy computes joint torques\n                torques = self.rl_policy.predict(state, goal=waypoint)\n\n                # Execute\n                self.robot.set_joint_torques(torques)\n                time.sleep(0.01)  # 100 Hz control\n```\n\n### Whole-Body Manipulation\n\n**Task**: \"Pick up the box from the floor while walking\"\n\n**Requirements**:\n- Locomotion: Maintain balance while walking\n- Manipulation: Reach down to grasp box\n- Coordination: Synchronize arm and leg movements\n\n**Whole-Body VLA**:\n\n```python\nclass WholeBodyVLA:\n    def __init__(self):\n        # Shared vision-language encoder\n        self.encoder = VisionLanguageEncoder()\n\n        # Separate heads for arms and legs\n        self.arm_head = ActionHead(action_dim=14)  # 7-DOF x2 arms\n        self.leg_head = ActionHead(action_dim=12)  # 6-DOF x2 legs\n\n    def forward(self, image, instruction):\n        # Shared encoding\n        features = self.encoder(image, instruction)\n\n        # Predict arm and leg actions\n        arm_actions = self.arm_head(features)  # (14,)\n        leg_actions = self.leg_head(features)  # (12,)\n\n        return {\n            'arms': arm_actions,\n            'legs': leg_actions\n        }\n\n# Usage\nwhole_body = WholeBodyVLA()\nactions = whole_body(image, \"pick up box while walking forward\")\n\n# Execute coordinated motion\nrobot.set_arm_positions(actions['arms'])\nrobot.set_leg_positions(actions['legs'])\n```\n\n### Mobile Manipulation\n\n**Task**: \"Navigate to the table and pick up the cup\"\n\n**Two-stage approach**:\n\n1. **Stage 1**: Navigate to table (locomotion VLA)\n2. **Stage 2**: Pick up cup (manipulation VLA)\n\n```python\ndef mobile_manipulation(image, instruction=\"go to table and pick up cup\"):\n    # Parse instruction into subtasks\n    subtasks = parse_instruction(instruction)\n    # ['navigate to table', 'pick up cup']\n\n    for subtask in subtasks:\n        if 'navigate' in subtask or 'go to' in subtask:\n            # Locomotion\n            navigate_to_goal(image, subtask)\n        elif 'pick up' in subtask or 'grasp' in subtask:\n            # Manipulation\n            grasp_object(image, subtask)\n        else:\n            # General VLA (handles both)\n            execute_action(image, subtask)\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 21,
        "chapter_title": "Chapter 4: Humanoid Skills with VLA",
        "section": "4.2 Locomotion Skills",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 361,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-humanoid-skills.md"
      }
    },
    {
      "content": "### Task Composition\n\n**Primitive Skills**:\n- `grasp(object)`\n- `place(location)`\n- `navigate(goal)`\n- `open(container)`\n\n**Composed Tasks**:\n- \"Pick and place\" = `grasp(cup)` → `navigate(table)` → `place(table)`\n- \"Fetch and deliver\" = `navigate(kitchen)` → `open(drawer)` → `grasp(item)` → `navigate(living_room)` → `place(table)`\n\n**Implementation** (Behavior Tree):\n\n```python\nclass BehaviorTree:\n    def __init__(self, vla_model):\n        self.vla = vla_model\n        self.skills = {\n            'grasp': self.grasp_skill,\n            'place': self.place_skill,\n            'navigate': self.navigate_skill,\n            'open': self.open_skill\n        }\n\n    def execute_task(self, task_description):\n        \"\"\"\n        Parse language instruction into skill sequence\n\n        Example:\n            \"Go to kitchen and pick up the mug\"\n            → ['navigate(kitchen)', 'grasp(mug)']\n        \"\"\"\n        skill_sequence = self.parse_to_skills(task_description)\n\n        for skill_name, args in skill_sequence:\n            skill_fn = self.skills[skill_name]\n            success = skill_fn(*args)\n\n            if not success:\n                return False  # Task failed\n\n        return True  # All skills succeeded\n\n    def parse_to_skills(self, instruction):\n        # Use VLA language understanding\n        # Or simple rule-based parsing for now\n        if \"go to\" in instruction and \"pick up\" in instruction:\n            location = extract_location(instruction)\n            object_name = extract_object(instruction)\n            return [('navigate', [location]), ('grasp', [object_name])]\n        # ... more rules\n```\n\n### Multi-Task VLA Training\n\n**Dataset**: Mix of manipulation, navigation, and tool-use tasks\n\n**Training Strategy**:\n\n```python\n# Multi-task dataset\ndatasets = {\n    'manipulation': load_dataset('bridge_v2'),  # 60k demos\n    'navigation': load_dataset('go_stanford'),  # 50k demos\n    'tool_use': load_dataset('language_table')  # 40k demos\n}\n\n# Multi-task dataloader\nmulti_task_loader = MultiTaskDataLoader(datasets, batch_size=32)\n\n# Training loop\nfor batch in multi_task_loader:\n    images, instructions, actions, task_ids = batch\n\n    # Forward pass (task-conditioned)\n    predicted_actions = vla(images, instructions, task_id=task_ids)\n\n    # Compute loss (per-task weighting)\n    losses = {}\n    for task_name in ['manipulation', 'navigation', 'tool_use']:\n        mask = (task_ids == task_name)\n        if mask.sum() > 0:\n            losses[task_name] = F.mse_loss(\n                predicted_actions[mask],\n                actions[mask]\n            )\n\n    # Weighted combination\n    total_loss = 0.5 * losses['manipulation'] + \\\n                 0.3 * losses['navigation'] + \\\n                 0.2 * losses['tool_use']\n\n    total_loss.backward()\n    optimizer.step()\n```\n\n### Transfer Learning\n\n**Scenario**: Train on simulated humanoid → Deploy on real humanoid\n\n**Approach**:\n\n1. **Pretrain** on Open X-Embodiment (diverse robots, 1M demos)\n2. **Fine-tune** on target humanoid in simulation (Isaac Sim, 10k demos)\n3. **Domain adapt** with real robot data (1k demos)\n\n```python\n# Step 1: Pretrain (done by OpenVLA team)\nvla = OpenVLA.from_pretrained(\"openvla-7b\")\n\n# Step 2: Fine-tune on humanoid sim data\nhumanoid_sim_data = load_isaac_sim_dataset(\"humanoid_tasks\")\nvla.fine_tune(humanoid_sim_data, epochs=20)\n\n# Step 3: Domain adaptation (real robot)\nreal_robot_data = collect_real_demos(num_demos=1000)\nvla.fine_tune(\n    real_robot_data,\n    epochs=5,\n    learning_rate=1e-6,  # Small LR to avoid catastrophic forgetting\n    freeze_encoder=True  # Only update action head\n)\n\n# Deploy\nvla.save(\"openvla-humanoid-real\")\n```\n\n### Continual Learning\n\n**Problem**: Robot learns new task, forgets old tasks (catastrophic forgetting)\n\n**Solution**: Elastic Weight Consolidation (EWC)\n\n```python\nclass ContinualVLA:\n    def __init__(self, base_model):\n        self.model = base_model\n        self.fisher_information = {}  # Importance of each weight\n        self.optimal_params = {}  # Previous task's optimal weights\n\n    def train_new_task(self, new_task_data, lambda_ewc=1000):\n        \"\"\"\n        Train on new task while preserving old task performance\n\n        Args:\n            new_task_data: Dataset for new task\n            lambda_ewc: Regularization strength\n        \"\"\"\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-5)\n\n        for batch in new_task_data:\n            # Standard loss on new task\n            loss_new = self.model.compute_loss(batch)\n\n            # EWC penalty (preserve important weights)\n            loss_ewc = 0\n            for name, param in self.model.named_parameters():\n                if name in self.fisher_information:\n                    fisher = self.fisher_information[name]\n                    optimal = self.optimal_params[name]\n                    loss_ewc += (fisher * (param - optimal) ** 2).sum()\n\n            # Combined loss\n            loss = loss_new + lambda_ewc * loss_ewc\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    def compute_fisher_information(self, old_task_data):\n        \"\"\"\n        Compute Fisher information after training on a task\n        \"\"\"\n        self.model.eval()\n        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters()}\n\n        for batch in old_task_data:\n            self.model.zero_grad()\n            loss = self.model.compute_loss(batch)\n            loss.backward()\n\n            for name, param in self.model.named_parameters():\n                fisher[name] += param.grad.data ** 2 / len(old_task_data)\n\n        self.fisher_information = fisher\n        self.optimal_params = {n: p.clone().detach() for n, p in self.model.named_parameters()}\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 21,
        "chapter_title": "Chapter 4: Humanoid Skills with VLA",
        "section": "4.3 Multi-Task Learning",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 558,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-humanoid-skills.md"
      }
    },
    {
      "content": "**Exercise 4.1**: VLA Grasping\n- Implement VLA-based grasping in simulation (Isaac Sim or Gazebo)\n- Test on 10 different objects (varied shapes, sizes, materials)\n- Measure success rate and analyze failure modes\n\n**Exercise 4.2**: Whole-Body Manipulation\n- Train whole-body VLA with separate arm/leg heads\n- Task: \"Walk forward 2 meters while holding a tray level\"\n- Evaluate: walking speed, tray stability, success rate\n\n**Exercise 4.3**: Multi-Task Learning\n- Create dataset with 3 tasks: grasping, navigation, opening drawers\n- Train single VLA on all tasks\n- Compare with 3 separate task-specific models (data efficiency, performance)\n\n**Exercise 4.4**: Continual Learning\n- Train VLA on Task A (grasping cups)\n- Train on Task B (grasping boxes) with and without EWC\n- Measure forgetting: performance on Task A after learning Task B",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 21,
        "chapter_title": "Chapter 4: Humanoid Skills with VLA",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 128,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-humanoid-skills.md"
      }
    },
    {
      "content": "**Manipulation**: VLA for grasping, dexterous manipulation, tool use (affordance reasoning)\n**Locomotion**: Hierarchical control (VLA high-level, RL low-level), whole-body coordination\n**Multi-Task**: Skill composition, task-conditioned training, continual learning (EWC)\n\n**Module 4 Complete!** Next module covers capstone project: autonomous humanoid system.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 21,
        "chapter_title": "Chapter 4: Humanoid Skills with VLA",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 38,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-humanoid-skills.md"
      }
    },
    {
      "content": "---\nsidebar_position: 1\ntitle: \"Chapter 1: Autonomous Humanoid System Overview\"\ndescription: System requirements, architecture, and development roadmap for autonomous humanoid robots\ntags: [capstone, system-architecture, humanoid, requirements, roadmap]\n---\n\n# Chapter 1: Autonomous Humanoid System Overview",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 22,
        "chapter_title": "Chapter 1: Autonomous Humanoid System Overview",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 35,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-system-overview.md"
      }
    },
    {
      "content": "1. Define system requirements for an autonomous humanoid robot\n2. Design complete system architecture (hardware + software)\n3. Create phased development roadmap for implementation",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 22,
        "chapter_title": "Chapter 1: Autonomous Humanoid System Overview",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 24,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-system-overview.md"
      }
    },
    {
      "content": "### Use Cases\n\n**Primary Use Case**: Autonomous household assistant\n\n**Scenarios**:\n1. **Kitchen Assistant**: \"Fetch me a drink from the fridge\"\n   - Navigate to kitchen\n   - Open refrigerator door\n   - Identify and grasp drink\n   - Navigate back to user\n   - Hand over drink\n\n2. **Cleaning Helper**: \"Clean up the toys from the floor\"\n   - Navigate to living room\n   - Detect toys on floor (vision)\n   - Pick up each toy\n   - Place in designated container\n   - Return to charging station\n\n3. **Object Delivery**: \"Bring me the book from the shelf\"\n   - Parse natural language instruction\n   - Navigate to bookshelf\n   - Identify target book (visual + semantic understanding)\n   - Grasp and retrieve\n   - Deliver to user\n\n### Functional Requirements\n\n**FR1: Locomotion**\n- Walk on flat surfaces at 0.5 m/s (minimum)\n- Climb stairs (max height: 20cm per step)\n- Maintain balance during manipulation tasks\n- Navigate around obstacles (dynamic path planning)\n\n**FR2: Manipulation**\n- Grasp objects (0.1-2 kg weight range)\n- Dual-arm coordination (e.g., open door while holding object)\n- Fine manipulation (e.g., press buttons, turn knobs)\n- Tool use (e.g., use broom, open drawers)\n\n**FR3: Perception**\n- 3D environment mapping (SLAM)\n- Object detection and recognition (100+ common household objects)\n- Human detection and tracking\n- Depth estimation for grasp planning\n\n**FR4: Autonomy**\n- Natural language command understanding\n- Task planning from high-level goals\n- Obstacle avoidance and replanning\n- Error recovery (e.g., retry failed grasp)\n\n**FR5: Safety**\n- Collision avoidance (humans, furniture)\n- Compliant control (soft contact with environment)\n- Emergency stop capability\n- Fall detection and safe shutdown\n\n### Non-Functional Requirements\n\n**NFR1: Performance**\n- End-to-end task latency: `<10s` (from command to action start)\n- Real-time control frequency: 100 Hz (low-level), 10 Hz (high-level)\n- Perception update rate: 30 Hz (vision), 100 Hz (IMU)\n\n**NFR2: Reliability**\n- Task success rate: `>80%` on known tasks\n- MTBF (Mean Time Between Failures): `>4 hours` continuous operation\n- Battery life: `>2 hours` active use\n\n**NFR3: Usability**\n- Natural language interface (no programming required)\n- One-button start/stop\n- Audio/visual feedback (status indicators)\n\n**NFR4: Cost**\n- Target: `<$50k` total system cost\n- Compute: Jetson Orin (edge AI) + optional cloud offload\n\n### System Specifications\n\n| Component | Specification |\n|-----------|---------------|\n| **Height** | 150-180 cm (human-scale) |\n| **Weight** | 40-60 kg |\n| **DOF** | 30+ (12 legs, 14 arms, 3 torso, 1+ head) |\n| **Payload** | 5 kg per arm |\n| **Speed** | 0.5 m/s walking, 1.0 m/s max |\n| **Battery** | 48V LiFePO4, 10Ah (2 hrs runtime) |\n| **Compute** | Jetson Orin (275 TOPS AI), ARM Cortex (real-time control) |\n| **Sensors** | RGB-D cameras (2), IMU (1), Force/torque (hands, feet) |",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 22,
        "chapter_title": "Chapter 1: Autonomous Humanoid System Overview",
        "section": "1.1 Requirements Analysis",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 443,
        "has_code_block": false,
        "has_math": true,
        "source_file": "01-system-overview.md"
      }
    },
    {
      "content": "### Hardware Architecture\n\n```mermaid\ngraph TB\n    subgraph Sensors[Sensors]\n        Camera1[Front Camera<br/>RealSense D435i]\n        Camera2[Wrist Camera<br/>RealSense D435i]\n        IMU[IMU<br/>BNO055]\n        FT_Hands[Force/Torque<br/>Hands x2]\n        FT_Feet[Force/Torque<br/>Feet x2]\n        Encoders[Joint Encoders<br/>30x]\n    end\n\n    subgraph Compute[Compute]\n        Jetson[NVIDIA Jetson Orin<br/>AI Perception + VLA]\n        RT_MCU[Real-Time MCU<br/>STM32H7<br/>Motor Control]\n    end\n\n    subgraph Actuators[Actuators]\n        MotorsLegs[Leg Motors<br/>12x Brushless]\n        MotorsArms[Arm Motors<br/>14x Brushless]\n        Grippers[Grippers<br/>2x Servo]\n    end\n\n    subgraph Power[Power]\n        Battery[Battery<br/>48V 10Ah LiFePO4]\n        BMS[Battery Management]\n        DC_DC[DC-DC Converters<br/>48V→24V→12V→5V]\n    end\n\n    Camera1 --> Jetson\n    Camera2 --> Jetson\n    IMU --> RT_MCU\n    FT_Hands --> RT_MCU\n    FT_Feet --> RT_MCU\n    Encoders --> RT_MCU\n\n    Jetson <-->|EtherCAT| RT_MCU\n\n    RT_MCU --> MotorsLegs\n    RT_MCU --> MotorsArms\n    RT_MCU --> Grippers\n\n    Battery --> BMS\n    BMS --> DC_DC\n    DC_DC --> Jetson\n    DC_DC --> RT_MCU\n    DC_DC --> MotorsLegs\n    DC_DC --> MotorsArms\n\n    style Compute fill:#e1f5ff\n    style Actuators fill:#ffe1e1\n    style Sensors fill:#ffffcc\n    style Power fill:#e1ffe1\n```\n\n**Figure 1.1**: Hardware architecture showing sensor inputs, compute nodes (Jetson for AI, MCU for real-time control), actuators, and power distribution.\n\n### Software Architecture\n\n```mermaid\ngraph TB\n    subgraph User[User Interface]\n        Voice[Voice Input<br/>Whisper ASR]\n        Display[Status Display<br/>LED + Screen]\n    end\n\n    subgraph HighLevel[High-Level Planning - Jetson Orin]\n        VLA[VLA Policy<br/>OpenVLA 7B]\n        TaskPlanner[Task Planner<br/>BT/FSM]\n        SLAM[SLAM<br/>Isaac ROS Visual SLAM]\n        ObjectDet[Object Detection<br/>YOLO + Segmentation]\n    end\n\n    subgraph MidLevel[Mid-Level Control - Jetson + MCU]\n        MotionPlan[Motion Planning<br/>cuRobo]\n        PathPlan[Path Planning<br/>Nav2]\n        GraspPlan[Grasp Planning<br/>GraspNet]\n    end\n\n    subgraph LowLevel[Low-Level Control - Real-Time MCU]\n        WBC[Whole-Body Controller<br/>QP-based]\n        BalanceCtrl[Balance Controller<br/>ZMP/DCM]\n        JointCtrl[Joint Controllers<br/>PID x30]\n    end\n\n    Voice --> VLA\n    VLA --> TaskPlanner\n    TaskPlanner --> MotionPlan\n    TaskPlanner --> PathPlan\n\n    ObjectDet --> GraspPlan\n    SLAM --> PathPlan\n    GraspPlan --> MotionPlan\n    PathPlan --> MotionPlan\n\n    MotionPlan --> WBC\n    WBC --> BalanceCtrl\n    BalanceCtrl --> JointCtrl\n\n    JointCtrl --> Display\n\n    style HighLevel fill:#e1f5ff\n    style MidLevel fill:#ffffcc\n    style LowLevel fill:#ffe1e1\n```\n\n**Figure 1.2**: Software architecture with three control layers: high-level (VLA + planning), mid-level (motion/path planning), low-level (whole-body + joint control).\n\n### System Integration\n\n**ROS 2 Communication**:\n```python\n# High-level VLA node publishes tasks\n/vla/task_command (std_msgs/String)\n\n# Task planner decomposes into primitives\n/planner/motion_goal (geometry_msgs/PoseStamped)\n/planner/navigation_goal (nav_msgs/Path)\n\n# Mid-level planners publish trajectories\n/motion/joint_trajectory (trajectory_msgs/JointTrajectory)\n/navigation/cmd_vel (geometry_msgs/Twist)\n\n# Low-level controller executes\n/joint_commands (control_msgs/JointJog)\n```\n\n**Data Flow Example** (Fetch drink task):\n1. User: \"Fetch me a drink\" (voice)\n2. VLA: Parse → \"navigate(kitchen) + grasp(drink) + navigate(user)\"\n3. Task Planner: Execute navigation → Execute grasp → Execute return\n4. Motion Planner: Generate arm trajectory for grasp\n5. Path Planner: Generate walking path to kitchen\n6. Whole-Body Controller: Coordinate arms + legs\n7. Joint Controllers: Execute motor commands",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 22,
        "chapter_title": "Chapter 1: Autonomous Humanoid System Overview",
        "section": "1.2 System Architecture",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 370,
        "has_code_block": true,
        "has_math": false,
        "source_file": "01-system-overview.md"
      }
    },
    {
      "content": "### Phase 1: Simulation Setup (Weeks 1-2)\n\n**Goal**: Virtual twin in Isaac Sim\n\n**Tasks**:\n- Import humanoid URDF (or use pre-built model like G1, Unitree H1)\n- Configure PhysX physics (realistic contacts, friction)\n- Add environment (kitchen, living room)\n- Integrate ROS 2 bridge\n\n**Deliverable**: Teleoperable humanoid in simulation\n\n**Validation**: Walk 10 meters, grasp objects, avoid obstacles\n\n### Phase 2: Perception Stack (Weeks 3-4)\n\n**Goal**: Real-time environment understanding\n\n**Tasks**:\n- Deploy Isaac ROS Visual SLAM on Jetson\n- Integrate YOLOv8 for object detection\n- Implement depth-based grasp pose estimation\n- Fuse IMU + camera for state estimation\n\n**Deliverable**: Perception pipeline (30 Hz)\n\n**Validation**: Detect 10 objects, map 50m² environment, localize with `<5cm` error\n\n### Phase 3: Base Control (Weeks 5-6)\n\n**Goal**: Stable locomotion + manipulation\n\n**Tasks**:\n- Implement balance controller (ZMP-based)\n- Train RL policy for walking in Isaac Gym (100k steps)\n- Fine-tune on Isaac Sim with randomization\n- Implement dual-arm manipulation primitives\n\n**Deliverable**: Walk + grasp controller\n\n**Validation**: Walk 20m without falling, grasp 5/5 objects in sim\n\n### Phase 4: VLA Integration (Weeks 7-8)\n\n**Goal**: Language-conditioned autonomy\n\n**Tasks**:\n- Fine-tune OpenVLA on humanoid tasks (1k demos)\n- Integrate VLA with task planner (behavior tree)\n- Implement safety wrappers (collision, workspace limits)\n- Deploy on Jetson Orin (optimize for `<50ms` latency)\n\n**Deliverable**: End-to-end autonomous system (sim)\n\n**Validation**: Complete 3 tasks: fetch, place, navigate\n\n### Phase 5: Sim-to-Real Transfer (Weeks 9-10)\n\n**Goal**: Deploy on real hardware\n\n**Tasks**:\n- System identification (measure real joint dynamics)\n- Domain randomization (10x varied sim environments)\n- Collect real-world fine-tuning data (100 demos)\n- Progressive deployment (sim → table-top → full robot)\n\n**Deliverable**: Real robot executing simple tasks\n\n**Validation**: 50% success on fetch task (real world)\n\n### Phase 6: Iteration and Scaling (Weeks 11-12)\n\n**Goal**: Improve robustness and generalization\n\n**Tasks**:\n- Failure analysis (log 100 attempts)\n- Update VLA with corrective demos (DAgger)\n- Expand object set (100 → 200 objects)\n- Multi-task evaluation (5 different tasks)\n\n**Deliverable**: Production-ready system\n\n**Validation**: 80% success across 5 tasks, `<5min` per task\n\n### Risk Mitigation\n\n| Risk | Impact | Mitigation |\n|------|--------|------------|\n| **Hardware failure** | High | Redundant sensors, modular design, spare parts |\n| **Sim-to-real gap** | High | Extensive randomization, real-world fine-tuning, progressive transfer |\n| **VLA latency** | Medium | Quantization (INT8), TensorRT, model distillation |\n| **Battery life** | Medium | Power-efficient hardware, sleep modes, hot-swappable batteries |\n| **Safety incidents** | Critical | Multi-layer safety (software + hardware e-stop), human-in-loop testing |",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 22,
        "chapter_title": "Chapter 1: Autonomous Humanoid System Overview",
        "section": "1.3 Development Roadmap",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 411,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-system-overview.md"
      }
    },
    {
      "content": "**Exercise 1.1**: Requirements Specification\n- Define requirements for a warehouse robot (vs household)\n- List 5 functional requirements specific to warehouse tasks\n- Specify non-functional requirements (speed, payload, uptime)\n\n**Exercise 1.2**: Hardware Selection\n- Choose sensors for outdoor humanoid (vs indoor)\n- Select compute platform (Jetson Orin vs AGX Xavier vs cloud)\n- Justify choices with performance/cost tradeoffs\n\n**Exercise 1.3**: Software Architecture Design\n- Design software stack for a quadruped robot (vs humanoid)\n- Identify differences in control layers\n- Draw architecture diagram (Mermaid or similar)\n\n**Exercise 1.4**: Development Planning\n- Create 8-week roadmap for your capstone project\n- Define milestones and validation criteria\n- Identify top 3 risks and mitigation strategies",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 22,
        "chapter_title": "Chapter 1: Autonomous Humanoid System Overview",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 112,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-system-overview.md"
      }
    },
    {
      "content": "**Requirements**: Household assistant with locomotion, manipulation, perception, autonomy, and safety\n**Architecture**: Hardware (sensors, compute, actuators, power) + Software (3-layer control hierarchy)\n**Roadmap**: 12-week phased development (sim → perception → control → VLA → real → iteration)\n\n**Next**: Chapter 2 covers perception stack implementation (SLAM, object detection, state estimation).",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 22,
        "chapter_title": "Chapter 1: Autonomous Humanoid System Overview",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 48,
        "has_code_block": false,
        "has_math": false,
        "source_file": "01-system-overview.md"
      }
    },
    {
      "content": "---\nsidebar_position: 2\ntitle: \"Chapter 2: Perception Stack\"\ndescription: Sensor integration, SLAM, object detection, and state estimation for autonomous humanoids\ntags: [perception, slam, object-detection, state-estimation, sensor-fusion]\n---\n\n# Chapter 2: Perception Stack",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 23,
        "chapter_title": "Chapter 2: Perception Stack",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 32,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-perception-stack.md"
      }
    },
    {
      "content": "1. Integrate multi-modal sensors (RGB-D cameras, IMU, force/torque)\n2. Implement real-time SLAM and object detection pipeline\n3. Fuse sensor data for robust state estimation",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 23,
        "chapter_title": "Chapter 2: Perception Stack",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 24,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-perception-stack.md"
      }
    },
    {
      "content": "### RGB-D Cameras\n\n**Intel RealSense D435i**: Depth + RGB + IMU\n\n**Specifications**:\n- Depth range: 0.3-10m\n- Resolution: 1280x720 @ 30 FPS\n- FOV: 87° x 58°\n- IMU: BMI055 (accel + gyro)\n\n**ROS 2 Integration**:\n```bash\n# Install RealSense ROS 2 wrapper\nsudo apt-get install ros-humble-realsense2-camera\n\n# Launch camera node\nros2 launch realsense2_camera rs_launch.py \\\n    enable_depth:=true \\\n    enable_color:=true \\\n    enable_infra:=false \\\n    enable_gyro:=true \\\n    enable_accel:=true \\\n    depth_module.profile:=640x480x30 \\\n    rgb_camera.profile:=640x480x30\n```\n\n**Published Topics**:\n```\n/camera/color/image_raw (sensor_msgs/Image)\n/camera/depth/image_rect_raw (sensor_msgs/Image)\n/camera/aligned_depth_to_color/image_raw (sensor_msgs/Image)\n/camera/imu (sensor_msgs/Imu)\n/camera/camera_info (sensor_msgs/CameraInfo)\n```\n\n### IMU (Inertial Measurement Unit)\n\n**BNO055**: 9-DOF IMU (accel, gyro, magnetometer)\n\n**Calibration**:\n```python\nimport board\nimport adafruit_bno055\n\ni2c = board.I2C()\nimu = adafruit_bno055.BNO055_I2C(i2c)\n\n# Calibration status (0-3 for each)\nprint(f\"System: {imu.calibration_status[0]}\")\nprint(f\"Gyro: {imu.calibration_status[1]}\")\nprint(f\"Accel: {imu.calibration_status[2]}\")\nprint(f\"Mag: {imu.calibration_status[3]}\")\n\n# Wait until fully calibrated\nwhile imu.calibration_status != (3, 3, 3, 3):\n    time.sleep(0.1)\n\n# Read orientation (quaternion)\nquat = imu.quaternion\nprint(f\"Quaternion: {quat}\")\n```\n\n**ROS 2 Publisher**:\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nimport adafruit_bno055\n\nclass IMUNode(Node):\n    def __init__(self):\n        super().__init__('imu_node')\n        self.publisher = self.create_publisher(Imu, '/imu/data', 10)\n        self.timer = self.create_timer(0.01, self.publish_imu)  # 100 Hz\n\n        i2c = board.I2C()\n        self.imu = adafruit_bno055.BNO055_I2C(i2c)\n\n    def publish_imu(self):\n        msg = Imu()\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.header.frame_id = 'imu_link'\n\n        # Orientation (quaternion)\n        quat = self.imu.quaternion\n        if quat is not None:\n            msg.orientation.x = quat[1]\n            msg.orientation.y = quat[2]\n            msg.orientation.z = quat[3]\n            msg.orientation.w = quat[0]\n\n        # Angular velocity\n        gyro = self.imu.gyro\n        if gyro is not None:\n            msg.angular_velocity.x = gyro[0]\n            msg.angular_velocity.y = gyro[1]\n            msg.angular_velocity.z = gyro[2]\n\n        # Linear acceleration\n        accel = self.imu.linear_acceleration\n        if accel is not None:\n            msg.linear_acceleration.x = accel[0]\n            msg.linear_acceleration.y = accel[1]\n            msg.linear_acceleration.z = accel[2]\n\n        self.publisher.publish(msg)\n```\n\n### Force/Torque Sensors\n\n**ATI Mini45**: 6-axis F/T sensor (hands + feet)\n\n**Use Cases**:\n- **Hands**: Grasp force feedback, object weight estimation\n- **Feet**: Ground contact detection, balance control\n\n**ROS 2 Integration**:\n```python\nfrom sensor_msgs.msg import WrenchStamped\n\nclass FTSensorNode(Node):\n    def __init__(self):\n        super().__init__('ft_sensor_node')\n        self.pub_left = self.create_publisher(WrenchStamped, '/ft/left_hand', 10)\n        self.pub_right = self.create_publisher(WrenchStamped, '/ft/right_hand', 10)\n\n        # Initialize sensor via serial/USB\n        self.ft_sensor = ATI_FT_Sensor('/dev/ttyUSB0')\n        self.timer = self.create_timer(0.01, self.publish_ft)\n\n    def publish_ft(self):\n        # Read F/T data\n        fx, fy, fz, tx, ty, tz = self.ft_sensor.read()\n\n        msg = WrenchStamped()\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.wrench.force.x = fx\n        msg.wrench.force.y = fy\n        msg.wrench.force.z = fz\n        msg.wrench.torque.x = tx\n        msg.wrench.torque.y = ty\n        msg.wrench.torque.z = tz\n\n        self.pub_left.publish(msg)\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 23,
        "chapter_title": "Chapter 2: Perception Stack",
        "section": "2.1 Sensor Suite Integration",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 362,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-perception-stack.md"
      }
    },
    {
      "content": "### Visual SLAM\n\n**Isaac ROS Visual SLAM**: GPU-accelerated ORB-SLAM3\n\n**Setup**:\n```bash\n# Install Isaac ROS Visual SLAM\nsudo apt-get install ros-humble-isaac-ros-visual-slam\n\n# Launch\nros2 launch isaac_ros_visual_slam isaac_ros_visual_slam.launch.py\n```\n\n**Configuration** (`config/vslam.yaml`):\n```yaml\nvisual_slam_node:\n  ros__parameters:\n    enable_imu_fusion: true\n    enable_localization_n_mapping: true\n    enable_observations_view: true\n    enable_landmarks_view: true\n    enable_debug_mode: false\n\n    # Performance\n    max_landmarks_per_frame: 500\n    num_cameras: 1\n\n    # IMU settings\n    imu_frame: 'imu_link'\n    gyroscope_noise_density: 0.001\n    accelerometer_noise_density: 0.01\n```\n\n**Subscribed Topics**:\n- `/camera/color/image_raw`\n- `/camera/camera_info`\n- `/imu/data`\n\n**Published Topics**:\n- `/visual_slam/tracking/odometry` (nav_msgs/Odometry)\n- `/visual_slam/tracking/vo_pose` (geometry_msgs/PoseStamped)\n- `/visual_slam/vis/observations_cloud` (sensor_msgs/PointCloud2)\n\n### Object Detection\n\n**YOLOv8 + Segmentation**:\n\n```python\nfrom ultralytics import YOLO\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D\nfrom cv_bridge import CvBridge\n\nclass ObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('object_detection_node')\n\n        # Load YOLOv8 model\n        self.model = YOLO('yolov8n.pt')  # Nano for speed\n        self.bridge = CvBridge()\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/color/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Publishers\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            '/detections',\n            10\n        )\n\n    def image_callback(self, msg):\n        # Convert ROS Image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')\n\n        # Run inference\n        results = self.model(cv_image, conf=0.5)\n\n        # Parse results\n        detections_msg = Detection2DArray()\n        detections_msg.header = msg.header\n\n        for result in results:\n            for box in result.boxes:\n                det = Detection2D()\n                det.bbox.center.position.x = (box.xyxy[0][0] + box.xyxy[0][2]) / 2\n                det.bbox.center.position.y = (box.xyxy[0][1] + box.xyxy[0][3]) / 2\n                det.bbox.size_x = box.xyxy[0][2] - box.xyxy[0][0]\n                det.bbox.size_y = box.xyxy[0][3] - box.xyxy[0][1]\n\n                # Class and confidence\n                det.results[0].hypothesis.class_id = str(int(box.cls[0]))\n                det.results[0].hypothesis.score = float(box.conf[0])\n\n                detections_msg.detections.append(det)\n\n        self.detection_pub.publish(detections_msg)\n```\n\n### Depth-Based Grasp Pose Estimation\n\n**GraspNet-1Billion** (or simpler heuristic):\n\n```python\nimport numpy as np\nfrom geometry_msgs.msg import PoseStamped\n\nclass GraspPoseEstimator:\n    def __init__(self):\n        self.camera_intrinsics = np.array([\n            [615.0, 0, 320.0],\n            [0, 615.0, 240.0],\n            [0, 0, 1.0]\n        ])\n\n    def estimate_grasp(self, depth_image, object_bbox):\n        \"\"\"\n        Estimate 6-DOF grasp pose from depth image and object bbox\n\n        Args:\n            depth_image: (H, W) depth in meters\n            object_bbox: (x_min, y_min, x_max, y_max)\n\n        Returns:\n            PoseStamped: Grasp pose in camera frame\n        \"\"\"\n        x_min, y_min, x_max, y_max = object_bbox\n\n        # Crop object region\n        object_depth = depth_image[y_min:y_max, x_min:x_max]\n\n        # Find closest point (naive approach)\n        min_depth = np.min(object_depth[object_depth > 0])\n        y_grasp, x_grasp = np.where(object_depth == min_depth)\n        x_grasp = x_grasp[0] + x_min\n        y_grasp = y_grasp[0] + y_min\n\n        # Backproject to 3D\n        z = depth_image[y_grasp, x_grasp]\n        x = (x_grasp - self.camera_intrinsics[0, 2]) * z / self.camera_intrinsics[0, 0]\n        y = (y_grasp - self.camera_intrinsics[1, 2]) * z / self.camera_intrinsics[1, 1]\n\n        # Grasp pose (top-down grasp)\n        pose = PoseStamped()\n        pose.header.frame_id = 'camera_color_optical_frame'\n        pose.pose.position.x = x\n        pose.pose.position.y = y\n        pose.pose.position.z = z\n        pose.pose.orientation.x = 0.707\n        pose.pose.orientation.y = 0.0\n        pose.pose.orientation.z = 0.0\n        pose.pose.orientation.w = 0.707  # 90° pitch for top-down grasp\n\n        return pose\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 23,
        "chapter_title": "Chapter 2: Perception Stack",
        "section": "2.2 Perception Pipeline",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 405,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-perception-stack.md"
      }
    },
    {
      "content": "### Sensor Fusion (EKF)\n\n**robot_localization** package for sensor fusion:\n\n```yaml\n# config/ekf.yaml\nekf_filter_node:\n  ros__parameters:\n    frequency: 50.0\n    sensor_timeout: 0.1\n    two_d_mode: false\n\n    # Input sources\n    odom0: /visual_slam/tracking/odometry\n    odom0_config: [true,  true,  true,  # x, y, z\n                   false, false, false, # roll, pitch, yaw\n                   true,  true,  true,  # vx, vy, vz\n                   false, false, false, # vroll, vpitch, vyaw\n                   false, false, false] # ax, ay, az\n\n    imu0: /imu/data\n    imu0_config: [false, false, false,  # x, y, z (position from IMU = false)\n                  true,  true,  true,   # roll, pitch, yaw\n                  false, false, false,  # vx, vy, vz\n                  true,  true,  true,   # vroll, vpitch, vyaw\n                  true,  true,  true]   # ax, ay, az\n\n    # Process noise covariance\n    process_noise_covariance: [0.05, 0,    0,    ...\n                                0,    0.05, 0,    ...\n                                ...]\n\n    # Initial estimate covariance\n    initial_estimate_covariance: [1e-9, 0,    0,    ...\n                                   0,    1e-9, 0,    ...\n                                   ...]\n```\n\n**Launch**:\n```bash\nros2 launch robot_localization ekf.launch.py\n```\n\n**Output**:\n- `/odometry/filtered` (nav_msgs/Odometry) - Fused pose and velocity\n\n### Complete Perception Pipeline\n\n```mermaid\ngraph LR\n    Camera[RGB-D Camera] --> SLAM[Visual SLAM]\n    Camera --> ObjDet[Object Detection<br/>YOLOv8]\n\n    SLAM --> Odom[Odometry]\n    IMU[IMU] --> EKF[Extended Kalman Filter]\n    Odom --> EKF\n\n    ObjDet --> GraspPose[Grasp Pose<br/>Estimator]\n    Camera --> GraspPose\n\n    EKF --> FilteredState[Filtered State<br/>position, velocity, orientation]\n    GraspPose --> ObjectPoses[Object Poses<br/>for manipulation]\n\n    style SLAM fill:#e1f5ff\n    style EKF fill:#ffffcc\n    style ObjDet fill:#ffe1e1\n```\n\n**Figure 2.1**: Perception pipeline showing sensor fusion (SLAM + IMU → EKF) and object understanding (detection + depth → grasp poses).\n\n### Perception Quality Metrics\n\n**Localization Accuracy**:\n```python\ndef evaluate_localization(ground_truth_poses, estimated_poses):\n    \"\"\"\n    Compute localization error metrics\n\n    Returns:\n        dict: {'mean_error': float, 'max_error': float, 'rmse': float}\n    \"\"\"\n    errors = []\n    for gt, est in zip(ground_truth_poses, estimated_poses):\n        error = np.linalg.norm(gt.position - est.position)\n        errors.append(error)\n\n    return {\n        'mean_error': np.mean(errors),\n        'max_error': np.max(errors),\n        'rmse': np.sqrt(np.mean(np.array(errors)**2))\n    }\n```\n\n**Object Detection Performance**:\n- **mAP (mean Average Precision)**: >0.7 for 100 household objects\n- **Inference time**: `<30ms` (YOLOv8-nano on Jetson Orin)\n- **Detection rate**: >90% at 1m distance",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 23,
        "chapter_title": "Chapter 2: Perception Stack",
        "section": "2.3 State Estimation",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 302,
        "has_code_block": true,
        "has_math": false,
        "source_file": "02-perception-stack.md"
      }
    },
    {
      "content": "**Exercise 2.1**: Sensor Integration\n- Connect RealSense D435i to ROS 2\n- Verify all topics are publishing (RGB, depth, IMU)\n- Visualize in RViz2\n- Measure actual frame rates\n\n**Exercise 2.2**: Visual SLAM Evaluation\n- Run Isaac ROS Visual SLAM in a room\n- Walk a closed loop (return to start)\n- Measure loop closure error (drift)\n- Compare with and without IMU fusion\n\n**Exercise 2.3**: Object Detection Pipeline\n- Train/fine-tune YOLOv8 on custom objects (5-10 classes)\n- Deploy on Jetson Orin\n- Benchmark inference time and accuracy (mAP)\n- Integrate with ROS 2 perception stack\n\n**Exercise 2.4**: Grasp Pose Estimation\n- Implement depth-based grasp pose estimator\n- Test on 10 different objects (varied shapes)\n- Measure success rate in simulation\n- Analyze failure modes (reflective surfaces, thin objects)",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 23,
        "chapter_title": "Chapter 2: Perception Stack",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 129,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-perception-stack.md"
      }
    },
    {
      "content": "**Sensor Integration**: RGB-D cameras (depth + color), IMU (orientation), F/T sensors (contact)\n**Perception Pipeline**: Visual SLAM (localization), YOLOv8 (object detection), depth-based grasp estimation\n**State Estimation**: EKF sensor fusion (SLAM + IMU) for robust pose and velocity estimates\n\n**Next**: Chapter 3 covers control stack (whole-body control, balance, joint-level control).",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 23,
        "chapter_title": "Chapter 2: Perception Stack",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 48,
        "has_code_block": false,
        "has_math": false,
        "source_file": "02-perception-stack.md"
      }
    },
    {
      "content": "---\nsidebar_position: 3\ntitle: \"Chapter 3: Control Stack\"\ndescription: Whole-body control, balance, and joint-level control for humanoid robots\ntags: [control, whole-body, balance, zmp, mpc, pid]\n---\n\n# Chapter 3: Control Stack",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 24,
        "chapter_title": "Chapter 3: Control Stack",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 31,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-control-stack.md"
      }
    },
    {
      "content": "1. Implement whole-body controller for coordinated motion\n2. Design balance controller using ZMP or MPC\n3. Tune joint-level PID controllers for tracking",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 24,
        "chapter_title": "Chapter 3: Control Stack",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 22,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-control-stack.md"
      }
    },
    {
      "content": "### Control Hierarchy\n\n**Three-Layer Architecture**:\n\n1. **High-Level** (10 Hz): Task-space commands (end-effector poses, CoM trajectory)\n2. **Mid-Level** (100 Hz): Whole-body QP optimization (joint torques/accelerations)\n3. **Low-Level** (1000 Hz): Joint PID control (motor commands)\n\n```mermaid\ngraph TB\n    HighLevel[High-Level Planner<br/>Task-space goals]\n    --> |Desired EE pose, CoM| WBC[Whole-Body Controller<br/>QP Optimization]\n\n    WBC --> |Joint torques| Balance[Balance Controller<br/>ZMP/DCM]\n\n    Balance --> |Adjusted torques| JointPID[Joint PID Controllers<br/>30x motors]\n\n    JointPID --> Motors[Motors<br/>Execute commands]\n\n    IMU[IMU] --> Balance\n    FT[Force/Torque<br/>Sensors] --> Balance\n    Encoders[Joint Encoders] --> JointPID\n\n    style WBC fill:#e1f5ff\n    style Balance fill:#ffffcc\n    style JointPID fill:#ffe1e1\n```\n\n**Figure 3.1**: Control hierarchy showing high-level task commands flowing through whole-body QP, balance controller, and joint PIDs with sensor feedback loops.\n\n### Quadratic Programming (QP) Formulation\n\n**Goal**: Find joint accelerations $\\ddot{q}$ that achieve multiple tasks while respecting constraints\n\n**Optimization Problem**:\n\n```\nminimize:   || J_ee * q̈ - ẍ_ee_des ||²     (end-effector task)\n          + || J_com * q̈ - ẍ_com_des ||²   (CoM task)\n          + || q̈ ||²                        (regularization)\n\nsubject to: M * q̈ + h = τ                  (dynamics)\n            τ_min ≤ τ ≤ τ_max               (torque limits)\n            J_contact * q̈ = 0               (contact constraints)\n```\n\n**Implementation** (using `qpsolvers`):\n\n```python\nimport numpy as np\nfrom qpsolvers import solve_qp\n\nclass WholeBodyController:\n    def __init__(self, robot_model):\n        self.robot = robot_model\n        self.n_joints = robot_model.nq\n\n    def solve(self, q, q_dot, ee_pose_des, com_pos_des):\n        \"\"\"\n        Solve QP for joint accelerations\n\n        Args:\n            q: Joint positions (n,)\n            q_dot: Joint velocities (n,)\n            ee_pose_des: Desired end-effector pose (6,)\n            com_pos_des: Desired CoM position (3,)\n\n        Returns:\n            tau: Joint torques (n,)\n        \"\"\"\n        # Compute Jacobians\n        J_ee = self.robot.get_ee_jacobian(q)  # (6, n)\n        J_com = self.robot.get_com_jacobian(q)  # (3, n)\n\n        # Desired accelerations\n        ee_error = ee_pose_des - self.robot.get_ee_pose(q)\n        com_error = com_pos_des - self.robot.get_com_position(q)\n\n        # PD control law\n        kp_ee, kd_ee = 100, 20\n        kp_com, kd_com = 200, 40\n\n        acc_ee_des = kp_ee * ee_error - kd_ee * (J_ee @ q_dot)\n        acc_com_des = kp_com * com_error - kd_com * (J_com @ q_dot)\n\n        # QP matrices\n        # Minimize: 1/2 q̈^T P q̈ + q^T q̈\n        P = J_ee.T @ J_ee + J_com.T @ J_com + 0.001 * np.eye(self.n_joints)\n        q_vec = -J_ee.T @ acc_ee_des - J_com.T @ acc_com_des\n\n        # Equality constraint: M * q̈ + h = τ (dynamics)\n        M = self.robot.get_mass_matrix(q)\n        h = self.robot.get_nonlinear_effects(q, q_dot)\n\n        # Torque limits\n        tau_max = self.robot.torque_limits\n        G = np.vstack([np.eye(self.n_joints), -np.eye(self.n_joints)])\n        h_ineq = np.hstack([tau_max, tau_max])\n\n        # Solve QP\n        q_ddot = solve_qp(P, q_vec, G, h_ineq, solver='quadprog')\n\n        # Compute torques\n        tau = M @ q_ddot + h\n\n        return tau\n```\n\n### Prioritized Tasks\n\n**Use Case**: High-priority task (balance) vs low-priority task (arm motion)\n\n**Hierarchical QP**:\n\n```python\nclass PrioritizedWBC(WholeBodyController):\n    def solve_hierarchical(self, q, q_dot, tasks_prioritized):\n        \"\"\"\n        Solve tasks in order of priority\n\n        Args:\n            tasks_prioritized: List of (Jacobian, acc_des, weight) tuples\n                               Ordered from high to low priority\n\n        Returns:\n            tau: Joint torques\n        \"\"\"\n        q_ddot = np.zeros(self.n_joints)\n\n        for J, acc_des, weight in tasks_prioritized:\n            # Minimize: || J * q̈ - acc_des ||² + || q̈ - q̈_prev ||²\n            P = J.T @ J + weight * np.eye(self.n_joints)\n            q_vec = -J.T @ acc_des + weight * q_ddot\n\n            # Solve\n            q_ddot = solve_qp(P, q_vec)\n\n        # Dynamics\n        M = self.robot.get_mass_matrix(q)\n        h = self.robot.get_nonlinear_effects(q, q_dot)\n        tau = M @ q_ddot + h\n\n        return tau\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 24,
        "chapter_title": "Chapter 3: Control Stack",
        "section": "3.1 Whole-Body Control Architecture",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 505,
        "has_code_block": true,
        "has_math": true,
        "source_file": "03-control-stack.md"
      }
    },
    {
      "content": "### Zero Moment Point (ZMP)\n\n**Definition**: Point on the ground where net moment is zero\n\n**Condition for Stability**: ZMP must be inside support polygon (footprint)\n\n**ZMP Computation**:\n\n```python\nclass ZMPController:\n    def __init__(self, robot):\n        self.robot = robot\n        self.g = 9.81\n\n    def compute_zmp(self, q, q_dot, q_ddot):\n        \"\"\"\n        Compute ZMP position from robot state\n\n        Returns:\n            zmp: (2,) - ZMP position in (x, y)\n        \"\"\"\n        # Center of mass\n        com_pos = self.robot.get_com_position(q)\n        com_vel = self.robot.get_com_velocity(q, q_dot)\n        com_acc = self.robot.get_com_acceleration(q, q_dot, q_ddot)\n\n        # ZMP formula (assuming flat ground at z=0)\n        zmp_x = com_pos[0] - (com_pos[2] / (com_acc[2] + self.g)) * com_acc[0]\n        zmp_y = com_pos[1] - (com_pos[2] / (com_acc[2] + self.g)) * com_acc[1]\n\n        return np.array([zmp_x, zmp_y])\n\n    def is_stable(self, zmp, support_polygon):\n        \"\"\"\n        Check if ZMP is inside support polygon\n\n        Args:\n            zmp: (2,) ZMP position\n            support_polygon: (N, 2) vertices of support polygon\n\n        Returns:\n            bool: True if stable\n        \"\"\"\n        from matplotlib.path import Path\n        polygon = Path(support_polygon)\n        return polygon.contains_point(zmp)\n```\n\n**ZMP-Based Walking**:\n\n```python\ndef generate_zmp_trajectory(footstep_plan, com_height=0.8):\n    \"\"\"\n    Generate ZMP trajectory for walking\n\n    Args:\n        footstep_plan: List of (x, y, theta) footstep poses\n        com_height: Desired CoM height\n\n    Returns:\n        zmp_traj: (T, 2) ZMP trajectory\n        com_traj: (T, 3) CoM trajectory\n    \"\"\"\n    zmp_traj = []\n    com_traj = []\n\n    for i in range(len(footstep_plan) - 1):\n        # ZMP stays at current foot during swing phase\n        foot_current = footstep_plan[i][:2]\n        foot_next = footstep_plan[i+1][:2]\n\n        # Double support: ZMP at midpoint\n        zmp_double = (foot_current + foot_next) / 2\n\n        # Single support: ZMP at stance foot\n        zmp_single = foot_current\n\n        # Trajectory (10 steps per phase)\n        for t in range(10):\n            zmp_traj.append(zmp_single)\n\n        for t in range(10):\n            zmp_traj.append(zmp_double)\n\n        # CoM follows ZMP with offset\n        com_x = zmp_single[0]\n        com_y = zmp_single[1]\n        com_traj.append([com_x, com_y, com_height])\n\n    return np.array(zmp_traj), np.array(com_traj)\n```\n\n### Model Predictive Control (MPC)\n\n**Formulation**:\n\nPredict future states over horizon $N$ and optimize control inputs:\n\n```\nminimize:   Σ || x_k - x_ref ||²_Q + || u_k ||²_R\n\nsubject to: x_{k+1} = A x_k + B u_k     (linear dynamics)\n            x_min ≤ x_k ≤ x_max         (state limits)\n            u_min ≤ u_k ≤ u_max         (control limits)\n```\n\n**Implementation** (using `cvxpy`):\n\n```python\nimport cvxpy as cp\n\nclass MPCBalanceController:\n    def __init__(self, dt=0.01, horizon=20):\n        self.dt = dt\n        self.N = horizon\n\n        # Linearized CoM dynamics (inverted pendulum)\n        # ẍ = g/h * x (where h = CoM height)\n        self.g = 9.81\n        self.h = 0.8\n\n        # Discretize: x_{k+1} = A x_k + B u_k\n        # State: [pos, vel], Control: [acc]\n        self.A = np.array([[1, dt], [0, 1 + (self.g/self.h)*dt**2]])\n        self.B = np.array([[0], [dt]])\n\n    def solve(self, x0, x_ref):\n        \"\"\"\n        Solve MPC for CoM control\n\n        Args:\n            x0: Initial state [pos, vel]\n            x_ref: Reference trajectory (N, 2)\n\n        Returns:\n            u_opt: Optimal control sequence (N, 1)\n        \"\"\"\n        n_states = 2\n        n_controls = 1\n\n        # Decision variables\n        x = cp.Variable((self.N + 1, n_states))\n        u = cp.Variable((self.N, n_controls))\n\n        # Cost matrices\n        Q = np.diag([100, 10])  # Position and velocity weights\n        R = np.diag([1])        # Control effort weight\n\n        # Objective\n        cost = 0\n        for k in range(self.N):\n            cost += cp.quad_form(x[k] - x_ref[k], Q)\n            cost += cp.quad_form(u[k], R)\n\n        # Constraints\n        constraints = [x[0] == x0]\n        for k in range(self.N):\n            constraints += [x[k+1] == self.A @ x[k] + self.B @ u[k]]\n            constraints += [cp.abs(u[k]) <= 5.0]  # Acceleration limits\n\n        # Solve\n        problem = cp.Problem(cp.Minimize(cost), constraints)\n        problem.solve(solver=cp.OSQP)\n\n        return u.value\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 24,
        "chapter_title": "Chapter 3: Control Stack",
        "section": "3.2 Balance Controller",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 522,
        "has_code_block": true,
        "has_math": true,
        "source_file": "03-control-stack.md"
      }
    },
    {
      "content": "### PID Tuning\n\n**Ziegler-Nichols Method**:\n\n1. Set $K_i = 0$, $K_d = 0$\n2. Increase $K_p$ until oscillation (critical gain $K_u$)\n3. Measure oscillation period $T_u$\n4. Compute PID gains:\n   - $K_p = 0.6 K_u$\n   - $K_i = 2 K_p / T_u$\n   - $K_d = K_p T_u / 8$\n\n**Implementation**:\n\n```python\nclass JointPIDController:\n    def __init__(self, kp, ki, kd, dt=0.001):\n        self.kp = kp\n        self.ki = ki\n        self.kd = kd\n        self.dt = dt\n\n        self.error_integral = 0\n        self.prev_error = 0\n\n    def compute(self, q_des, q_actual, q_dot_actual):\n        \"\"\"\n        Compute PID control\n\n        Args:\n            q_des: Desired joint position\n            q_actual: Actual joint position\n            q_dot_actual: Actual joint velocity\n\n        Returns:\n            tau: Joint torque command\n        \"\"\"\n        # Error\n        error = q_des - q_actual\n\n        # Integral (with anti-windup)\n        self.error_integral += error * self.dt\n        self.error_integral = np.clip(self.error_integral, -10, 10)\n\n        # Derivative (from velocity feedback)\n        error_derivative = -q_dot_actual\n\n        # PID law\n        tau = self.kp * error + self.ki * self.error_integral + self.kd * error_derivative\n\n        self.prev_error = error\n        return tau\n```\n\n### Impedance Control\n\n**Compliant interaction** with environment:\n\n```python\nclass ImpedanceController:\n    def __init__(self, K_stiffness, D_damping):\n        \"\"\"\n        Args:\n            K_stiffness: (6, 6) stiffness matrix (task space)\n            D_damping: (6, 6) damping matrix\n        \"\"\"\n        self.K = K_stiffness\n        self.D = D_damping\n\n    def compute(self, x_des, x_actual, x_dot_actual, f_external):\n        \"\"\"\n        Compute impedance control force\n\n        Args:\n            x_des: Desired end-effector pose (6,)\n            x_actual: Actual pose (6,)\n            x_dot_actual: Actual velocity (6,)\n            f_external: External force (6,)\n\n        Returns:\n            f_cmd: Commanded force (6,)\n        \"\"\"\n        # Position error\n        x_error = x_des - x_actual\n\n        # Impedance law: F = K*(x_des - x) - D*ẋ\n        f_cmd = self.K @ x_error - self.D @ x_dot_actual\n\n        # Add external force compensation\n        f_cmd += f_external\n\n        return f_cmd\n\n    def joint_torques(self, f_cmd, J):\n        \"\"\"\n        Convert task-space force to joint torques\n\n        Args:\n            f_cmd: Task-space force (6,)\n            J: Jacobian (6, n)\n\n        Returns:\n            tau: Joint torques (n,)\n        \"\"\"\n        return J.T @ f_cmd\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 24,
        "chapter_title": "Chapter 3: Control Stack",
        "section": "3.3 Joint-Level Control",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 294,
        "has_code_block": true,
        "has_math": true,
        "source_file": "03-control-stack.md"
      }
    },
    {
      "content": "**Exercise 3.1**: Whole-Body Controller Implementation\n- Implement QP-based whole-body controller in simulation\n- Test with dual-arm reaching task (2 end-effectors)\n- Compare prioritized vs weighted multi-task approach\n\n**Exercise 3.2**: ZMP-Based Walking\n- Generate ZMP trajectory for 5-step walking pattern\n- Simulate in Isaac Sim or MuJoCo\n- Measure stability margin (distance from ZMP to support polygon edge)\n\n**Exercise 3.3**: PID Tuning\n- Tune PID gains for a single joint (elbow or knee)\n- Use Ziegler-Nichols method or manual tuning\n- Plot step response and measure: rise time, overshoot, settling time\n\n**Exercise 3.4**: Impedance Control\n- Implement impedance controller for arm\n- Test compliance by pushing end-effector in simulation\n- Measure interaction force and position deviation",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 24,
        "chapter_title": "Chapter 3: Control Stack",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 115,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-control-stack.md"
      }
    },
    {
      "content": "**Whole-Body Control**: QP optimization for multi-task coordination (end-effector + CoM tracking)\n**Balance Control**: ZMP for stability verification, MPC for predictive balance\n**Joint Control**: PID for tracking, impedance for compliant interaction\n\n**Next**: Chapter 4 covers VLA autonomy integration and real-world deployment.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 24,
        "chapter_title": "Chapter 3: Control Stack",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 40,
        "has_code_block": false,
        "has_math": false,
        "source_file": "03-control-stack.md"
      }
    },
    {
      "content": "---\nsidebar_position: 4\ntitle: \"Chapter 4: VLA Autonomy\"\ndescription: VLA policy integration, task planning, and real-world deployment for autonomous humanoids\ntags: [vla, autonomy, task-planning, deployment, testing]\n---\n\n# Chapter 4: VLA Autonomy",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 25,
        "chapter_title": "Chapter 4: VLA Autonomy",
        "section": "Introduction",
        "subsection": null,
        "page_number": null,
        "chunk_index": 0,
        "word_count": 32,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-vla-autonomy.md"
      }
    },
    {
      "content": "1. Integrate OpenVLA policy with humanoid control stack\n2. Implement task planning and execution framework\n3. Deploy and test autonomous system in real-world scenarios",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 25,
        "chapter_title": "Chapter 4: VLA Autonomy",
        "section": "Learning Objectives",
        "subsection": null,
        "page_number": null,
        "chunk_index": 1,
        "word_count": 24,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-vla-autonomy.md"
      }
    },
    {
      "content": "### OpenVLA Deployment on Humanoid\n\n**System Architecture**:\n\n```mermaid\ngraph TB\n    User[User Command<br/>fetch the cup]\n\n    User --> VLA[VLA Policy<br/>OpenVLA 7B]\n\n    Camera[RGB-D Camera] --> VLA\n    State[Robot State<br/>joint angles, IMU] --> VLA\n\n    VLA --> Skills[Skill Library]\n\n    Skills --> Navigate[navigate<br/>goal]\n    Skills --> Grasp[grasp<br/>object]\n    Skills --> Place[place<br/>location]\n    Skills --> Open[open<br/>container]\n\n    Navigate --> PathPlan[Path Planner<br/>Nav2]\n    Grasp --> MotionPlan[Motion Planner<br/>cuRobo]\n    Place --> MotionPlan\n    Open --> MotionPlan\n\n    PathPlan --> WBC[Whole-Body<br/>Controller]\n    MotionPlan --> WBC\n\n    WBC --> Robot[Robot<br/>Execution]\n\n    style VLA fill:#e1f5ff\n    style Skills fill:#ffffcc\n    style WBC fill:#ffe1e1\n```\n\n**Figure 4.1**: VLA autonomy architecture showing language commands flowing through VLA policy to skill primitives, then to motion/path planners, and finally whole-body controller.\n\n### Fine-Tuning OpenVLA for Humanoid\n\n**Dataset Collection** (simulation):\n\n```python\nimport isaac_sim\nfrom openvla.data import RobotDataset\n\nclass HumanoidDataCollector:\n    def __init__(self, sim_env):\n        self.env = sim_env\n        self.demonstrations = []\n\n    def collect_demonstration(self, task_instruction):\n        \"\"\"\n        Collect one demonstration via teleoperation or scripted policy\n\n        Args:\n            task_instruction: str, e.g., \"pick up the red cup\"\n\n        Returns:\n            demo: Dict with observations, actions, language\n        \"\"\"\n        demo = {\n            'images': [],\n            'joint_states': [],\n            'actions': [],\n            'instruction': task_instruction\n        }\n\n        # Reset environment\n        obs = self.env.reset()\n\n        # Execute task (teleoperation or expert policy)\n        done = False\n        while not done:\n            # Capture observation\n            image = self.env.get_camera_image()\n            joint_state = self.env.get_joint_positions()\n\n            # Get action (from human or expert)\n            action = self.get_expert_action()  # Implement this\n\n            # Execute\n            obs, reward, done, info = self.env.step(action)\n\n            # Store\n            demo['images'].append(image)\n            demo['joint_states'].append(joint_state)\n            demo['actions'].append(action)\n\n        self.demonstrations.append(demo)\n        return demo\n\n    def save_dataset(self, path):\n        \"\"\"Save demonstrations in RLDS format\"\"\"\n        dataset = RobotDataset(demonstrations=self.demonstrations)\n        dataset.save(path)\n```\n\n**Fine-Tuning**:\n\n```python\nfrom openvla import OpenVLA\nfrom torch.utils.data import DataLoader\n\n# Load pretrained model\nvla = OpenVLA.from_pretrained(\"openvla-7b\")\n\n# Load humanoid dataset\ndataset = RobotDataset(data_path=\"humanoid_demos/\")\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n# Fine-tune\noptimizer = torch.optim.AdamW(vla.parameters(), lr=1e-5)\n\nfor epoch in range(20):\n    for batch in dataloader:\n        images, instructions, actions = batch\n\n        # Forward pass\n        predicted_actions = vla(images, instructions)\n        loss = F.mse_loss(predicted_actions, actions)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n# Save fine-tuned model\nvla.save_pretrained(\"openvla-humanoid\")\n```\n\n### ROS 2 Integration\n\n**VLA Policy Node** (from Module 4, adapted for humanoid):\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, JointState\nfrom std_msgs.msg import String\nfrom trajectory_msgs.msg import JointTrajectory\nfrom cv_bridge import CvBridge\nfrom openvla import OpenVLA\n\nclass HumanoidVLANode(Node):\n    def __init__(self):\n        super().__init__('humanoid_vla_node')\n\n        # Load fine-tuned model\n        self.vla = OpenVLA.from_pretrained(\"openvla-humanoid\").to(\"cuda\")\n        self.vla.eval()\n\n        self.bridge = CvBridge()\n        self.current_image = None\n        self.current_joint_state = None\n        self.current_instruction = \"stand still\"\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/color/image_raw', self.image_callback, 10\n        )\n        self.joint_sub = self.create_subscription(\n            JointState, '/joint_states', self.joint_callback, 10\n        )\n        self.instruction_sub = self.create_subscription(\n            String, '/task/instruction', self.instruction_callback, 10\n        )\n\n        # Publishers (high-level skills)\n        self.skill_pub = self.create_publisher(\n            String, '/vla/skill_command', 10\n        )\n\n        # Policy loop\n        self.timer = self.create_timer(0.1, self.policy_loop)  # 10 Hz\n\n    def policy_loop(self):\n        if self.current_image is None or self.current_joint_state is None:\n            return\n\n        # Predict skill from VLA\n        with torch.no_grad():\n            skill_command = self.vla.predict_skill(\n                image=self.current_image,\n                instruction=self.current_instruction,\n                proprioception=self.current_joint_state\n            )\n\n        # Publish skill (e.g., \"navigate(kitchen)\" or \"grasp(cup)\")\n        msg = String()\n        msg.data = skill_command\n        self.skill_pub.publish(msg)\n\n    def image_callback(self, msg):\n        self.current_image = self.bridge.imgmsg_to_cv2(msg, 'rgb8')\n\n    def joint_callback(self, msg):\n        self.current_joint_state = np.array(msg.position)\n\n    def instruction_callback(self, msg):\n        self.current_instruction = msg.data\n        self.get_logger().info(f\"New instruction: {msg.data}\")\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 25,
        "chapter_title": "Chapter 4: VLA Autonomy",
        "section": "4.1 VLA Policy Integration",
        "subsection": null,
        "page_number": null,
        "chunk_index": 2,
        "word_count": 489,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-vla-autonomy.md"
      }
    },
    {
      "content": "### Behavior Tree Framework\n\n**Skill Primitives**:\n\n```python\nimport py_trees\n\nclass NavigateSkill(py_trees.behaviour.Behaviour):\n    def __init__(self, goal_location):\n        super().__init__(name=f\"Navigate to {goal_location}\")\n        self.goal = goal_location\n\n    def update(self):\n        # Publish navigation goal to Nav2\n        self.publish_nav_goal(self.goal)\n\n        # Check if reached\n        if self.distance_to_goal() < 0.5:  # meters\n            return py_trees.common.Status.SUCCESS\n        else:\n            return py_trees.common.Status.RUNNING\n\nclass GraspSkill(py_trees.behaviour.Behaviour):\n    def __init__(self, object_name):\n        super().__init__(name=f\"Grasp {object_name}\")\n        self.object = object_name\n\n    def update(self):\n        # Detect object\n        object_pose = self.detect_object(self.object)\n\n        if object_pose is None:\n            return py_trees.common.Status.FAILURE\n\n        # Plan grasp\n        grasp_trajectory = self.plan_grasp(object_pose)\n\n        # Execute\n        self.execute_trajectory(grasp_trajectory)\n\n        # Check success (force sensor)\n        if self.gripper_force() > 1.0:\n            return py_trees.common.Status.SUCCESS\n        else:\n            return py_trees.common.Status.FAILURE\n```\n\n**Composing Tasks**:\n\n```python\ndef create_fetch_task(object_name, delivery_location):\n    \"\"\"\n    Create behavior tree for fetch task\n\n    Task: \"Fetch me the {object_name} from the {source_location}\"\n    \"\"\"\n    root = py_trees.composites.Sequence(name=\"Fetch Task\")\n\n    # 1. Navigate to object\n    navigate_to_object = NavigateSkill(goal_location=\"kitchen\")\n    root.add_child(navigate_to_object)\n\n    # 2. Grasp object\n    grasp_object = GraspSkill(object_name=object_name)\n    root.add_child(grasp_object)\n\n    # 3. Navigate to user\n    navigate_to_user = NavigateSkill(goal_location=delivery_location)\n    root.add_child(navigate_to_user)\n\n    # 4. Hand over (open gripper)\n    handover = HandoverSkill()\n    root.add_child(handover)\n\n    return root\n\n# Execute\ntask_tree = create_fetch_task(\"cup\", \"living_room\")\ntask_tree.setup_with_descendants()\n\nwhile task_tree.status != py_trees.common.Status.SUCCESS:\n    task_tree.tick_once()\n    time.sleep(0.1)\n\nprint(\"Task completed!\")\n```\n\n### Error Recovery\n\n**Retry Logic**:\n\n```python\nclass RobustGraspSkill(GraspSkill):\n    def __init__(self, object_name, max_retries=3):\n        super().__init__(object_name)\n        self.max_retries = max_retries\n        self.retry_count = 0\n\n    def update(self):\n        # Attempt grasp\n        result = super().update()\n\n        if result == py_trees.common.Status.FAILURE:\n            self.retry_count += 1\n\n            if self.retry_count < self.max_retries:\n                self.get_logger().warn(f\"Grasp failed, retrying ({self.retry_count}/{self.max_retries})\")\n                # Adjust grasp pose slightly\n                self.adjust_grasp_strategy()\n                return py_trees.common.Status.RUNNING\n            else:\n                self.get_logger().error(\"Max retries exceeded\")\n                return py_trees.common.Status.FAILURE\n\n        return result\n```\n\n**Fallback Behaviors**:\n\n```python\nfallback = py_trees.composites.Selector(name=\"Grasp with Fallback\")\n\n# Try primary grasp\nfallback.add_child(GraspSkill(\"cup\"))\n\n# Fallback 1: Try different approach angle\nfallback.add_child(GraspSkill(\"cup\", approach=\"side\"))\n\n# Fallback 2: Request human help\nfallback.add_child(RequestHumanHelp(\"Cannot grasp cup\"))\n\nfallback.tick_once()\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 25,
        "chapter_title": "Chapter 4: VLA Autonomy",
        "section": "4.2 Task Planning and Execution",
        "subsection": null,
        "page_number": null,
        "chunk_index": 3,
        "word_count": 264,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-vla-autonomy.md"
      }
    },
    {
      "content": "### Deployment Checklist\n\n**Pre-Deployment**:\n- ✅ Simulate 100+ task executions (80%+ success)\n- ✅ Safety tests (emergency stop, collision avoidance, fall detection)\n- ✅ Battery life test (2+ hours continuous operation)\n- ✅ Latency verification (VLA policy `<50ms`, end-to-end `<10s`)\n- ✅ Hardware checks (all sensors functional, motors calibrated)\n\n**Initial Deployment** (controlled environment):\n- ✅ Human supervisor present at all times\n- ✅ Soft objects only (foam, cloth) for first 50 trials\n- ✅ Restricted workspace (1m radius)\n- ✅ Emergency stop button within reach\n\n### Evaluation Metrics\n\n**Task Success Rate**:\n\n```python\nclass TaskEvaluator:\n    def __init__(self):\n        self.results = []\n\n    def evaluate_task(self, task_name, success, time_taken, error_type=None):\n        \"\"\"\n        Log task execution result\n\n        Args:\n            task_name: str\n            success: bool\n            time_taken: float (seconds)\n            error_type: str or None (e.g., \"grasp_failure\", \"navigation_timeout\")\n        \"\"\"\n        self.results.append({\n            'task': task_name,\n            'success': success,\n            'time': time_taken,\n            'error': error_type\n        })\n\n    def compute_metrics(self):\n        \"\"\"\n        Compute aggregate metrics\n\n        Returns:\n            dict: Success rate, avg time, error breakdown\n        \"\"\"\n        total = len(self.results)\n        successes = sum(r['success'] for r in self.results)\n\n        metrics = {\n            'success_rate': successes / total,\n            'avg_time': np.mean([r['time'] for r in self.results if r['success']]),\n            'error_breakdown': {}\n        }\n\n        # Error analysis\n        for result in self.results:\n            if result['error'] is not None:\n                error_type = result['error']\n                metrics['error_breakdown'][error_type] = \\\n                    metrics['error_breakdown'].get(error_type, 0) + 1\n\n        return metrics\n\n# Usage\nevaluator = TaskEvaluator()\n\nfor i in range(100):\n    success, time_taken, error = execute_fetch_task(\"cup\")\n    evaluator.evaluate_task(\"fetch_cup\", success, time_taken, error)\n\nmetrics = evaluator.compute_metrics()\nprint(f\"Success rate: {metrics['success_rate']:.1%}\")\nprint(f\"Average time: {metrics['avg_time']:.1f}s\")\nprint(f\"Errors: {metrics['error_breakdown']}\")\n```\n\n### Iterative Improvement\n\n**Failure Analysis**:\n\n1. **Collect failure cases** (video + sensor logs)\n2. **Categorize errors** (perception, planning, control, hardware)\n3. **Prioritize fixes** (highest impact, easiest to fix)\n4. **Update system** (retrain VLA, tune controllers, fix bugs)\n5. **Re-evaluate** (measure improvement)\n\n**DAgger for Continuous Learning**:\n\n```python\nclass DAggerCollector:\n    def __init__(self, vla_policy, expert_policy):\n        self.vla = vla_policy\n        self.expert = expert_policy\n        self.new_demos = []\n\n    def collect_corrective_demo(self, task):\n        \"\"\"\n        Collect demonstration where VLA policy is corrected by expert\n\n        Args:\n            task: Task description\n\n        Returns:\n            demo: Demonstration with VLA actions + expert corrections\n        \"\"\"\n        demo = {'images': [], 'actions_vla': [], 'actions_expert': []}\n\n        obs = self.env.reset()\n        done = False\n\n        while not done:\n            image = self.env.get_camera_image()\n\n            # VLA prediction\n            action_vla = self.vla.predict_action(image, task)\n\n            # Expert correction (human or oracle)\n            action_expert = self.expert.get_action(image, task)\n\n            # Execute expert action\n            obs, _, done, _ = self.env.step(action_expert)\n\n            # Store both\n            demo['images'].append(image)\n            demo['actions_vla'].append(action_vla)\n            demo['actions_expert'].append(action_expert)\n\n        self.new_demos.append(demo)\n        return demo\n\n    def retrain_vla(self):\n        \"\"\"Retrain VLA on mixture of original + corrective demos\"\"\"\n        # Mix: 80% original, 20% corrective\n        mixed_dataset = self.mix_datasets(\n            original_ratio=0.8,\n            corrective_ratio=0.2\n        )\n\n        # Fine-tune\n        self.vla.train(mixed_dataset, epochs=10)\n```",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 25,
        "chapter_title": "Chapter 4: VLA Autonomy",
        "section": "4.3 Real-World Testing and Iteration",
        "subsection": null,
        "page_number": null,
        "chunk_index": 4,
        "word_count": 405,
        "has_code_block": true,
        "has_math": false,
        "source_file": "04-vla-autonomy.md"
      }
    },
    {
      "content": "**Exercise 4.1**: VLA Fine-Tuning\n- Collect 100 demonstrations of a custom task in Isaac Sim\n- Fine-tune OpenVLA on your dataset\n- Evaluate on held-out test set (20 trials)\n- Measure success rate before vs after fine-tuning\n\n**Exercise 4.2**: Behavior Tree Design\n- Design behavior tree for \"clean the table\" task\n- Implement in py_trees\n- Add error recovery (retry logic, fallback behaviors)\n- Test in simulation (50 trials)\n\n**Exercise 4.3**: Real-World Deployment (Simulation)\n- Deploy full autonomous system in Isaac Sim\n- Run 5 different tasks (fetch, place, navigate, open, stack)\n- Log all failures and categorize errors\n- Compute overall success rate\n\n**Exercise 4.4**: DAgger Iteration\n- Implement DAgger data collection\n- Collect 50 corrective demonstrations\n- Retrain VLA with mixed dataset\n- Measure improvement in success rate",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 25,
        "chapter_title": "Chapter 4: VLA Autonomy",
        "section": "Exercises",
        "subsection": null,
        "page_number": null,
        "chunk_index": 5,
        "word_count": 130,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-vla-autonomy.md"
      }
    },
    {
      "content": "**VLA Integration**: Fine-tune OpenVLA on humanoid tasks, deploy with ROS 2 policy node\n**Task Planning**: Behavior trees for skill composition, error recovery with retries and fallbacks\n**Real-World Testing**: Systematic evaluation (success rate, time, errors), iterative improvement with DAgger\n\n**Module 5 Complete!** You now have a complete autonomous humanoid system ready for deployment.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 25,
        "chapter_title": "Chapter 4: VLA Autonomy",
        "section": "Summary",
        "subsection": null,
        "page_number": null,
        "chunk_index": 6,
        "word_count": 52,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-vla-autonomy.md"
      }
    },
    {
      "content": "✅ **System Design** (Chapter 1):\n- Requirements analysis (functional, non-functional)\n- Hardware and software architecture\n- 12-week development roadmap\n\n✅ **Perception Stack** (Chapter 2):\n- Multi-modal sensor integration (RGB-D, IMU, F/T)\n- Visual SLAM + object detection pipeline\n- EKF sensor fusion for state estimation\n\n✅ **Control Stack** (Chapter 3):\n- Whole-body QP controller for multi-task coordination\n- Balance controller (ZMP or MPC)\n- Joint-level PID and impedance control\n\n✅ **VLA Autonomy** (Chapter 4):\n- OpenVLA fine-tuning and deployment\n- Task planning with behavior trees\n- Real-world testing and iterative improvement\n\n**Congratulations!** You've completed the Physical AI and Humanoid Robotics book. You're now equipped to build, deploy, and iterate on autonomous humanoid robot systems.",
      "metadata": {
        "book_id": "physical-ai-robotics",
        "book_version": "v1.0.0",
        "chapter_number": 25,
        "chapter_title": "Chapter 4: VLA Autonomy",
        "section": "Capstone Project Checklist",
        "subsection": null,
        "page_number": null,
        "chunk_index": 7,
        "word_count": 114,
        "has_code_block": false,
        "has_math": false,
        "source_file": "04-vla-autonomy.md"
      }
    }
  ]
}